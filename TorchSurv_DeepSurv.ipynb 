{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1EY87_PKv0dTOF6mK_Kx8YPzRYVzJ6o-8",
      "authorship_tag": "ABX9TyO3MdskTY3tqAisZporJTvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osun24/nsclc-adj-chemo/blob/main/TorchSurv_DeepSurv.ipynb%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torchsurv scikit-survival\n",
        "\n",
        "# Import required packages\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "\n",
        "# (Optional) Mount Google Drive if you plan to load/save files there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81TEvMle6mqs",
        "outputId": "f1e50376-3dc0-4011-bca0-c00e57d065a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsurv in /usr/local/lib/python3.12/dist-packages (0.1.5)\n",
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.16.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.0.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.8.1)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.11.0)\n",
            "Requirement already satisfied: osqp<1.0.0,>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (0.6.7.post3)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.6.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.12/dist-packages (from osqp<1.0.0,>=0.6.3->scikit-survival) (0.1.7.post5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8,>=1.6.1->scikit-survival) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.4.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (25.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchsurv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchsurv) (3.0.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "baO87aHV3DUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a7b37b-36ba-4cba-c3ef-5ddcfc2c14e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3173553835.py:117: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Adjuvant Chemo'] = df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Rung 1/3 → target 10 epochs ===\n",
            "Alive before prune: 40; keeping top 20\n",
            "\n",
            "=== Rung 2/3 → target 30 epochs ===\n",
            "Trial 5 early-stopped at epoch 26 (best Val CI=0.6661)\n",
            "Trial 7 early-stopped at epoch 20 (best Val CI=0.6491)\n",
            "Trial 8 early-stopped at epoch 22 (best Val CI=0.6608)\n",
            "Trial 9 early-stopped at epoch 15 (best Val CI=0.6525)\n",
            "Trial 11 early-stopped at epoch 15 (best Val CI=0.6822)\n",
            "Trial 15 early-stopped at epoch 29 (best Val CI=0.6575)\n",
            "Trial 16 early-stopped at epoch 27 (best Val CI=0.6600)\n",
            "Trial 17 early-stopped at epoch 19 (best Val CI=0.6522)\n",
            "Trial 19 early-stopped at epoch 17 (best Val CI=0.6562)\n",
            "Trial 20 early-stopped at epoch 20 (best Val CI=0.6653)\n",
            "Trial 21 early-stopped at epoch 14 (best Val CI=0.6523)\n",
            "Trial 25 early-stopped at epoch 25 (best Val CI=0.6517)\n",
            "Trial 26 early-stopped at epoch 14 (best Val CI=0.6571)\n",
            "Trial 30 early-stopped at epoch 14 (best Val CI=0.6559)\n",
            "Trial 32 early-stopped at epoch 15 (best Val CI=0.6551)\n",
            "Trial 33 early-stopped at epoch 15 (best Val CI=0.6497)\n",
            "Trial 34 early-stopped at epoch 17 (best Val CI=0.6480)\n",
            "Trial 35 early-stopped at epoch 13 (best Val CI=0.6570)\n",
            "Trial 37 early-stopped at epoch 15 (best Val CI=0.6674)\n",
            "Alive before prune: 20; keeping top 10\n",
            "\n",
            "=== Rung 3/3 → target 60 epochs ===\n",
            "Trial 5 early-stopped at epoch 27 (best Val CI=0.6661)\n",
            "Trial 8 early-stopped at epoch 23 (best Val CI=0.6608)\n",
            "Trial 11 early-stopped at epoch 16 (best Val CI=0.6822)\n",
            "Trial 15 early-stopped at epoch 30 (best Val CI=0.6575)\n",
            "Trial 16 early-stopped at epoch 28 (best Val CI=0.6600)\n",
            "Trial 19 early-stopped at epoch 18 (best Val CI=0.6562)\n",
            "Trial 20 early-stopped at epoch 21 (best Val CI=0.6653)\n",
            "Trial 26 early-stopped at epoch 15 (best Val CI=0.6571)\n",
            "Trial 35 early-stopped at epoch 14 (best Val CI=0.6570)\n",
            "Trial 37 early-stopped at epoch 16 (best Val CI=0.6674)\n",
            "Alive before prune: 10; keeping top 5\n",
            "\n",
            "Best Hyperparameters:\n",
            "{'layers': [32, 16], 'dropout': 0.3799632809900433, 'lr': 3.0512902722570623e-05, 'wd': 0.006122437741262166, 'l1': 2.1364941755036067e-05}\n",
            "Best Validation CI: 0.6822047836097928\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial0_layers-8_drop0.48_lr2.17e-04_wd4.98e-03_l11.54e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial1_layers-16-8_drop0.69_lr1.73e-04_wd6.11e-03_l11.80e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial2_layers-32-16_drop0.45_lr2.53e-04_wd4.40e-03_l14.42e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial3_layers-32_drop0.48_lr5.06e-05_wd3.59e-03_l11.34e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial4_layers-32-16_drop0.55_lr1.72e-04_wd2.26e-03_l18.74e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial5_layers-32-16_drop0.66_lr1.80e-04_wd1.57e-03_l18.58e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial6_layers-32_drop0.36_lr1.45e-04_wd5.56e-03_l18.61e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial7_layers-8_drop0.43_lr7.04e-05_wd2.95e-03_l12.39e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial8_layers-32_drop0.49_lr5.06e-05_wd4.68e-03_l17.49e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial9_layers-8_drop0.63_lr1.50e-04_wd2.05e-03_l14.62e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial10_layers-32_drop0.45_lr5.83e-05_wd4.81e-03_l11.90e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial11_layers-32-16_drop0.38_lr3.05e-05_wd6.12e-03_l12.14e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial12_layers-32_drop0.61_lr8.63e-05_wd3.70e-03_l11.90e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial13_layers-16-8_drop0.35_lr1.40e-04_wd2.96e-03_l11.35e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial14_layers-8_drop0.55_lr1.07e-04_wd3.62e-03_l14.05e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial15_layers-16-8_drop0.31_lr8.20e-05_wd1.64e-03_l16.56e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial16_layers-32-16_drop0.39_lr3.43e-05_wd1.91e-03_l13.87e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial17_layers-32-16_drop0.56_lr1.08e-04_wd6.08e-03_l12.13e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial18_layers-32_drop0.63_lr4.41e-05_wd1.05e-03_l11.51e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial19_layers-32_drop0.59_lr8.69e-05_wd1.45e-03_l11.00e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial20_layers-32-16_drop0.58_lr8.38e-05_wd2.40e-03_l14.01e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial21_layers-8_drop0.55_lr6.90e-05_wd1.22e-03_l11.72e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial22_layers-16_drop0.66_lr1.50e-04_wd1.84e-03_l18.68e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial23_layers-32-16_drop0.61_lr1.56e-04_wd2.81e-03_l13.50e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial24_layers-8_drop0.66_lr8.57e-05_wd1.59e-03_l14.09e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial25_layers-8_drop0.53_lr4.51e-05_wd7.19e-03_l13.29e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial26_layers-32_drop0.47_lr1.27e-04_wd3.84e-03_l11.99e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial27_layers-16-8_drop0.33_lr7.81e-05_wd1.10e-03_l19.73e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial28_layers-8_drop0.36_lr3.81e-05_wd3.87e-03_l12.19e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial29_layers-16_drop0.67_lr1.14e-04_wd2.22e-03_l11.52e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial30_layers-16_drop0.68_lr9.11e-05_wd6.06e-03_l11.46e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial31_layers-8_drop0.49_lr9.29e-05_wd8.67e-03_l11.39e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial32_layers-32_drop0.41_lr6.44e-05_wd3.32e-03_l17.55e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial33_layers-32_drop0.31_lr2.01e-04_wd7.87e-03_l11.91e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial34_layers-16_drop0.34_lr1.41e-04_wd1.91e-03_l12.08e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial35_layers-32_drop0.59_lr1.76e-04_wd1.28e-03_l16.79e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial36_layers-32-16_drop0.31_lr1.08e-04_wd2.35e-03_l14.57e-05.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial37_layers-16_drop0.62_lr6.23e-05_wd8.97e-03_l13.82e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial38_layers-16-8_drop0.40_lr2.59e-04_wd1.46e-03_l11.23e-06.png\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_trial39_layers-32_drop0.47_lr2.95e-04_wd7.79e-03_l13.14e-05.png\n",
            "Hyperparameter search results saved to /content/drive/MyDrive/deepsurv_results/20250829_deepsurv_randomSH_results.csv\n",
            "Best model saved to /content/drive/MyDrive/deepsurv_results/20250829_best_deepsurv_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3173553835.py:251: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  test_df['Adjuvant Chemo'] = test_df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test CI: 0.6550\n",
            "Training completed. Check your log file at: /content/drive/MyDrive/deepsurv_8-28-25_training_log.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "class Tee:\n",
        "    def __init__(self, *files): self.files = files\n",
        "    def write(self, data):\n",
        "        for f in self.files: f.write(data)\n",
        "    def flush(self):\n",
        "        for f in self.files: f.flush()\n",
        "\n",
        "def loguniform(rng, lo, hi):\n",
        "    return float(np.exp(rng.uniform(np.log(lo), np.log(hi))))\n",
        "\n",
        "def sample_hparams(rng):\n",
        "    hidden_options = [[8],[16],[32],[16,8],[32,16]]\n",
        "    layers = hidden_options[rng.integers(len(hidden_options))]\n",
        "    dropout = float(rng.uniform(0.3, 0.7))\n",
        "    lr = loguniform(rng, 3e-5, 3e-4)\n",
        "    wd = loguniform(rng, 1e-3, 1e-2)     # L2 via AdamW weight decay\n",
        "    l1 = loguniform(rng, 1e-6, 1e-4)     # L1 added to loss\n",
        "    return {'layers': layers, 'dropout': dropout, 'lr': lr, 'wd': wd, 'l1': l1}\n",
        "\n",
        "# ---------------- model & data ----------------\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(d, units), activation]\n",
        "            if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "            d = units\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events, dtype=torch.bool)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.time[idx], self.event[idx]\n",
        "\n",
        "def l1_penalty(model):\n",
        "    return sum(p.abs().sum() for n,p in model.named_parameters()\n",
        "               if p.requires_grad and p.dim() > 1)\n",
        "\n",
        "def train_one_epoch(model, optimizer, dataloader, device, l1_lambda=0.0):\n",
        "    model.train()\n",
        "    for x, t, e in dataloader:\n",
        "        if e.sum().item() == 0:  # skip non-informative batch\n",
        "            continue\n",
        "        x, t, e = x.to(device), t.to(device), e.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = neg_partial_log_likelihood(out, e, t, reduction='mean')\n",
        "        if l1_lambda > 0: loss = loss + l1_lambda * l1_penalty(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_ci(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, times, events = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for x, t, e in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = torch.clamp(model(x), -20, 20)\n",
        "            preds.append(y.cpu().numpy().ravel())\n",
        "            times.append(t.numpy()); events.append(e.numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    if np.isnan(preds).any():\n",
        "        print(\"Warning: NaN predictions detected, returning -inf for concordance index\")\n",
        "        return -np.inf\n",
        "    times = np.concatenate(times); events = np.concatenate(events)\n",
        "    return concordance_index_censored(events.astype(bool), times, preds)[0]\n",
        "\n",
        "# ---------------- random search with successive halving ----------------\n",
        "def main():\n",
        "    # logging\n",
        "    original_stdout = sys.stdout\n",
        "    log_path = \"/content/drive/MyDrive/deepsurv_8-28-25_training_log.txt\"\n",
        "    with open(log_path, \"w\") as log_file:\n",
        "        sys.stdout = Tee(original_stdout, log_file)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "        output_dir = \"/content/drive/MyDrive/deepsurv_results\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # ----- load & prep data -----\n",
        "        train_df = pd.read_csv(\"/content/drive/MyDrive/affyTrain.csv\")\n",
        "        valid_df = pd.read_csv(\"/content/drive/MyDrive/affyValidation.csv\")\n",
        "        for df in (train_df, valid_df):\n",
        "            if 'Adjuvant Chemo' in df.columns:\n",
        "                df['Adjuvant Chemo'] = df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n",
        "        binary_columns = ['Adjuvant Chemo','IS_MALE']\n",
        "        for col in binary_columns:\n",
        "            if col in train_df.columns: train_df[col] = train_df[col].astype(int)\n",
        "            if col in valid_df.columns: valid_df[col] = valid_df[col].astype(int)\n",
        "\n",
        "        survival_cols = ['OS_STATUS','OS_MONTHS']\n",
        "        feature_cols = [c for c in train_df.columns if c not in survival_cols]\n",
        "\n",
        "        X_train = train_df[feature_cols].values.astype(np.float32)\n",
        "        X_valid = valid_df[feature_cols].values.astype(np.float32)\n",
        "        scaler = StandardScaler().fit(X_train)\n",
        "        X_train = scaler.transform(X_train).astype(np.float32)\n",
        "        X_valid = scaler.transform(X_valid).astype(np.float32)\n",
        "\n",
        "        y_train_time = train_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_train_event = train_df['OS_STATUS'].values.astype(np.float32)\n",
        "        y_valid_time = valid_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_valid_event = valid_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        train_ds = SurvivalDataset(X_train, y_train_time, y_train_event)\n",
        "        valid_ds = SurvivalDataset(X_valid, y_valid_time, y_valid_event)\n",
        "\n",
        "        batch_size = 32\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)\n",
        "        train_eval_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
        "        valid_loader      = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # ----- search budget & rungs -----\n",
        "        rng = np.random.default_rng(42)\n",
        "        num_trials = 40                     # budget (adjust as needed)\n",
        "        rungs = [10, 30, 60]                # epochs at which we prune\n",
        "        eta = 2                             # keep top 1/eta each rung\n",
        "        patience, min_delta = 10, 1e-4      # early stopping inside trials\n",
        "        PRINT_EVERY = 1\n",
        "\n",
        "        # ----- initialize trials -----\n",
        "        trials = []\n",
        "        for tid in range(num_trials):\n",
        "            hp = sample_hparams(rng)\n",
        "            model = DeepSurvMLP(X_train.shape[1], hp['layers'], dropout=hp['dropout']).to(device)\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=hp['lr'], weight_decay=hp['wd'])\n",
        "            trials.append({\n",
        "                'id': tid, 'hp': hp, 'model': model, 'opt': optimizer,\n",
        "                'best_ci': -np.inf, 'best_state': copy.deepcopy(model.state_dict()),\n",
        "                'hist_train_ci': [], 'hist_val_ci': [], 'epochs_done': 0,\n",
        "                'alive': True, 'no_improve': 0, 'last_val_ci': -np.inf\n",
        "            })\n",
        "\n",
        "        # ----- successive halving loop -----\n",
        "        for rung_idx, rung_ep in enumerate(rungs, start=1):\n",
        "            print(f\"\\n=== Rung {rung_idx}/{len(rungs)} → target {rung_ep} epochs ===\")\n",
        "            # train all alive trials up to rung_ep (with early stopping)\n",
        "            for tr in trials:\n",
        "                if not tr['alive']: continue\n",
        "                target = rung_ep\n",
        "                while tr['epochs_done'] < target:\n",
        "                    train_one_epoch(tr['model'], tr['opt'], train_loader, device, l1_lambda=tr['hp']['l1'])\n",
        "                    tr['epochs_done'] += 1\n",
        "                    # diagnostics each epoch\n",
        "                    tr_ci = evaluate_ci(tr['model'], train_eval_loader, device)\n",
        "                    va_ci = evaluate_ci(tr['model'], valid_loader, device)\n",
        "                    tr['hist_train_ci'].append(tr_ci)\n",
        "                    tr['hist_val_ci'].append(va_ci)\n",
        "\n",
        "                    if tr['epochs_done'] % PRINT_EVERY == 0:\n",
        "                      print(f\"[Trial {tr['id']:02d} | Rung {rung_idx}/{len(rungs)} | \"\n",
        "                            f\"Epoch {tr['epochs_done']:3d}] Train CI {tr_ci:.4f} | \"\n",
        "                            f\"Val CI {va_ci:.4f} | Best {tr['best_ci']:.4f}\")\n",
        "\n",
        "                    # track best\n",
        "                    if va_ci > tr['best_ci'] + min_delta:\n",
        "                        tr['best_ci'] = va_ci\n",
        "                        tr['best_state'] = copy.deepcopy(tr['model'].state_dict())\n",
        "                        tr['no_improve'] = 0\n",
        "                    else:\n",
        "                        tr['no_improve'] += 1\n",
        "                        if tr['no_improve'] >= patience:\n",
        "                            print(f\"Trial {tr['id']} early-stopped at epoch {tr['epochs_done']} (best Val CI={tr['best_ci']:.4f})\")\n",
        "                            break\n",
        "\n",
        "            # prune: keep top 1/eta among alive\n",
        "            alive = [tr for tr in trials if tr['alive']]\n",
        "            alive.sort(key=lambda z: z['best_ci'], reverse=True)\n",
        "            keep_n = max(1, math.ceil(len(alive) / eta))\n",
        "            survivors = set(tr['id'] for tr in alive[:keep_n])\n",
        "\n",
        "            print(f\"Alive before prune: {len(alive)}; keeping top {keep_n}\")\n",
        "            for tr in alive:\n",
        "                if tr['id'] not in survivors:\n",
        "                    tr['alive'] = False\n",
        "                    # free memory\n",
        "                    del tr['model']; tr['model'] = None\n",
        "                    tr['opt'] = None\n",
        "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "        # ----- select best trial -----\n",
        "        best_trial = max(trials, key=lambda z: z['best_ci'])\n",
        "        best_hp = best_trial['hp']\n",
        "        print(\"\\nBest Hyperparameters:\")\n",
        "        print(best_hp)\n",
        "        print(\"Best Validation CI:\", best_trial['best_ci'])\n",
        "\n",
        "        # ----- save plots per trial -----\n",
        "        out_date = current_date\n",
        "        for tr in trials:\n",
        "            cfg = f\"trial{tr['id']}_layers-{'-'.join(map(str, tr['hp']['layers']))}_drop{tr['hp']['dropout']:.2f}_lr{tr['hp']['lr']:.2e}_wd{tr['hp']['wd']:.2e}_l1{tr['hp']['l1']:.2e}\"\n",
        "            epochs = range(1, len(tr['hist_train_ci'])+1)\n",
        "            plt.figure()\n",
        "            plt.plot(epochs, tr['hist_train_ci'], label='Train CI')\n",
        "            plt.plot(epochs, tr['hist_val_ci'], label='Val CI')\n",
        "            plt.xlabel('Epoch'); plt.ylabel('Concordance Index'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "            plt.title(cfg)\n",
        "            plt.ylim(0.4, 1.0)\n",
        "            plot_path = os.path.join(output_dir, f\"{out_date}_ci_{cfg}.png\")\n",
        "            plt.savefig(plot_path, dpi=150, bbox_inches='tight'); plt.close()\n",
        "            print(f\"Saved CI plot to {plot_path}\")\n",
        "\n",
        "        # ----- save results table -----\n",
        "        results = []\n",
        "        for tr in trials:\n",
        "            row = {'trial_id': tr['id'], 'val_ci': tr['best_ci'], 'epochs_trained': len(tr['hist_val_ci']),\n",
        "                   'alive_final': tr['alive']}\n",
        "            row.update({ 'layers': '-'.join(map(str, tr['hp']['layers'])),\n",
        "                         'dropout': tr['hp']['dropout'], 'lr': tr['hp']['lr'],\n",
        "                         'weight_decay(L2)': tr['hp']['wd'], 'l1_lambda': tr['hp']['l1']})\n",
        "            results.append(row)\n",
        "        df = pd.DataFrame(results).sort_values('val_ci', ascending=False)\n",
        "        csv_path = os.path.join(output_dir, f\"{out_date}_deepsurv_randomSH_results.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Hyperparameter search results saved to {csv_path}\")\n",
        "\n",
        "        # ----- save best model -----\n",
        "        best_model_path = os.path.join(output_dir, f\"{out_date}_best_deepsurv_model.pth\")\n",
        "        torch.save(best_trial['best_state'], best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "\n",
        "        # ----- test evaluation -----\n",
        "        test_df = pd.read_csv(\"/content/drive/MyDrive/affyTest.csv\")\n",
        "        if 'Adjuvant Chemo' in test_df.columns:\n",
        "            test_df['Adjuvant Chemo'] = test_df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n",
        "        for col in binary_columns:\n",
        "            if col in test_df.columns: test_df[col] = test_df[col].astype(int)\n",
        "        X_test = scaler.transform(test_df[feature_cols].values.astype(np.float32)).astype(np.float32)\n",
        "        y_test_time = test_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_test_event = test_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        test_loader = DataLoader(SurvivalDataset(X_test, y_test_time, y_test_event),\n",
        "                                 batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # rebuild & load best\n",
        "        final_model = DeepSurvMLP(X_train.shape[1], best_hp['layers'], dropout=best_hp['dropout']).to(device)\n",
        "        final_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "        test_ci = evaluate_ci(final_model, test_loader, device)\n",
        "        print(f\"Test CI: {test_ci:.4f}\")\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    sys.stdout = original_stdout\n",
        "    print(\"Training completed. Check your log file at:\", log_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}