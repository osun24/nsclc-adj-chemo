{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1EY87_PKv0dTOF6mK_Kx8YPzRYVzJ6o-8",
      "authorship_tag": "ABX9TyPq2+lX7ZsHMBgp+qGXorAf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osun24/nsclc-adj-chemo/blob/main/TorchSurv_DeepSurv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torchsurv scikit-survival\n",
        "\n",
        "# Import required packages\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "\n",
        "# (Optional) Mount Google Drive if you plan to load/save files there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81TEvMle6mqs",
        "outputId": "5922c44e-5f8d-4376-cdee-9ae80a04edb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsurv\n",
            "  Downloading torchsurv-0.1.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting scikit-survival\n",
            "  Downloading scikit_survival-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.0.2)\n",
            "Collecting torchmetrics (from torchsurv)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ecos (from scikit-survival)\n",
            "  Downloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.12.1)\n",
            "Collecting osqp<1.0.0,>=0.6.3 (from scikit-survival)\n",
            "  Downloading osqp-0.6.7.post3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.6.1)\n",
            "Collecting qdldl (from osqp<1.0.0,>=0.6.3->scikit-survival)\n",
            "  Downloading qdldl-0.1.7.post5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8,>=1.6.1->scikit-survival) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.4.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (25.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->torchsurv)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchsurv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchsurv) (3.0.2)\n",
            "Downloading torchsurv-0.1.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_survival-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading osqp-0.6.7.post3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.1/222.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading qdldl-0.1.7.post5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, qdldl, ecos, osqp, torchmetrics, scikit-survival, torchsurv\n",
            "  Attempting uninstall: osqp\n",
            "    Found existing installation: osqp 1.0.4\n",
            "    Uninstalling osqp-1.0.4:\n",
            "      Successfully uninstalled osqp-1.0.4\n",
            "Successfully installed ecos-2.0.14 lightning-utilities-0.15.2 osqp-0.6.7.post3 qdldl-0.1.7.post5 scikit-survival-0.25.0 torchmetrics-1.8.2 torchsurv-0.1.5\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "class Tee:\n",
        "    def __init__(self, *files): self.files = files\n",
        "    def write(self, data):\n",
        "        for f in self.files: f.write(data)\n",
        "    def flush(self):\n",
        "        for f in self.files: f.flush()\n",
        "\n",
        "def loguniform(rng, lo, hi):\n",
        "    return float(np.exp(rng.uniform(np.log(lo), np.log(hi))))\n",
        "\n",
        "# saner anchors (include 0 for L1); WD in 1e-6–1e-2-ish, L1 in 0–1e-3-ish\n",
        "wd_anchors = np.array([0.0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 3e-3, 1e-2])\n",
        "l1_anchors = np.array([0.0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 3e-4, 5e-4, 7e-4, 1e-3])\n",
        "\n",
        "def sample_hparams(rng):\n",
        "    # leaner nets for small, censored data\n",
        "    hidden_options = [[64], [128], [64, 64], [128, 64], [256]]\n",
        "    layers = hidden_options[rng.integers(len(hidden_options))]\n",
        "\n",
        "    # milder dropout\n",
        "    dropout = float(rng.uniform(0.10, 0.40))\n",
        "\n",
        "    # slightly higher but still conservative LR (use AdamW)\n",
        "    lr = loguniform(rng, 3e-5, 5e-4)\n",
        "\n",
        "    r = rng.random()\n",
        "    if r < 0.25:\n",
        "        wd = float(rng.choice(wd_anchors))\n",
        "        l1 = float(rng.choice(l1_anchors))\n",
        "    elif r < 0.85:\n",
        "        # common case\n",
        "        wd = loguniform(rng, 1e-6, 3e-3)   # 1e-6–3e-3\n",
        "        l1 = loguniform(rng, 0.0 + 1e-9, 7e-4)  # ~0–7e-4\n",
        "    else:\n",
        "        # explore slightly stronger reg (still safe)\n",
        "        wd = loguniform(rng, 3e-3, 1e-2)   # 0.003–0.01\n",
        "        l1 = loguniform(rng, 7e-4, 1e-3)   # 7e-4–1e-3\n",
        "\n",
        "    return {'layers': layers, 'dropout': dropout, 'lr': lr, 'wd': wd, 'l1': l1}\n",
        "\n",
        "# ---------------- model & data ----------------\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(d, units), activation]\n",
        "            if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "            d = units\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events, dtype=torch.bool)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.time[idx], self.event[idx]\n",
        "\n",
        "# ---- (1) Event-balanced batch sampler: guarantee ≥1 event per batch ----\n",
        "class EventBalancedBatchSampler(Sampler):\n",
        "    def __init__(self, events_numpy, batch_size, seed=0):\n",
        "        events = np.asarray(events_numpy).astype(bool)\n",
        "        self.pos_idx = np.where(events)[0]\n",
        "        self.neg_idx = np.where(~events)[0]\n",
        "        assert len(self.pos_idx) > 0, \"No events in training set — cannot balance batches.\"\n",
        "        self.bs = int(batch_size)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # one finite pass == one epoch\n",
        "        pos = self.rng.permutation(self.pos_idx)\n",
        "        neg = self.rng.permutation(self.neg_idx)\n",
        "        n_total = len(pos) + len(neg)\n",
        "        n_batches = math.ceil(n_total / self.bs)\n",
        "\n",
        "        pi = ni = 0\n",
        "        for _ in range(n_batches):\n",
        "            take_pos = 1 if pi < len(pos) else 0\n",
        "            # fill the rest with negatives if we can\n",
        "            avail_neg = max(0, len(neg) - ni)\n",
        "            take_neg = min(self.bs - take_pos, avail_neg)\n",
        "\n",
        "            # if we ran out of negatives, top up with extra positives\n",
        "            need = self.bs - (take_pos + take_neg)\n",
        "            extra_pos = min(need, max(0, len(pos) - (pi + take_pos)))\n",
        "            take_pos += extra_pos\n",
        "\n",
        "            batch = np.concatenate([\n",
        "                pos[pi:pi+take_pos],\n",
        "                neg[ni:ni+take_neg]\n",
        "            ])\n",
        "            pi += take_pos\n",
        "            ni += take_neg\n",
        "\n",
        "            if batch.size == 0:\n",
        "                break\n",
        "            self.rng.shuffle(batch)\n",
        "            yield batch.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil((len(self.pos_idx) + len(self.neg_idx)) / self.bs)\n",
        "\n",
        "# ---- Param groups: L2 only on non-bias, non-final weights ----\n",
        "def make_optimizer(model, lr, wd):\n",
        "    # find last Linear\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight):\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "\n",
        "    param_groups = [\n",
        "        {'params': decay, 'weight_decay': wd},\n",
        "        {'params': no_decay, 'weight_decay': 0.0},\n",
        "    ]\n",
        "    return optim.AdamW(param_groups, lr=lr)\n",
        "\n",
        "# ---- (2) Reg warm-up helpers (dropout + WD) ----\n",
        "def set_dropout_p(model, p):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout):\n",
        "            m.p = float(p)\n",
        "\n",
        "def set_weight_decay(optimizer, wd):\n",
        "    for g in optimizer.param_groups:\n",
        "        g['weight_decay'] = float(wd)\n",
        "\n",
        "# L1 ONLY on the first (input) Linear layer (already warmed up in train loop)\n",
        "def l1_penalty_first_layer(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            return m.weight.abs().sum()\n",
        "    return torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "\n",
        "def train_one_epoch(model, optimizer, dataloader, device, l1_lambda=0.0, epoch=0, warmup_epochs=20):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))  # linear warmup of L1\n",
        "    loss_sum, n_seen = 0.0, 0\n",
        "    skipped, total_batches = 0, 0\n",
        "    for x, t, e in dataloader:\n",
        "        total_batches += 1\n",
        "        # with balanced sampler, this should almost never trigger, but keep it safe:\n",
        "        if e.sum().item() == 0:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        x, t, e = x.to(device), t.to(device), e.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = neg_partial_log_likelihood(out, e, t, reduction='mean')\n",
        "        if l1_lambda > 0:\n",
        "            loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        n_seen += x.size(0)\n",
        "    avg_loss = loss_sum / max(n_seen, 1)\n",
        "    skip_frac = skipped / max(total_batches, 1)\n",
        "    return {'avg_loss': avg_loss, 'skip_frac': skip_frac, 'warm': warm}\n",
        "\n",
        "# ---- (3) Full-risk-set correction step (1x per epoch) ----\n",
        "def full_risk_set_step(model, optimizer, train_ds, device, l1_lambda=0.0, warm=1.0):\n",
        "    model.train()\n",
        "    X_all = train_ds.x.to(device)\n",
        "    t_all = train_ds.time.to(device)\n",
        "    e_all = train_ds.event.to(device)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(X_all), -20, 20)\n",
        "    loss_full = neg_partial_log_likelihood(out_all, e_all, t_all, reduction='mean')\n",
        "    if l1_lambda > 0:\n",
        "        loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "def evaluate_ci(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, times, events = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for x, t, e in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = torch.clamp(model(x), -20, 20)\n",
        "            preds.append(y.cpu().numpy().ravel())\n",
        "            times.append(t.numpy()); events.append(e.numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    if np.isnan(preds).any():\n",
        "        print(\"Warning: NaN predictions detected, returning -inf for concordance index\")\n",
        "        return -np.inf\n",
        "    times = np.concatenate(times); events = np.concatenate(events)\n",
        "    return concordance_index_censored(events.astype(bool), times, preds)[0]\n",
        "\n",
        "def count_params(in_dim, hidden_layers):\n",
        "    params, d = 0, in_dim\n",
        "    for h in hidden_layers:\n",
        "        params += d*h + h  # weights + bias\n",
        "        d = h\n",
        "    params += d*1 + 1     # final layer\n",
        "    return params\n",
        "\n",
        "# --- Bootstrap SE on validation C-index for a trial's best checkpoint ---\n",
        "def _bootstrap_val_ci_for_trial(tr, X_val, t_val, e_val, in_dim, device, B=150, seed=123):\n",
        "    if tr['best_state'] is None:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    hp = tr['hp']\n",
        "    # rebuild model at best checkpoint\n",
        "    m = DeepSurvMLP(in_dim, hp['layers'], dropout=hp['dropout']).to(device)\n",
        "    m.load_state_dict(tr['best_state'])\n",
        "    m.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = m(torch.tensor(X_val, dtype=torch.float32, device=device)).cpu().numpy().ravel()\n",
        "    n = len(t_val)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    boot = []\n",
        "    for _ in range(B):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        boot.append(concordance_index_censored(e_val[idx].astype(bool), t_val[idx], preds[idx])[0])\n",
        "    boot = np.asarray(boot, dtype=float)\n",
        "    return float(np.nanmean(boot)), float(np.nanstd(boot, ddof=1)), float(np.nanpercentile(boot, 5))\n",
        "\n",
        "# ---------------- random search with successive halving ----------------\n",
        "def main():\n",
        "    # logging\n",
        "    original_stdout = sys.stdout\n",
        "    log_path = \"/content/drive/MyDrive/deepsurv_10-2-25_training_log_fRMA.txt\"\n",
        "    with open(log_path, \"w\") as log_file:\n",
        "        sys.stdout = Tee(original_stdout, log_file)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "        output_dir = \"/content/drive/MyDrive/deepsurv_results\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # ----- load & prep data -----\n",
        "        train_df = pd.read_csv(\"/content/drive/MyDrive/affyTrainFrozen.csv\")\n",
        "        valid_df = pd.read_csv(\"/content/drive/MyDrive/affyValidationFrozen.csv\")\n",
        "\n",
        "        # keep only stage IA or IB even if one-hot encoded\n",
        "        def keep_ia_ib(df):\n",
        "            if {'Stage_IA','Stage_IB'}.issubset(df.columns):\n",
        "                df = df[(df['Stage_IA'] == 1) | (df['Stage_IB'] == 1)].copy()\n",
        "                df.drop(columns=[c for c in df.columns if c.startswith('Stage_') and c not in {'Stage_IA','Stage_IB'}],\n",
        "                        inplace=True)\n",
        "            elif 'Stage' in df.columns:\n",
        "                df = df[df['Stage'].isin(['IA','IB'])].copy()\n",
        "            return df\n",
        "\n",
        "        train_df = keep_ia_ib(train_df)\n",
        "        valid_df = keep_ia_ib(valid_df)\n",
        "\n",
        "        for df in (train_df, valid_df):\n",
        "            if 'Adjuvant Chemo' in df.columns:\n",
        "                df['Adjuvant Chemo'] = df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n",
        "        binary_columns = ['Adjuvant Chemo','IS_MALE']\n",
        "        for col in binary_columns:\n",
        "            if col in train_df.columns: train_df[col] = train_df[col].astype(int)\n",
        "            if col in valid_df.columns: valid_df[col] = valid_df[col].astype(int)\n",
        "\n",
        "\n",
        "\n",
        "        survival_cols = ['OS_STATUS','OS_MONTHS']\n",
        "        feature_cols = [c for c in train_df.columns if c not in survival_cols]\n",
        "\n",
        "        X_train = train_df[feature_cols].values.astype(np.float32)\n",
        "        X_valid = valid_df[feature_cols].values.astype(np.float32)\n",
        "        scaler = StandardScaler().fit(X_train)\n",
        "        X_train = scaler.transform(X_train).astype(np.float32)\n",
        "        X_valid = scaler.transform(X_valid).astype(np.float32)\n",
        "\n",
        "        y_train_time = train_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_train_event = train_df['OS_STATUS'].values.astype(np.float32)\n",
        "        y_valid_time = valid_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_valid_event = valid_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        train_ds = SurvivalDataset(X_train, y_train_time, y_train_event)\n",
        "        valid_ds = SurvivalDataset(X_valid, y_valid_time, y_valid_event)\n",
        "\n",
        "        BATCH_SIZE = 64\n",
        "\n",
        "        # (1) swap in the event-balanced sampler\n",
        "        train_sampler = EventBalancedBatchSampler(y_train_event, BATCH_SIZE, seed=42)\n",
        "        train_loader  = DataLoader(train_ds, batch_sampler=train_sampler)\n",
        "        # evaluation loaders remain standard\n",
        "        train_eval_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        valid_loader      = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # ----- search budget & rungs -----\n",
        "        rng = np.random.default_rng(42)\n",
        "        num_trials = 100\n",
        "        rungs = [16, 64, 160, 320]  # keep your longer rungs\n",
        "        eta = 3\n",
        "        min_delta = 1e-4\n",
        "        PRINT_EVERY = 1\n",
        "\n",
        "        # Warm-ups\n",
        "        L1_WARMUP_EPOCHS = 30\n",
        "        WD_WARMUP_EPOCHS = 30\n",
        "        DROPOUT_WARMUP_EPOCHS = 30\n",
        "        DROPOUT_START = 0.15\n",
        "        WD_START = 0.0\n",
        "\n",
        "        # ΔVal-CI moving-average window\n",
        "        DELTA_CI_MA_K = 10\n",
        "\n",
        "        # ----- initialize trials -----\n",
        "        trials = []\n",
        "        for tid in range(num_trials):\n",
        "            hp = sample_hparams(rng)\n",
        "            model = DeepSurvMLP(X_train.shape[1], hp['layers'], dropout=hp['dropout']).to(device)\n",
        "            optimizer = make_optimizer(model, lr=hp['lr'], wd=hp['wd'])\n",
        "            trials.append({\n",
        "                'id': tid, 'hp': hp, 'model': model, 'opt': optimizer, 'sched': None, 'sched_target': 0,\n",
        "                'best_ci': -np.inf, 'best_state': copy.deepcopy(model.state_dict()),\n",
        "                'best_epoch': 0, 'best_gap': np.inf, 'best_gap_abs': np.inf,\n",
        "                'train_ci_at_best': np.nan, 'val_ci_at_best': np.nan,\n",
        "                'hist_train_ci': [], 'hist_val_ci': [], 'hist_gap': [],\n",
        "                'hist_loss': [], 'hist_skip': [],\n",
        "                'hist_val_ci_delta': [], 'hist_val_ci_delta_ma': [],\n",
        "                'epochs_done': 0, 'alive': True, 'no_improve': 0\n",
        "            })\n",
        "\n",
        "        # ----- successive halving loop -----\n",
        "        prev_target = 0\n",
        "        for rung_idx, rung_ep in enumerate(rungs, start=1):\n",
        "            print(f\"\\n=== Rung {rung_idx}/{len(rungs)} → target {rung_ep} epochs ===\")\n",
        "\n",
        "            span = rung_ep - prev_target\n",
        "            es_patience = float('inf') if rung_idx == 1 else max(5, int(0.25 * span))\n",
        "\n",
        "            for tr in trials:\n",
        "                if not tr['alive']: continue\n",
        "                steps_remaining = rung_ep - tr['epochs_done']\n",
        "                if steps_remaining <= 0:\n",
        "                    continue\n",
        "\n",
        "                tr['sched'] = torch.optim.lr_scheduler.CosineAnnealingLR(tr['opt'], T_max=steps_remaining)\n",
        "                tr['sched_target'] = rung_ep\n",
        "\n",
        "                while tr['epochs_done'] < rung_ep:\n",
        "                    # (2) apply dropout + weight-decay warm-up schedules\n",
        "                    frac_d = min(1.0, tr['epochs_done'] / float(DROPOUT_WARMUP_EPOCHS))\n",
        "                    frac_w = min(1.0, tr['epochs_done'] / float(WD_WARMUP_EPOCHS))\n",
        "                    p_t = DROPOUT_START + (tr['hp']['dropout'] - DROPOUT_START) * frac_d\n",
        "                    wd_t = WD_START + (tr['hp']['wd'] - WD_START) * frac_w\n",
        "                    set_dropout_p(tr['model'], p_t)\n",
        "                    set_weight_decay(tr['opt'], wd_t)\n",
        "\n",
        "                    # normal mini-batch training epoch\n",
        "                    stats = train_one_epoch(\n",
        "                        tr['model'], tr['opt'], train_loader, device,\n",
        "                        l1_lambda=tr['hp']['l1'], epoch=tr['epochs_done'], warmup_epochs=L1_WARMUP_EPOCHS\n",
        "                    )\n",
        "                    # (3) one full-risk-set correction step per epoch\n",
        "                    full_loss = full_risk_set_step(\n",
        "                        tr['model'], tr['opt'], train_ds, device,\n",
        "                        l1_lambda=tr['hp']['l1'], warm=stats['warm']\n",
        "                    )\n",
        "\n",
        "                    tr['epochs_done'] += 1\n",
        "\n",
        "                    # diagnostics each epoch\n",
        "                    tr_ci = evaluate_ci(tr['model'], train_eval_loader, device)\n",
        "                    va_ci = evaluate_ci(tr['model'], valid_loader, device)\n",
        "                    gap = tr_ci - va_ci  # overfitting gap (Train − Val)\n",
        "\n",
        "                    tr['hist_train_ci'].append(tr_ci)\n",
        "                    tr['hist_val_ci'].append(va_ci)\n",
        "                    tr['hist_gap'].append(gap)\n",
        "                    tr['hist_loss'].append(stats['avg_loss'])\n",
        "                    tr['hist_skip'].append(stats['skip_frac'])\n",
        "\n",
        "                    # ΔVal-CI and its moving average\n",
        "                    if len(tr['hist_val_ci']) >= 2:\n",
        "                        delta = tr['hist_val_ci'][-1] - tr['hist_val_ci'][-2]\n",
        "                    else:\n",
        "                        delta = 0.0\n",
        "                    tr['hist_val_ci_delta'].append(delta)\n",
        "                    ma = float(np.mean(tr['hist_val_ci_delta'][-DELTA_CI_MA_K:]))\n",
        "                    tr['hist_val_ci_delta_ma'].append(ma)\n",
        "\n",
        "                    if tr['sched'] is not None:\n",
        "                        tr['sched'].step()\n",
        "\n",
        "                    if tr['epochs_done'] % PRINT_EVERY == 0:\n",
        "                        print(f\"[Trial {tr['id']:02d} | Rung {rung_idx}/{len(rungs)} | \"\n",
        "                              f\"Epoch {tr['epochs_done']:3d}] \"\n",
        "                              f\"Loss {stats['avg_loss']:.4f} | FullStepLoss {full_loss:.4f} | \"\n",
        "                              f\"Skip% {100*stats['skip_frac']:.1f} | \"\n",
        "                              f\"Train CI {tr_ci:.4f} | Val CI {va_ci:.4f} | \"\n",
        "                              f\"Gap(T−V) {gap:+.4f} | \"\n",
        "                              f\"ΔVal-CI MA({DELTA_CI_MA_K}) {ma:+.4f} | Best {tr['best_ci']:.4f}\")\n",
        "\n",
        "                    # track best with tie/close-call handling using smallest |gap|\n",
        "                    if va_ci > tr['best_ci'] + min_delta:\n",
        "                        tr['best_ci'] = va_ci\n",
        "                        tr['best_state'] = copy.deepcopy(tr['model'].state_dict())\n",
        "                        tr['best_epoch'] = tr['epochs_done']\n",
        "                        tr['best_gap'] = gap\n",
        "                        tr['best_gap_abs'] = abs(gap)\n",
        "                        tr['train_ci_at_best'] = tr_ci\n",
        "                        tr['val_ci_at_best'] = va_ci\n",
        "                        tr['no_improve'] = 0\n",
        "                    else:\n",
        "                        # if tied/close (within min_delta), prefer epoch with smaller |gap|\n",
        "                        if abs(va_ci - tr['best_ci']) <= min_delta and abs(gap) < tr['best_gap_abs'] - 1e-12:\n",
        "                            tr['best_state'] = copy.deepcopy(tr['model'].state_dict())\n",
        "                            tr['best_epoch'] = tr['epochs_done']\n",
        "                            tr['best_gap'] = gap\n",
        "                            tr['best_gap_abs'] = abs(gap)\n",
        "                            tr['train_ci_at_best'] = tr_ci\n",
        "                            tr['val_ci_at_best'] = va_ci\n",
        "                        tr['no_improve'] += 1\n",
        "                        if tr['no_improve'] >= es_patience:\n",
        "                            print(f\"Trial {tr['id']} early-stopped at epoch {tr['epochs_done']} \"\n",
        "                                  f\"(best Val CI={tr['best_ci']:.4f})\")\n",
        "                            break\n",
        "\n",
        "            # prune: keep top 1/eta among alive\n",
        "            alive = [tr for tr in trials if tr['alive']]\n",
        "            alive.sort(key=lambda z: z['best_ci'], reverse=True)\n",
        "            keep_n = max(1, math.ceil(len(alive) / eta))\n",
        "            survivors = set(tr['id'] for tr in alive[:keep_n])\n",
        "\n",
        "            print(f\"Alive before prune: {len(alive)}; keeping top {keep_n}\")\n",
        "            for tr in alive:\n",
        "                if tr['id'] not in survivors:\n",
        "                    tr['alive'] = False\n",
        "                    # free memory\n",
        "                    del tr['model']; tr['model'] = None\n",
        "                    tr['opt'] = None; tr['sched'] = None\n",
        "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "            prev_target = rung_ep\n",
        "\n",
        "        # ----- save plots per trial -----\n",
        "        out_date = current_date\n",
        "        for tr in trials:\n",
        "            cfg = (f\"trial{tr['id']}_layers-{'-'.join(map(str, tr['hp']['layers']))}\"\n",
        "                   f\"_drop{tr['hp']['dropout']:.2f}_lr{tr['hp']['lr']:.2e}\"\n",
        "                   f\"_wd{tr['hp']['wd']:.2e}_l1{tr['hp']['l1']:.2e}\")\n",
        "            epochs = range(1, len(tr['hist_train_ci'])+1)\n",
        "            plt.figure()\n",
        "            plt.plot(epochs, tr['hist_train_ci'], label='Train CI')\n",
        "            plt.plot(epochs, tr['hist_val_ci'], label='Val CI')\n",
        "            # Optional: plot ΔVal-CI MA on a secondary axis as trend\n",
        "            ax = plt.gca()\n",
        "            ax2 = ax.twinx()\n",
        "            ax2.plot(epochs, tr['hist_val_ci_delta_ma'], linestyle='--', alpha=0.5, label=f'ΔVal-CI MA')\n",
        "            ax.set_xlabel('Epoch'); ax.set_ylabel('Concordance Index')\n",
        "            ax2.set_ylabel('ΔVal-CI MA')\n",
        "            lines, labels = ax.get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax.legend(lines + lines2, labels + labels2, loc='lower right')\n",
        "            plt.grid(True, alpha=0.3); plt.title(cfg)\n",
        "            ax.set_ylim(0.4, 1.0)  # constant y-scale across plots\n",
        "            plot_path = os.path.join(output_dir, f\"{out_date}_ci_{cfg}.png\")\n",
        "            plt.savefig(plot_path, dpi=150, bbox_inches='tight'); plt.close()\n",
        "            print(f\"Saved CI plot to {plot_path}\")\n",
        "\n",
        "        # ----- build results table (needed before 1-SE selection) -----\n",
        "        results = []\n",
        "        for tr in trials:\n",
        "            row = {\n",
        "                'trial_id': tr['id'],\n",
        "                'val_ci': tr['best_ci'],\n",
        "                'best_epoch': tr.get('best_epoch', np.nan),\n",
        "                'overfit_gap_at_best': tr.get('best_gap', np.nan),     # Train−Val at chosen epoch\n",
        "                'train_ci_at_best': tr.get('train_ci_at_best', np.nan),\n",
        "                'val_ci_at_best': tr.get('val_ci_at_best', np.nan),\n",
        "                'epochs_trained': len(tr['hist_val_ci']),\n",
        "                'alive_final': tr['alive'],\n",
        "                'avg_loss_last': tr['hist_loss'][-1] if tr['hist_loss'] else np.nan,\n",
        "                'skip_frac_last': tr['hist_skip'][-1] if tr['hist_skip'] else np.nan,\n",
        "                'val_ci_ma10_last': tr['hist_val_ci_delta_ma'][-1] if tr['hist_val_ci_delta_ma'] else np.nan\n",
        "            }\n",
        "            row.update({\n",
        "                'layers': '-'.join(map(str, tr['hp']['layers'])),\n",
        "                'dropout': tr['hp']['dropout'], 'lr': tr['hp']['lr'],\n",
        "                'weight_decay(L2)': tr['hp']['wd'], 'l1_lambda': tr['hp']['l1'],\n",
        "                'param_count': count_params(X_train.shape[1], tr['hp']['layers'])\n",
        "            })\n",
        "            results.append(row)\n",
        "\n",
        "        df = pd.DataFrame(results).sort_values('val_ci', ascending=False)\n",
        "\n",
        "        # --- 1-SE + simplicity selection on the validation set (no nested CV) ---\n",
        "        boot_rows = []\n",
        "        for tr in trials:\n",
        "            mu, se, p05 = _bootstrap_val_ci_for_trial(\n",
        "                tr, X_valid, y_valid_time, y_valid_event, X_train.shape[1], device, B=150, seed=123\n",
        "            )\n",
        "            boot_rows.append({'trial_id': tr['id'], 'boot_mean_val_ci': mu, 'boot_se_val_ci': se, 'boot_p05_val_ci': p05})\n",
        "\n",
        "        boot_df = pd.DataFrame(boot_rows)\n",
        "        df = df.merge(boot_df, on='trial_id', how='left')\n",
        "\n",
        "        # 1-SE threshold\n",
        "        best_mu_idx = df['boot_mean_val_ci'].idxmax()\n",
        "        best_mu = df.loc[best_mu_idx, 'boot_mean_val_ci']\n",
        "        best_se = df.loc[best_mu_idx, 'boot_se_val_ci']\n",
        "        one_se_threshold = best_mu - best_se\n",
        "\n",
        "        # Candidates within 1-SE; prefer simpler (param_count), then smaller |gap|, then higher pessimistic bound\n",
        "        df['abs_gap'] = df['overfit_gap_at_best'].abs()\n",
        "        cands = df[df['boot_mean_val_ci'] >= one_se_threshold].copy()\n",
        "        one_se_pick = cands.sort_values(['param_count', 'abs_gap', 'boot_p05_val_ci'],\n",
        "                                        ascending=[True, True, False]).iloc[0]\n",
        "\n",
        "        print(\"\\n[Selection] 1-SE threshold:\", one_se_threshold)\n",
        "        print(\"[Selection] 1-SE pick:\",\n",
        "              dict(one_se_pick[['trial_id','boot_mean_val_ci','boot_se_val_ci','param_count','overfit_gap_at_best','boot_p05_val_ci']]))\n",
        "\n",
        "        # Use the chosen trial as the final model\n",
        "        selected_id = int(one_se_pick['trial_id'])\n",
        "        best_trial = next(tr for tr in trials if tr['id'] == selected_id)\n",
        "        best_hp = best_trial['hp']\n",
        "\n",
        "        # (optional) annotate CSV\n",
        "        df['selected_via'] = (df['trial_id'] == selected_id).astype(int)\n",
        "        csv_path = os.path.join(output_dir, f\"{current_date}_deepsurv_randomSH_results.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Hyperparameter search results saved to {csv_path}\")\n",
        "\n",
        "        # ----- save best model -----\n",
        "        best_model_path = os.path.join(output_dir, f\"{out_date}_best_deepsurv_model.pth\")\n",
        "        torch.save(best_trial['best_state'], best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "\n",
        "        # ----- test evaluation -----\n",
        "        test_df = pd.read_csv(\"/content/drive/MyDrive/affyTestFrozen.csv\")\n",
        "        test_df = keep_ia_ib(test_df)\n",
        "        if 'Adjuvant Chemo' in test_df.columns:\n",
        "            test_df['Adjuvant Chemo'] = test_df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n",
        "        for col in binary_columns:\n",
        "            if col in test_df.columns: test_df[col] = test_df[col].astype(int)\n",
        "        X_test = scaler.transform(test_df[feature_cols].values.astype(np.float32)).astype(np.float32)\n",
        "        y_test_time = test_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_test_event = test_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        test_loader = DataLoader(SurvivalDataset(X_test, y_test_time, y_test_event),\n",
        "                                 batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # rebuild & load best\n",
        "        final_model = DeepSurvMLP(X_train.shape[1], best_hp['layers'], dropout=best_hp['dropout']).to(device)\n",
        "        final_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "        test_ci = evaluate_ci(final_model, test_loader, device)\n",
        "        print(f\"Test CI: {test_ci:.4f}\")\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    sys.stdout = original_stdout\n",
        "    print(\"Training completed. Check your log file at:\", log_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "lbcrdoE4tyWq",
        "outputId": "1d2eda48-8fae-408a-a923-2ceb1e7da96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchsurv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2201863250.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsurv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneg_partial_log_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msksurv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcordance_index_censored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsurv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SINGLE MODEL**"
      ],
      "metadata": {
        "id": "tBOrUYccGuQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ---------------- utils ----------------\n",
        "class Tee:\n",
        "    def __init__(self, *files): self.files = files\n",
        "    def write(self, data):\n",
        "        for f in self.files: f.write(data)\n",
        "    def flush(self):\n",
        "        for f in self.files: f.flush()\n",
        "\n",
        "def loguniform(rng, lo, hi):\n",
        "    return float(np.exp(rng.uniform(np.log(lo), np.log(hi))))\n",
        "\n",
        "# ---------------- model & data ----------------\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(d, units), activation]\n",
        "            if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "            d = units\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events, dtype=torch.bool)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.time[idx], self.event[idx]\n",
        "\n",
        "# ---- (1) Event-balanced batch sampler: guarantee ≥1 event per batch ----\n",
        "class EventBalancedBatchSampler(Sampler):\n",
        "    def __init__(self, events_numpy, batch_size, seed=0):\n",
        "        events = np.asarray(events_numpy).astype(bool)\n",
        "        self.pos_idx = np.where(events)[0]\n",
        "        self.neg_idx = np.where(~events)[0]\n",
        "        assert len(self.pos_idx) > 0, \"No events in training set — cannot balance batches.\"\n",
        "        self.bs = int(batch_size)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # one finite pass == one epoch\n",
        "        pos = self.rng.permutation(self.pos_idx)\n",
        "        neg = self.rng.permutation(self.neg_idx)\n",
        "        n_total = len(pos) + len(neg)\n",
        "        n_batches = math.ceil(n_total / self.bs)\n",
        "\n",
        "        pi = ni = 0\n",
        "        for _ in range(n_batches):\n",
        "            take_pos = 1 if pi < len(pos) else 0\n",
        "            # fill the rest with negatives if we can\n",
        "            avail_neg = max(0, len(neg) - ni)\n",
        "            take_neg = min(self.bs - take_pos, avail_neg)\n",
        "\n",
        "            # if we ran out of negatives, top up with extra positives\n",
        "            need = self.bs - (take_pos + take_neg)\n",
        "            extra_pos = min(need, max(0, len(pos) - (pi + take_pos)))\n",
        "            take_pos += extra_pos\n",
        "\n",
        "            batch = np.concatenate([\n",
        "                pos[pi:pi+take_pos],\n",
        "                neg[ni:ni+take_neg]\n",
        "            ])\n",
        "            pi += take_pos\n",
        "            ni += take_neg\n",
        "\n",
        "            if batch.size == 0:\n",
        "                break\n",
        "            self.rng.shuffle(batch)\n",
        "            yield batch.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil((len(self.pos_idx) + len(self.neg_idx)) / self.bs)\n",
        "\n",
        "# ---- Param groups: L2 only on non-bias, non-final weights ----\n",
        "def make_optimizer(model, lr, wd):\n",
        "    # find last Linear\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight):\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "\n",
        "    param_groups = [\n",
        "        {'params': decay, 'weight_decay': wd},\n",
        "        {'params': no_decay, 'weight_decay': 0.0},\n",
        "    ]\n",
        "    return optim.AdamW(param_groups, lr=lr)\n",
        "\n",
        "# ---- (2) Reg warm-up helpers (dropout + WD) ----\n",
        "def set_dropout_p(model, p):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout):\n",
        "            m.p = float(p)\n",
        "\n",
        "def set_weight_decay(optimizer, wd):\n",
        "    for g in optimizer.param_groups:\n",
        "        g['weight_decay'] = float(wd)\n",
        "\n",
        "# L1 ONLY on the first (input) Linear layer (already warmed up in train loop)\n",
        "def l1_penalty_first_layer(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            return m.weight.abs().sum()\n",
        "    return torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "\n",
        "def train_one_epoch(model, optimizer, dataloader, device, l1_lambda=0.0, epoch=0, warmup_epochs=20):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))  # linear warmup of L1\n",
        "    loss_sum, n_seen = 0.0, 0\n",
        "    skipped, total_batches = 0, 0\n",
        "    for x, t, e in dataloader:\n",
        "        total_batches += 1\n",
        "        # with balanced sampler, this should almost never trigger, but keep it safe:\n",
        "        if e.sum().item() == 0:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        x, t, e = x.to(device), t.to(device), e.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = neg_partial_log_likelihood(out, e, t, reduction='mean')\n",
        "        if l1_lambda > 0:\n",
        "            loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        n_seen += x.size(0)\n",
        "    avg_loss = loss_sum / max(n_seen, 1)\n",
        "    skip_frac = skipped / max(total_batches, 1)\n",
        "    return {'avg_loss': avg_loss, 'skip_frac': skip_frac, 'warm': warm}\n",
        "\n",
        "# ---- (3) Full-risk-set correction step (1x per epoch) ----\n",
        "def full_risk_set_step(model, optimizer, train_ds, device, l1_lambda=0.0, warm=1.0):\n",
        "    model.train()\n",
        "    X_all = train_ds.x.to(device)\n",
        "    t_all = train_ds.time.to(device)\n",
        "    e_all = train_ds.event.to(device)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(X_all), -20, 20)\n",
        "    loss_full = neg_partial_log_likelihood(out_all, e_all, t_all, reduction='mean')\n",
        "    if l1_lambda > 0:\n",
        "        loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "def evaluate_ci(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, times, events = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for x, t, e in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = torch.clamp(model(x), -20, 20)\n",
        "            preds.append(y.cpu().numpy().ravel())\n",
        "            times.append(t.numpy()); events.append(e.numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    if np.isnan(preds).any():\n",
        "        print(\"Warning: NaN predictions detected, returning -inf for concordance index\")\n",
        "        return -np.inf\n",
        "    times = np.concatenate(times); events = np.concatenate(events)\n",
        "    return concordance_index_censored(events.astype(bool), times, preds)[0]\n",
        "\n",
        "def count_params(in_dim, hidden_layers):\n",
        "    params, d = 0, in_dim\n",
        "    for h in hidden_layers:\n",
        "        params += d*h + h  # weights + bias\n",
        "        d = h\n",
        "    params += d*1 + 1     # final layer\n",
        "    return params\n",
        "\n",
        "# single model - trial 71\n",
        "def main():\n",
        "    # ----- fixed hyperparameters (single run) -----\n",
        "    HIDDEN_LAYERS = [512]\n",
        "    DROPOUT = 0.355164077\n",
        "    LR = 2.69E-05\n",
        "    WD = 0.4063904607\n",
        "    L1 = 0.008506512284\n",
        "\n",
        "    MAX_EPOCHS = 30\n",
        "    BATCH_SIZE = 64\n",
        "    L1_WARMUP_EPOCHS = 30\n",
        "    WD_WARMUP_EPOCHS = 30\n",
        "    DROPOUT_WARMUP_EPOCHS = 30\n",
        "    DROPOUT_START = 0.15\n",
        "    WD_START = 0.0\n",
        "    PRINT_EVERY = 1\n",
        "    MIN_DELTA = 1e-4\n",
        "    DELTA_CI_MA_K = 10\n",
        "\n",
        "    # logging\n",
        "    original_stdout = sys.stdout\n",
        "    log_path = \"/content/drive/MyDrive/deepsurv_single_run_log_71.txt\"\n",
        "    with open(log_path, \"w\") as log_file:\n",
        "        sys.stdout = Tee(original_stdout, log_file)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "        output_dir = \"/content/drive/MyDrive/deepsurv_results_single\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # ----- load & prep data -----\n",
        "        train_df = pd.read_csv(\"/content/drive/MyDrive/affyTrainFrozen.csv\")\n",
        "        valid_df = pd.read_csv(\"/content/drive/MyDrive/affyValidationFrozen.csv\")\n",
        "        for df in (train_df, valid_df):\n",
        "            if 'Adjuvant Chemo' in df.columns:\n",
        "                df['Adjuvant Chemo'] = df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n",
        "        binary_columns = ['Adjuvant Chemo','IS_MALE']\n",
        "        for col in binary_columns:\n",
        "            if col in train_df.columns: train_df[col] = train_df[col].astype(int)\n",
        "            if col in valid_df.columns: valid_df[col] = valid_df[col].astype(int)\n",
        "\n",
        "        survival_cols = ['OS_STATUS','OS_MONTHS']\n",
        "        feature_cols = [c for c in train_df.columns if c not in survival_cols]\n",
        "\n",
        "        X_train = train_df[feature_cols].values.astype(np.float32)\n",
        "        X_valid = valid_df[feature_cols].values.astype(np.float32)\n",
        "        scaler = StandardScaler().fit(X_train)\n",
        "        X_train = scaler.transform(X_train).astype(np.float32)\n",
        "        X_valid = scaler.transform(X_valid).astype(np.float32)\n",
        "\n",
        "        y_train_time = train_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_train_event = train_df['OS_STATUS'].values.astype(np.float32)\n",
        "        y_valid_time = valid_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_valid_event = valid_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        train_ds = SurvivalDataset(X_train, y_train_time, y_train_event)\n",
        "        valid_ds = SurvivalDataset(X_valid, y_valid_time, y_valid_event)\n",
        "\n",
        "        # loaders (event-balanced batches for training)\n",
        "        train_sampler = EventBalancedBatchSampler(y_train_event, BATCH_SIZE, seed=42)\n",
        "        train_loader  = DataLoader(train_ds, batch_sampler=train_sampler)\n",
        "        train_eval_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        valid_loader      = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # ----- build single model -----\n",
        "        model = DeepSurvMLP(X_train.shape[1], HIDDEN_LAYERS, dropout=DROPOUT).to(device)\n",
        "        optimizer = make_optimizer(model, lr=LR, wd=WD)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS)\n",
        "\n",
        "        # tracking\n",
        "        best_state = copy.deepcopy(model.state_dict())\n",
        "        best_val_ci = -np.inf\n",
        "        best_epoch = 0\n",
        "        best_gap = np.inf\n",
        "        best_gap_abs = np.inf\n",
        "        train_ci_at_best = np.nan\n",
        "        val_ci_at_best = np.nan\n",
        "\n",
        "        hist_train_ci, hist_val_ci, hist_gap = [], [], []\n",
        "        hist_loss, hist_skip, hist_val_ci_delta, hist_val_ci_delta_ma = [], [], [], []\n",
        "\n",
        "        # ----- train loop -----\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            # warm up dropout & weight decay schedules\n",
        "            frac_d = min(1.0, (epoch-1) / float(DROPOUT_WARMUP_EPOCHS))\n",
        "            frac_w = min(1.0, (epoch-1) / float(WD_WARMUP_EPOCHS))\n",
        "            p_t = DROPOUT_START + (DROPOUT - DROPOUT_START) * frac_d\n",
        "            wd_t = WD_START + (WD - WD_START) * frac_w\n",
        "            set_dropout_p(model, p_t)\n",
        "            set_weight_decay(optimizer, wd_t)\n",
        "\n",
        "            stats = train_one_epoch(\n",
        "                model, optimizer, train_loader, device,\n",
        "                l1_lambda=L1, epoch=epoch-1, warmup_epochs=L1_WARMUP_EPOCHS\n",
        "            )\n",
        "            full_loss = full_risk_set_step(\n",
        "                model, optimizer, train_ds, device,\n",
        "                l1_lambda=L1, warm=stats['warm']\n",
        "            )\n",
        "\n",
        "            # eval\n",
        "            tr_ci = evaluate_ci(model, train_eval_loader, device)\n",
        "            va_ci = evaluate_ci(model, valid_loader, device)\n",
        "            gap = tr_ci - va_ci\n",
        "\n",
        "            hist_train_ci.append(tr_ci)\n",
        "            hist_val_ci.append(va_ci)\n",
        "            hist_gap.append(gap)\n",
        "            hist_loss.append(stats['avg_loss'])\n",
        "            hist_skip.append(stats['skip_frac'])\n",
        "\n",
        "            if len(hist_val_ci) >= 2:\n",
        "                delta = hist_val_ci[-1] - hist_val_ci[-2]\n",
        "            else:\n",
        "                delta = 0.0\n",
        "            hist_val_ci_delta.append(delta)\n",
        "            ma = float(np.mean(hist_val_ci_delta[-DELTA_CI_MA_K:]))\n",
        "            hist_val_ci_delta_ma.append(ma)\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            if epoch % PRINT_EVERY == 0:\n",
        "                print(f\"[Epoch {epoch:3d}] \"\n",
        "                      f\"Loss {stats['avg_loss']:.4f} | FullStepLoss {full_loss:.4f} | \"\n",
        "                      f\"Skip% {100*stats['skip_frac']:.1f} | \"\n",
        "                      f\"Train CI {tr_ci:.4f} | Val CI {va_ci:.4f} | \"\n",
        "                      f\"Gap(T−V) {gap:+.4f} | ΔVal-CI MA({DELTA_CI_MA_K}) {ma:+.4f} | \"\n",
        "                      f\"Best {best_val_ci:.4f}\")\n",
        "\n",
        "            # best-by-val-ci; break ties by smaller |gap|\n",
        "            if va_ci > best_val_ci + MIN_DELTA:\n",
        "                best_val_ci = va_ci\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "                best_epoch = epoch\n",
        "                best_gap = gap\n",
        "                best_gap_abs = abs(gap)\n",
        "                train_ci_at_best = tr_ci\n",
        "                val_ci_at_best = va_ci\n",
        "            else:\n",
        "                if abs(va_ci - best_val_ci) <= MIN_DELTA and abs(gap) < best_gap_abs - 1e-12:\n",
        "                    best_state = copy.deepcopy(model.state_dict())\n",
        "                    best_epoch = epoch\n",
        "                    best_gap = gap\n",
        "                    best_gap_abs = abs(gap)\n",
        "                    train_ci_at_best = tr_ci\n",
        "                    val_ci_at_best = va_ci\n",
        "\n",
        "        print(\"\\n=== Training complete ===\")\n",
        "        print(f\"Best Val CI: {best_val_ci:.4f} at epoch {best_epoch} (Train CI {train_ci_at_best:.4f}, Gap {best_gap:+.4f})\")\n",
        "\n",
        "        # ----- save artifacts -----\n",
        "        # plot (optional, small)\n",
        "        epochs = range(1, len(hist_train_ci)+1)\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, hist_train_ci, label='Train CI')\n",
        "        plt.plot(epochs, hist_val_ci, label='Val CI')\n",
        "        ax = plt.gca()\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(epochs, hist_val_ci_delta_ma, linestyle='--', alpha=0.5, label='ΔVal-CI MA')\n",
        "        ax.set_xlabel('Epoch'); ax.set_ylabel('Concordance Index'); ax2.set_ylabel('ΔVal-CI MA')\n",
        "        lines, labels = ax.get_legend_handles_labels()\n",
        "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "        ax.legend(lines + lines2, labels + labels2, loc='lower right')\n",
        "        ax.set_ylim(0.4, 1.0)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        cfg = f\"layers-{'-'.join(map(str,HIDDEN_LAYERS))}_drop{DROPOUT:.2f}_lr{LR:.2e}_wd{WD:.2e}_l1{L1:.2e}\"\n",
        "        plt.title(cfg)\n",
        "        plot_path = os.path.join(output_dir, f\"{current_date}_ci_single_{cfg}.png\")\n",
        "        plt.savefig(plot_path, dpi=150, bbox_inches='tight'); plt.close()\n",
        "        print(f\"Saved CI plot to {plot_path}\")\n",
        "\n",
        "        # save best model\n",
        "        best_model_path = os.path.join(output_dir, f\"{current_date}_single_deepsurv_model.pth\")\n",
        "        torch.save(best_state, best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "\n",
        "        # save brief results CSV\n",
        "        results = pd.DataFrame([{\n",
        "            'layers': '-'.join(map(str, HIDDEN_LAYERS)),\n",
        "            'dropout': DROPOUT, 'lr': LR,\n",
        "            'weight_decay(L2)': WD, 'l1_lambda': L1,\n",
        "            'param_count': count_params(X_train.shape[1], HIDDEN_LAYERS),\n",
        "            'best_epoch': best_epoch,\n",
        "            'train_ci_at_best': train_ci_at_best,\n",
        "            'val_ci_at_best': val_ci_at_best,\n",
        "            'overfit_gap_at_best': best_gap,\n",
        "            'epochs_trained': len(hist_val_ci),\n",
        "            'avg_loss_last': hist_loss[-1] if hist_loss else np.nan,\n",
        "            'skip_frac_last': hist_skip[-1] if hist_skip else np.nan,\n",
        "            'val_ci_ma10_last': hist_val_ci_delta_ma[-1] if hist_val_ci_delta_ma else np.nan\n",
        "        }])\n",
        "        csv_path = os.path.join(output_dir, f\"{current_date}_single_run_results.csv\")\n",
        "        results.to_csv(csv_path, index=False)\n",
        "        print(f\"Single-run results saved to {csv_path}\")\n",
        "\n",
        "        # ----- test evaluation -----\n",
        "        test_df = pd.read_csv(\"/content/drive/MyDrive/affyTestFrozen.csv\")\n",
        "        if 'Adjuvant Chemo' in test_df.columns:\n",
        "            test_df['Adjuvant Chemo'] = test_df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n",
        "        for col in binary_columns:\n",
        "            if col in test_df.columns: test_df[col] = test_df[col].astype(int)\n",
        "\n",
        "        X_test = scaler.transform(test_df[feature_cols].values.astype(np.float32)).astype(np.float32)\n",
        "        y_test_time = test_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_test_event = test_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        test_loader = DataLoader(SurvivalDataset(X_test, y_test_time, y_test_event),\n",
        "                                 batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # rebuild & load best\n",
        "        final_model = DeepSurvMLP(X_train.shape[1], HIDDEN_LAYERS, dropout=DROPOUT).to(device)\n",
        "        final_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "        test_ci = evaluate_ci(final_model, test_loader, device)\n",
        "        print(f\"Test CI: {test_ci:.4f}\")\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    sys.stdout = original_stdout\n",
        "    print(\"Single-model training completed. Check your log file at:\", log_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZItIY8HiIZoY",
        "outputId": "3b212fd3-2683-4076-dc1b-9eee0dbfeb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1690969216.py:241: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Adjuvant Chemo'] = df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch   1] Loss 12.1418 | FullStepLoss 15.2923 | Skip% 0.0 | Train CI 0.5215 | Val CI 0.5354 | Gap(T−V) -0.0139 | ΔVal-CI MA(10) +0.0000 | Best -inf\n",
            "[Epoch   2] Loss 20.1175 | FullStepLoss 22.7214 | Skip% 0.0 | Train CI 0.6804 | Val CI 0.6259 | Gap(T−V) +0.0546 | ΔVal-CI MA(10) +0.0452 | Best 0.5354\n",
            "[Epoch   3] Loss 27.5403 | FullStepLoss 30.2976 | Skip% 0.0 | Train CI 0.6819 | Val CI 0.6455 | Gap(T−V) +0.0365 | ΔVal-CI MA(10) +0.0367 | Best 0.6259\n",
            "[Epoch   4] Loss 34.7130 | FullStepLoss 37.2511 | Skip% 0.0 | Train CI 0.7156 | Val CI 0.6512 | Gap(T−V) +0.0644 | ΔVal-CI MA(10) +0.0289 | Best 0.6455\n",
            "[Epoch   5] Loss 41.1263 | FullStepLoss 43.5886 | Skip% 0.0 | Train CI 0.7696 | Val CI 0.6482 | Gap(T−V) +0.1214 | ΔVal-CI MA(10) +0.0226 | Best 0.6512\n",
            "[Epoch   6] Loss 47.3751 | FullStepLoss 49.5353 | Skip% 0.0 | Train CI 0.6963 | Val CI 0.6253 | Gap(T−V) +0.0710 | ΔVal-CI MA(10) +0.0150 | Best 0.6512\n",
            "[Epoch   7] Loss 52.5724 | FullStepLoss 54.6075 | Skip% 0.0 | Train CI 0.7819 | Val CI 0.6278 | Gap(T−V) +0.1541 | ΔVal-CI MA(10) +0.0132 | Best 0.6512\n",
            "[Epoch   8] Loss 57.5452 | FullStepLoss 58.9215 | Skip% 0.0 | Train CI 0.7889 | Val CI 0.6375 | Gap(T−V) +0.1513 | ΔVal-CI MA(10) +0.0128 | Best 0.6512\n",
            "[Epoch   9] Loss 61.4516 | FullStepLoss 62.6404 | Skip% 0.0 | Train CI 0.7726 | Val CI 0.6621 | Gap(T−V) +0.1104 | ΔVal-CI MA(10) +0.0141 | Best 0.6512\n",
            "[Epoch  10] Loss 64.7496 | FullStepLoss 66.0194 | Skip% 0.0 | Train CI 0.7921 | Val CI 0.6314 | Gap(T−V) +0.1607 | ΔVal-CI MA(10) +0.0096 | Best 0.6621\n",
            "[Epoch  11] Loss 67.6485 | FullStepLoss 69.1953 | Skip% 0.0 | Train CI 0.7114 | Val CI 0.6098 | Gap(T−V) +0.1016 | ΔVal-CI MA(10) +0.0074 | Best 0.6621\n",
            "[Epoch  12] Loss 70.5082 | FullStepLoss 72.0480 | Skip% 0.0 | Train CI 0.7945 | Val CI 0.6262 | Gap(T−V) +0.1683 | ΔVal-CI MA(10) +0.0000 | Best 0.6621\n",
            "[Epoch  13] Loss 73.2315 | FullStepLoss 74.4964 | Skip% 0.0 | Train CI 0.8006 | Val CI 0.6626 | Gap(T−V) +0.1380 | ΔVal-CI MA(10) +0.0017 | Best 0.6621\n",
            "[Epoch  14] Loss 75.2454 | FullStepLoss 76.5715 | Skip% 0.0 | Train CI 0.8209 | Val CI 0.6516 | Gap(T−V) +0.1692 | ΔVal-CI MA(10) +0.0000 | Best 0.6626\n",
            "[Epoch  15] Loss 77.0425 | FullStepLoss 78.5405 | Skip% 0.0 | Train CI 0.8152 | Val CI 0.6479 | Gap(T−V) +0.1673 | ΔVal-CI MA(10) -0.0000 | Best 0.6626\n",
            "[Epoch  16] Loss 79.0701 | FullStepLoss 80.3918 | Skip% 0.0 | Train CI 0.8165 | Val CI 0.6497 | Gap(T−V) +0.1667 | ΔVal-CI MA(10) +0.0024 | Best 0.6626\n",
            "[Epoch  17] Loss 80.8865 | FullStepLoss 82.2508 | Skip% 0.0 | Train CI 0.8180 | Val CI 0.6585 | Gap(T−V) +0.1595 | ΔVal-CI MA(10) +0.0031 | Best 0.6626\n",
            "[Epoch  18] Loss 82.6977 | FullStepLoss 84.1358 | Skip% 0.0 | Train CI 0.8239 | Val CI 0.6472 | Gap(T−V) +0.1767 | ΔVal-CI MA(10) +0.0010 | Best 0.6626\n",
            "[Epoch  19] Loss 84.3195 | FullStepLoss 86.1423 | Skip% 0.0 | Train CI 0.8102 | Val CI 0.6474 | Gap(T−V) +0.1628 | ΔVal-CI MA(10) -0.0015 | Best 0.6626\n",
            "[Epoch  20] Loss 86.2687 | FullStepLoss 88.2219 | Skip% 0.0 | Train CI 0.8218 | Val CI 0.6487 | Gap(T−V) +0.1731 | ΔVal-CI MA(10) +0.0017 | Best 0.6626\n",
            "[Epoch  21] Loss 88.5953 | FullStepLoss 90.4446 | Skip% 0.0 | Train CI 0.8239 | Val CI 0.6547 | Gap(T−V) +0.1692 | ΔVal-CI MA(10) +0.0045 | Best 0.6626\n",
            "[Epoch  22] Loss 90.6410 | FullStepLoss 92.8503 | Skip% 0.0 | Train CI 0.8258 | Val CI 0.6451 | Gap(T−V) +0.1807 | ΔVal-CI MA(10) +0.0019 | Best 0.6626\n",
            "[Epoch  23] Loss 93.2607 | FullStepLoss 95.4771 | Skip% 0.0 | Train CI 0.8214 | Val CI 0.6480 | Gap(T−V) +0.1734 | ΔVal-CI MA(10) -0.0015 | Best 0.6626\n",
            "[Epoch  24] Loss 95.7703 | FullStepLoss 98.2688 | Skip% 0.0 | Train CI 0.8229 | Val CI 0.6404 | Gap(T−V) +0.1825 | ΔVal-CI MA(10) -0.0011 | Best 0.6626\n",
            "[Epoch  25] Loss 98.7511 | FullStepLoss 101.2648 | Skip% 0.0 | Train CI 0.8219 | Val CI 0.6411 | Gap(T−V) +0.1808 | ΔVal-CI MA(10) -0.0007 | Best 0.6626\n",
            "[Epoch  26] Loss 102.0191 | FullStepLoss 104.4453 | Skip% 0.0 | Train CI 0.8256 | Val CI 0.6415 | Gap(T−V) +0.1841 | ΔVal-CI MA(10) -0.0008 | Best 0.6626\n",
            "[Epoch  27] Loss 105.1245 | FullStepLoss 107.8113 | Skip% 0.0 | Train CI 0.8259 | Val CI 0.6410 | Gap(T−V) +0.1849 | ΔVal-CI MA(10) -0.0017 | Best 0.6626\n",
            "[Epoch  28] Loss 108.4416 | FullStepLoss 111.3305 | Skip% 0.0 | Train CI 0.8259 | Val CI 0.6464 | Gap(T−V) +0.1794 | ΔVal-CI MA(10) -0.0001 | Best 0.6626\n",
            "[Epoch  29] Loss 112.1215 | FullStepLoss 114.9617 | Skip% 0.0 | Train CI 0.8258 | Val CI 0.6477 | Gap(T−V) +0.1781 | ΔVal-CI MA(10) +0.0000 | Best 0.6626\n",
            "[Epoch  30] Loss 115.7221 | FullStepLoss 118.7081 | Skip% 0.0 | Train CI 0.8258 | Val CI 0.6476 | Gap(T−V) +0.1782 | ΔVal-CI MA(10) -0.0001 | Best 0.6626\n",
            "\n",
            "=== Training complete ===\n",
            "Best Val CI: 0.6626 at epoch 13 (Train CI 0.8006, Gap +0.1380)\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results_single/20250915_ci_single_layers-512_drop0.36_lr2.69e-05_wd4.06e-01_l18.51e-03.png\n",
            "Best model saved to /content/drive/MyDrive/deepsurv_results_single/20250915_single_deepsurv_model.pth\n",
            "Single-run results saved to /content/drive/MyDrive/deepsurv_results_single/20250915_single_run_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1690969216.py:404: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  test_df['Adjuvant Chemo'] = test_df['Adjuvant Chemo'].replace({'OBS':0,'ACT':1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test CI: 0.6184\n",
            "Single-model training completed. Check your log file at: /content/drive/MyDrive/deepsurv_single_run_log_71.txt\n"
          ]
        }
      ]
    }
  ]
}