{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1EY87_PKv0dTOF6mK_Kx8YPzRYVzJ6o-8",
      "authorship_tag": "ABX9TyMrEDtjrRbE24BENTsook26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osun24/nsclc-adj-chemo/blob/main/TorchSurv_DeepSurv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torchsurv scikit-survival\n",
        "\n",
        "# Import required packages\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "\n",
        "# (Optional) Mount Google Drive if you plan to load/save files there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81TEvMle6mqs",
        "outputId": "c0fca219-d6b3-476d-9316-771d08c25da1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsurv\n",
            "  Downloading torchsurv-0.1.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting scikit-survival\n",
            "  Downloading scikit_survival-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.16.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.0.2)\n",
            "Collecting torchmetrics (from torchsurv)\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ecos (from scikit-survival)\n",
            "  Downloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.11.0)\n",
            "Collecting osqp<1.0.0,>=0.6.3 (from scikit-survival)\n",
            "  Downloading osqp-0.6.7.post3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.6.1)\n",
            "Collecting qdldl (from osqp<1.0.0,>=0.6.3->scikit-survival)\n",
            "  Downloading qdldl-0.1.7.post5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8,>=1.6.1->scikit-survival) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.4.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (25.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->torchsurv)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchsurv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchsurv) (3.0.2)\n",
            "Downloading torchsurv-0.1.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_survival-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading osqp-0.6.7.post3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.1/222.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading qdldl-0.1.7.post5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, qdldl, ecos, osqp, torchmetrics, scikit-survival, torchsurv\n",
            "  Attempting uninstall: osqp\n",
            "    Found existing installation: osqp 1.0.4\n",
            "    Uninstalling osqp-1.0.4:\n",
            "      Successfully uninstalled osqp-1.0.4\n",
            "Successfully installed ecos-2.0.14 lightning-utilities-0.15.2 osqp-0.6.7.post3 qdldl-0.1.7.post5 scikit-survival-0.25.0 torchmetrics-1.8.1 torchsurv-0.1.5\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baO87aHV3DUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce887fbd-9a48-46ea-c225-98249c3ea53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3455961078.py:114: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  train_df['Adjuvant Chemo'] = train_df['Adjuvant Chemo'].replace({'OBS': 0, 'ACT': 1})\n",
            "/tmp/ipython-input-3455961078.py:115: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  valid_df['Adjuvant Chemo'] = valid_df['Adjuvant Chemo'].replace({'OBS': 0, 'ACT': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with layers=[32], dropout=0.0, lr=0.0001, weight_decay=0.0\n",
            "Epoch 10/100 - Loss: 1.3607, Train CI: 0.9598, Val CI: 0.6332\n",
            "Epoch 20/100 - Loss: 0.8653, Train CI: 0.9816, Val CI: 0.6220\n",
            "Epoch 30/100 - Loss: 0.7363, Train CI: 0.9859, Val CI: 0.6235\n",
            "Epoch 40/100 - Loss: 0.6460, Train CI: 0.9859, Val CI: 0.6213\n",
            "Epoch 50/100 - Loss: 0.5740, Train CI: 0.9857, Val CI: 0.6270\n",
            "Epoch 60/100 - Loss: 0.5281, Train CI: 0.9882, Val CI: 0.6232\n",
            "Epoch 70/100 - Loss: 0.4913, Train CI: 0.9901, Val CI: 0.6188\n",
            "Epoch 80/100 - Loss: 0.4897, Train CI: 0.9889, Val CI: 0.6213\n",
            "Epoch 90/100 - Loss: 0.4640, Train CI: 0.9903, Val CI: 0.6166\n",
            "Epoch 100/100 - Loss: 0.4409, Train CI: 0.9896, Val CI: 0.6211\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.0001_wd0.0.png\n",
            "Finished config: Val CI = 0.6483\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.0001, weight_decay=0.0001\n",
            "Epoch 10/100 - Loss: 1.3618, Train CI: 0.9676, Val CI: 0.6070\n",
            "Epoch 20/100 - Loss: 0.8715, Train CI: 0.9809, Val CI: 0.6042\n",
            "Epoch 30/100 - Loss: 0.7398, Train CI: 0.9835, Val CI: 0.6119\n",
            "Epoch 40/100 - Loss: 0.6323, Train CI: 0.9853, Val CI: 0.6119\n",
            "Epoch 50/100 - Loss: 0.6210, Train CI: 0.9852, Val CI: 0.6090\n",
            "Epoch 60/100 - Loss: 0.5334, Train CI: 0.9890, Val CI: 0.6131\n",
            "Epoch 70/100 - Loss: 0.5059, Train CI: 0.9881, Val CI: 0.6095\n",
            "Epoch 80/100 - Loss: 0.4886, Train CI: 0.9886, Val CI: 0.6075\n",
            "Epoch 90/100 - Loss: 0.4413, Train CI: 0.9905, Val CI: 0.6129\n",
            "Epoch 100/100 - Loss: 0.4054, Train CI: 0.9885, Val CI: 0.6084\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.0001_wd0.0001.png\n",
            "Finished config: Val CI = 0.6373\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.0001, weight_decay=0.001\n",
            "Epoch 10/100 - Loss: 1.4394, Train CI: 0.9626, Val CI: 0.6333\n",
            "Epoch 20/100 - Loss: 0.9265, Train CI: 0.9806, Val CI: 0.6179\n",
            "Epoch 30/100 - Loss: 0.7794, Train CI: 0.9838, Val CI: 0.6140\n",
            "Epoch 40/100 - Loss: 0.6351, Train CI: 0.9869, Val CI: 0.6207\n",
            "Epoch 50/100 - Loss: 0.5998, Train CI: 0.9887, Val CI: 0.6227\n",
            "Epoch 60/100 - Loss: 0.5160, Train CI: 0.9874, Val CI: 0.6253\n",
            "Epoch 70/100 - Loss: 0.4825, Train CI: 0.9888, Val CI: 0.6178\n",
            "Epoch 80/100 - Loss: 0.4873, Train CI: 0.9907, Val CI: 0.6158\n",
            "Epoch 90/100 - Loss: 0.4663, Train CI: 0.9875, Val CI: 0.6151\n",
            "Epoch 100/100 - Loss: 0.4624, Train CI: 0.9898, Val CI: 0.6199\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.0001_wd0.001.png\n",
            "Finished config: Val CI = 0.6549\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.0003, weight_decay=0.0\n",
            "Epoch 10/100 - Loss: 1.1684, Train CI: 0.9605, Val CI: 0.6380\n",
            "Epoch 20/100 - Loss: 1.0314, Train CI: 0.9658, Val CI: 0.6287\n",
            "Epoch 30/100 - Loss: 0.7856, Train CI: 0.9742, Val CI: 0.6259\n",
            "Epoch 40/100 - Loss: 0.7312, Train CI: 0.9808, Val CI: 0.6162\n",
            "Epoch 50/100 - Loss: 0.7099, Train CI: 0.9818, Val CI: 0.6166\n",
            "Epoch 60/100 - Loss: 0.6479, Train CI: 0.9830, Val CI: 0.6166\n",
            "Epoch 70/100 - Loss: 0.6251, Train CI: 0.9846, Val CI: 0.6212\n",
            "Epoch 80/100 - Loss: 0.5248, Train CI: 0.9843, Val CI: 0.6276\n",
            "Epoch 90/100 - Loss: 0.5544, Train CI: 0.9851, Val CI: 0.6063\n",
            "Epoch 100/100 - Loss: 0.6103, Train CI: 0.9849, Val CI: 0.6085\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.0003_wd0.0.png\n",
            "Finished config: Val CI = 0.6428\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.0003, weight_decay=0.0001\n",
            "Epoch 10/100 - Loss: 1.1413, Train CI: 0.9601, Val CI: 0.6326\n",
            "Epoch 20/100 - Loss: 0.9018, Train CI: 0.9754, Val CI: 0.6270\n",
            "Epoch 30/100 - Loss: 0.8335, Train CI: 0.9804, Val CI: 0.6198\n",
            "Epoch 40/100 - Loss: 0.8177, Train CI: 0.9793, Val CI: 0.6060\n",
            "Epoch 50/100 - Loss: 0.6624, Train CI: 0.9818, Val CI: 0.6265\n",
            "Epoch 60/100 - Loss: 0.6421, Train CI: 0.9843, Val CI: 0.6185\n",
            "Epoch 70/100 - Loss: 0.6349, Train CI: 0.9829, Val CI: 0.6176\n",
            "Epoch 80/100 - Loss: 0.6375, Train CI: 0.9848, Val CI: 0.6297\n",
            "Epoch 90/100 - Loss: 0.6016, Train CI: 0.9838, Val CI: 0.6279\n",
            "Epoch 100/100 - Loss: 0.5688, Train CI: 0.9854, Val CI: 0.6094\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.0003_wd0.0001.png\n",
            "Finished config: Val CI = 0.6425\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.0003, weight_decay=0.001\n",
            "Epoch 10/100 - Loss: 1.2137, Train CI: 0.9591, Val CI: 0.5977\n",
            "Epoch 20/100 - Loss: 0.9964, Train CI: 0.9708, Val CI: 0.6104\n",
            "Epoch 30/100 - Loss: 0.7076, Train CI: 0.9789, Val CI: 0.6150\n",
            "Epoch 40/100 - Loss: 0.7706, Train CI: 0.9824, Val CI: 0.6131\n",
            "Epoch 50/100 - Loss: 0.7029, Train CI: 0.9835, Val CI: 0.6178\n",
            "Epoch 60/100 - Loss: 0.7017, Train CI: 0.9832, Val CI: 0.6217\n",
            "Epoch 70/100 - Loss: 0.5188, Train CI: 0.9834, Val CI: 0.6146\n",
            "Epoch 80/100 - Loss: 0.5757, Train CI: 0.9846, Val CI: 0.6143\n",
            "Epoch 90/100 - Loss: 0.5506, Train CI: 0.9860, Val CI: 0.6159\n",
            "Epoch 100/100 - Loss: 0.5946, Train CI: 0.9848, Val CI: 0.6200\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.0003_wd0.001.png\n",
            "Finished config: Val CI = 0.6332\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.001, weight_decay=0.0\n",
            "Epoch 10/100 - Loss: 1.5194, Train CI: 0.9389, Val CI: 0.6031\n",
            "Epoch 20/100 - Loss: 1.2384, Train CI: 0.9604, Val CI: 0.6263\n",
            "Epoch 30/100 - Loss: 0.9087, Train CI: 0.9723, Val CI: 0.6185\n",
            "Epoch 40/100 - Loss: 0.7798, Train CI: 0.9789, Val CI: 0.6047\n",
            "Epoch 50/100 - Loss: 0.7755, Train CI: 0.9761, Val CI: 0.6231\n",
            "Epoch 60/100 - Loss: 0.7255, Train CI: 0.9786, Val CI: 0.6138\n",
            "Epoch 70/100 - Loss: 0.6943, Train CI: 0.9803, Val CI: 0.6200\n",
            "Epoch 80/100 - Loss: 0.6271, Train CI: 0.9835, Val CI: 0.6202\n",
            "Epoch 90/100 - Loss: 0.5292, Train CI: 0.9840, Val CI: 0.6144\n",
            "Epoch 100/100 - Loss: 0.6232, Train CI: 0.9820, Val CI: 0.6143\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.001_wd0.0.png\n",
            "Finished config: Val CI = 0.6317\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.001, weight_decay=0.0001\n",
            "Epoch 10/100 - Loss: 1.5047, Train CI: 0.9358, Val CI: 0.6126\n",
            "Epoch 20/100 - Loss: 1.0694, Train CI: 0.9654, Val CI: 0.6201\n",
            "Epoch 30/100 - Loss: 0.8461, Train CI: 0.9692, Val CI: 0.6176\n",
            "Epoch 40/100 - Loss: 0.8524, Train CI: 0.9764, Val CI: 0.5997\n",
            "Epoch 50/100 - Loss: 0.7382, Train CI: 0.9798, Val CI: 0.6081\n",
            "Epoch 60/100 - Loss: 0.7867, Train CI: 0.9791, Val CI: 0.6033\n",
            "Epoch 70/100 - Loss: 0.6679, Train CI: 0.9804, Val CI: 0.6170\n",
            "Epoch 80/100 - Loss: 0.6150, Train CI: 0.9797, Val CI: 0.6041\n",
            "Epoch 90/100 - Loss: 0.5832, Train CI: 0.9828, Val CI: 0.5999\n",
            "Epoch 100/100 - Loss: 0.6174, Train CI: 0.9850, Val CI: 0.6043\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.001_wd0.0001.png\n",
            "Finished config: Val CI = 0.6323\n",
            "\n",
            "Training with layers=[32], dropout=0.0, lr=0.001, weight_decay=0.001\n",
            "Epoch 10/100 - Loss: 1.3103, Train CI: 0.9611, Val CI: 0.6118\n",
            "Epoch 20/100 - Loss: 1.1120, Train CI: 0.9548, Val CI: 0.6008\n",
            "Epoch 30/100 - Loss: 0.9947, Train CI: 0.9734, Val CI: 0.6117\n",
            "Epoch 40/100 - Loss: 0.9079, Train CI: 0.9792, Val CI: 0.6176\n",
            "Epoch 50/100 - Loss: 0.7962, Train CI: 0.9709, Val CI: 0.5999\n",
            "Epoch 60/100 - Loss: 0.6772, Train CI: 0.9793, Val CI: 0.6138\n",
            "Epoch 70/100 - Loss: 0.6643, Train CI: 0.9809, Val CI: 0.6075\n",
            "Epoch 80/100 - Loss: 0.6052, Train CI: 0.9834, Val CI: 0.5969\n",
            "Epoch 90/100 - Loss: 0.5442, Train CI: 0.9864, Val CI: 0.6190\n",
            "Epoch 100/100 - Loss: 0.6236, Train CI: 0.9807, Val CI: 0.6179\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.0_lr0.001_wd0.001.png\n",
            "Finished config: Val CI = 0.6378\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.0001, weight_decay=0.0\n",
            "Epoch 10/100 - Loss: 1.6404, Train CI: 0.9535, Val CI: 0.6174\n",
            "Epoch 20/100 - Loss: 1.2097, Train CI: 0.9760, Val CI: 0.6351\n",
            "Epoch 30/100 - Loss: 1.0452, Train CI: 0.9783, Val CI: 0.6107\n",
            "Epoch 40/100 - Loss: 0.9680, Train CI: 0.9820, Val CI: 0.6225\n",
            "Epoch 50/100 - Loss: 0.8783, Train CI: 0.9852, Val CI: 0.6242\n",
            "Epoch 60/100 - Loss: 0.8341, Train CI: 0.9836, Val CI: 0.6211\n",
            "Epoch 70/100 - Loss: 0.8221, Train CI: 0.9865, Val CI: 0.6189\n",
            "Epoch 80/100 - Loss: 0.8095, Train CI: 0.9868, Val CI: 0.6222\n",
            "Epoch 90/100 - Loss: 0.8147, Train CI: 0.9865, Val CI: 0.6140\n",
            "Epoch 100/100 - Loss: 0.7566, Train CI: 0.9850, Val CI: 0.6346\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.0001_wd0.0.png\n",
            "Finished config: Val CI = 0.6414\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.0001, weight_decay=0.0001\n",
            "Epoch 10/100 - Loss: 1.5579, Train CI: 0.9611, Val CI: 0.6289\n",
            "Epoch 20/100 - Loss: 1.1762, Train CI: 0.9756, Val CI: 0.6257\n",
            "Epoch 30/100 - Loss: 1.0399, Train CI: 0.9812, Val CI: 0.6224\n",
            "Epoch 40/100 - Loss: 0.9640, Train CI: 0.9777, Val CI: 0.6117\n",
            "Epoch 50/100 - Loss: 0.9644, Train CI: 0.9795, Val CI: 0.6308\n",
            "Epoch 60/100 - Loss: 0.8570, Train CI: 0.9845, Val CI: 0.6120\n",
            "Epoch 70/100 - Loss: 0.8726, Train CI: 0.9850, Val CI: 0.6205\n",
            "Epoch 80/100 - Loss: 0.8101, Train CI: 0.9877, Val CI: 0.6177\n",
            "Epoch 90/100 - Loss: 0.7830, Train CI: 0.9861, Val CI: 0.6230\n",
            "Epoch 100/100 - Loss: 0.7768, Train CI: 0.9868, Val CI: 0.6319\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.0001_wd0.0001.png\n",
            "Finished config: Val CI = 0.6383\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.0001, weight_decay=0.001\n",
            "Epoch 10/100 - Loss: 1.5829, Train CI: 0.9501, Val CI: 0.6416\n",
            "Epoch 20/100 - Loss: 1.2141, Train CI: 0.9736, Val CI: 0.6276\n",
            "Epoch 30/100 - Loss: 1.0708, Train CI: 0.9785, Val CI: 0.6266\n",
            "Epoch 40/100 - Loss: 0.9046, Train CI: 0.9808, Val CI: 0.6327\n",
            "Epoch 50/100 - Loss: 0.9127, Train CI: 0.9827, Val CI: 0.6371\n",
            "Epoch 60/100 - Loss: 0.8397, Train CI: 0.9823, Val CI: 0.6218\n",
            "Epoch 70/100 - Loss: 0.7978, Train CI: 0.9836, Val CI: 0.6372\n",
            "Epoch 80/100 - Loss: 0.7756, Train CI: 0.9846, Val CI: 0.6295\n",
            "Epoch 90/100 - Loss: 0.7853, Train CI: 0.9863, Val CI: 0.6346\n",
            "Epoch 100/100 - Loss: 0.7680, Train CI: 0.9860, Val CI: 0.6257\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.0001_wd0.001.png\n",
            "Finished config: Val CI = 0.6541\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.0003, weight_decay=0.0\n",
            "Epoch 10/100 - Loss: 1.4802, Train CI: 0.9534, Val CI: 0.5964\n",
            "Epoch 20/100 - Loss: 1.1896, Train CI: 0.9684, Val CI: 0.6198\n",
            "Epoch 30/100 - Loss: 1.1068, Train CI: 0.9733, Val CI: 0.6184\n",
            "Epoch 40/100 - Loss: 0.9538, Train CI: 0.9789, Val CI: 0.6274\n",
            "Epoch 50/100 - Loss: 0.9870, Train CI: 0.9783, Val CI: 0.6368\n",
            "Epoch 60/100 - Loss: 0.8422, Train CI: 0.9801, Val CI: 0.6397\n",
            "Epoch 70/100 - Loss: 0.8590, Train CI: 0.9817, Val CI: 0.6328\n",
            "Epoch 80/100 - Loss: 0.9372, Train CI: 0.9828, Val CI: 0.6248\n",
            "Epoch 90/100 - Loss: 0.8513, Train CI: 0.9835, Val CI: 0.6250\n",
            "Epoch 100/100 - Loss: 0.8766, Train CI: 0.9829, Val CI: 0.6183\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.0003_wd0.0.png\n",
            "Finished config: Val CI = 0.6511\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.0003, weight_decay=0.0001\n",
            "Epoch 10/100 - Loss: 1.5811, Train CI: 0.9540, Val CI: 0.6265\n",
            "Epoch 20/100 - Loss: 1.3137, Train CI: 0.9640, Val CI: 0.6270\n",
            "Epoch 30/100 - Loss: 1.1693, Train CI: 0.9663, Val CI: 0.6229\n",
            "Epoch 40/100 - Loss: 1.0765, Train CI: 0.9751, Val CI: 0.6403\n",
            "Epoch 50/100 - Loss: 0.9565, Train CI: 0.9753, Val CI: 0.6284\n",
            "Epoch 60/100 - Loss: 0.8765, Train CI: 0.9817, Val CI: 0.6297\n",
            "Epoch 70/100 - Loss: 0.8862, Train CI: 0.9817, Val CI: 0.6313\n",
            "Epoch 80/100 - Loss: 0.8562, Train CI: 0.9827, Val CI: 0.6356\n",
            "Epoch 90/100 - Loss: 0.9083, Train CI: 0.9828, Val CI: 0.6306\n",
            "Epoch 100/100 - Loss: 0.8234, Train CI: 0.9853, Val CI: 0.6412\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.0003_wd0.0001.png\n",
            "Finished config: Val CI = 0.6460\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.0003, weight_decay=0.001\n",
            "Epoch 10/100 - Loss: 1.6178, Train CI: 0.9412, Val CI: 0.6269\n",
            "Epoch 20/100 - Loss: 1.3753, Train CI: 0.9625, Val CI: 0.6123\n",
            "Epoch 30/100 - Loss: 1.2671, Train CI: 0.9693, Val CI: 0.6185\n",
            "Epoch 40/100 - Loss: 1.2023, Train CI: 0.9726, Val CI: 0.6202\n",
            "Epoch 50/100 - Loss: 1.1040, Train CI: 0.9754, Val CI: 0.6263\n",
            "Epoch 60/100 - Loss: 0.9436, Train CI: 0.9774, Val CI: 0.6144\n",
            "Epoch 70/100 - Loss: 0.8336, Train CI: 0.9811, Val CI: 0.6316\n",
            "Epoch 80/100 - Loss: 0.9194, Train CI: 0.9823, Val CI: 0.6353\n",
            "Epoch 90/100 - Loss: 0.8256, Train CI: 0.9831, Val CI: 0.6288\n",
            "Epoch 100/100 - Loss: 0.8751, Train CI: 0.9834, Val CI: 0.6290\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.0003_wd0.001.png\n",
            "Finished config: Val CI = 0.6482\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.001, weight_decay=0.0\n",
            "Epoch 10/100 - Loss: 1.7005, Train CI: 0.9422, Val CI: 0.6393\n",
            "Epoch 20/100 - Loss: 1.3214, Train CI: 0.9603, Val CI: 0.6144\n",
            "Epoch 30/100 - Loss: 1.2027, Train CI: 0.9671, Val CI: 0.6177\n",
            "Epoch 40/100 - Loss: 1.0479, Train CI: 0.9745, Val CI: 0.6319\n",
            "Epoch 50/100 - Loss: 1.0437, Train CI: 0.9778, Val CI: 0.6006\n",
            "Epoch 60/100 - Loss: 1.0288, Train CI: 0.9741, Val CI: 0.6297\n",
            "Epoch 70/100 - Loss: 0.9851, Train CI: 0.9818, Val CI: 0.6228\n",
            "Epoch 80/100 - Loss: 0.9012, Train CI: 0.9814, Val CI: 0.6349\n",
            "Epoch 90/100 - Loss: 0.8801, Train CI: 0.9816, Val CI: 0.6277\n",
            "Epoch 100/100 - Loss: 0.8128, Train CI: 0.9823, Val CI: 0.6270\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.001_wd0.0.png\n",
            "Finished config: Val CI = 0.6509\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.001, weight_decay=0.0001\n",
            "Epoch 10/100 - Loss: 1.7878, Train CI: 0.9371, Val CI: 0.6415\n",
            "Epoch 20/100 - Loss: 1.3349, Train CI: 0.9593, Val CI: 0.6235\n",
            "Epoch 30/100 - Loss: 1.2282, Train CI: 0.9704, Val CI: 0.6349\n",
            "Epoch 40/100 - Loss: 1.0960, Train CI: 0.9760, Val CI: 0.6287\n",
            "Epoch 50/100 - Loss: 1.0582, Train CI: 0.9755, Val CI: 0.6238\n",
            "Epoch 60/100 - Loss: 1.0697, Train CI: 0.9800, Val CI: 0.6231\n",
            "Epoch 70/100 - Loss: 0.8637, Train CI: 0.9804, Val CI: 0.6386\n",
            "Epoch 80/100 - Loss: 0.9343, Train CI: 0.9821, Val CI: 0.6081\n",
            "Epoch 90/100 - Loss: 0.8700, Train CI: 0.9830, Val CI: 0.6428\n",
            "Epoch 100/100 - Loss: 0.8267, Train CI: 0.9822, Val CI: 0.6186\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.001_wd0.0001.png\n",
            "Finished config: Val CI = 0.6664\n",
            "\n",
            "Training with layers=[32], dropout=0.2, lr=0.001, weight_decay=0.001\n",
            "Epoch 10/100 - Loss: 1.7756, Train CI: 0.9255, Val CI: 0.6296\n",
            "Epoch 20/100 - Loss: 1.5737, Train CI: 0.9544, Val CI: 0.6394\n",
            "Epoch 30/100 - Loss: 1.2388, Train CI: 0.9695, Val CI: 0.6127\n",
            "Epoch 40/100 - Loss: 1.1340, Train CI: 0.9731, Val CI: 0.6236\n",
            "Epoch 50/100 - Loss: 1.0670, Train CI: 0.9757, Val CI: 0.6313\n",
            "Epoch 60/100 - Loss: 0.9588, Train CI: 0.9780, Val CI: 0.6114\n",
            "Epoch 70/100 - Loss: 1.0610, Train CI: 0.9788, Val CI: 0.6226\n",
            "Epoch 80/100 - Loss: 0.9780, Train CI: 0.9818, Val CI: 0.6050\n",
            "Epoch 90/100 - Loss: 0.9177, Train CI: 0.9795, Val CI: 0.6170\n",
            "Epoch 100/100 - Loss: 0.8388, Train CI: 0.9818, Val CI: 0.6382\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.2_lr0.001_wd0.001.png\n",
            "Finished config: Val CI = 0.6553\n",
            "\n",
            "Training with layers=[32], dropout=0.5, lr=0.0001, weight_decay=0.0\n",
            "Epoch 10/100 - Loss: 1.9965, Train CI: 0.9411, Val CI: 0.6418\n",
            "Epoch 20/100 - Loss: 1.6163, Train CI: 0.9606, Val CI: 0.6430\n",
            "Epoch 30/100 - Loss: 1.4478, Train CI: 0.9655, Val CI: 0.6412\n",
            "Epoch 40/100 - Loss: 1.3430, Train CI: 0.9740, Val CI: 0.6248\n",
            "Epoch 50/100 - Loss: 1.3087, Train CI: 0.9764, Val CI: 0.6322\n",
            "Epoch 60/100 - Loss: 1.1806, Train CI: 0.9791, Val CI: 0.6368\n",
            "Epoch 70/100 - Loss: 1.1956, Train CI: 0.9811, Val CI: 0.6377\n",
            "Epoch 80/100 - Loss: 1.1757, Train CI: 0.9753, Val CI: 0.6326\n",
            "Epoch 90/100 - Loss: 1.1150, Train CI: 0.9818, Val CI: 0.6300\n",
            "Epoch 100/100 - Loss: 1.1871, Train CI: 0.9831, Val CI: 0.6329\n",
            "Saved CI plot to /content/drive/MyDrive/deepsurv_results/20250829_ci_layers-32_drop0.5_lr0.0001_wd0.0.png\n",
            "Finished config: Val CI = 0.6505\n",
            "\n",
            "Training with layers=[32], dropout=0.5, lr=0.0001, weight_decay=0.0001\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "import warnings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "\n",
        "# Define a Tee class for logging output to both console and file\n",
        "class Tee:\n",
        "    def __init__(self, *files):\n",
        "        self.files = files\n",
        "    def write(self, data):\n",
        "        for f in self.files:\n",
        "            f.write(data)\n",
        "    def flush(self):\n",
        "        for f in self.files:\n",
        "            f.flush()\n",
        "\n",
        "# Define a custom MLP model for DeepSurv (remove BatchNorm)\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        current_dim = in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(current_dim, units), activation]\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            current_dim = units\n",
        "        layers.append(nn.Linear(current_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Define a PyTorch Dataset for survival data\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events, dtype=torch.bool)\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.time[idx], self.event[idx]\n",
        "\n",
        "def train_model(model, optimizer, dataloader, device):\n",
        "    model.train()\n",
        "    running_loss, n_seen = 0.0, 0\n",
        "    for x, time_vals, events in dataloader:\n",
        "        if events.sum().item() == 0:\n",
        "            continue  # skip non-informative batches for Cox\n",
        "        x = x.to(device); time_vals = time_vals.to(device); events = events.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = torch.clamp(model(x), -20, 20)\n",
        "        loss = neg_partial_log_likelihood(outputs, events, time_vals, reduction='mean')\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        n_seen += x.size(0)\n",
        "    return running_loss / max(n_seen, 1)\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_time = []\n",
        "    all_event = []\n",
        "    with torch.no_grad():\n",
        "        for x, time_vals, events in dataloader:\n",
        "            x = x.to(device)\n",
        "            preds = torch.clamp(model(x), -20, 20) # clamp to prevent blowups\n",
        "            all_preds.append(preds.cpu().numpy().flatten())\n",
        "            all_time.append(time_vals.cpu().numpy())\n",
        "            all_event.append(events.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    if np.isnan(all_preds).any():\n",
        "        print(\"Warning: NaN predictions detected, returning -inf for concordance index\")\n",
        "        return -np.inf\n",
        "    all_time = np.concatenate(all_time)\n",
        "    all_event = np.concatenate(all_event)\n",
        "    ci = concordance_index_censored(all_event.astype(bool), all_time, all_preds)[0]\n",
        "    return ci\n",
        "\n",
        "def main():\n",
        "    # Capture the original stdout\n",
        "    original_stdout = sys.stdout\n",
        "    log_path = \"/content/drive/MyDrive/deepsurv_8-27-25_training_log.txt\"\n",
        "\n",
        "    # Open log file with context, and use Tee to write to both original_stdout and file\n",
        "    with open(log_path, \"w\") as log_file:\n",
        "        sys.stdout = Tee(original_stdout, log_file)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "        output_dir = \"/content/drive/MyDrive/deepsurv_results\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        train_df = pd.read_csv(\"/content/drive/MyDrive/affyTrain.csv\")\n",
        "        valid_df = pd.read_csv(\"/content/drive/MyDrive/affyValidation.csv\")\n",
        "\n",
        "        train_df['Adjuvant Chemo'] = train_df['Adjuvant Chemo'].replace({'OBS': 0, 'ACT': 1})\n",
        "        valid_df['Adjuvant Chemo'] = valid_df['Adjuvant Chemo'].replace({'OBS': 0, 'ACT': 1})\n",
        "\n",
        "        binary_columns = ['Adjuvant Chemo', 'IS_MALE']\n",
        "        for col in binary_columns:\n",
        "            if col in train_df.columns:\n",
        "                train_df[col] = train_df[col].astype(int)\n",
        "            if col in valid_df.columns:\n",
        "                valid_df[col] = valid_df[col].astype(int)\n",
        "\n",
        "        survival_cols = ['OS_STATUS', 'OS_MONTHS']\n",
        "        feature_cols = [col for col in train_df.columns if col not in survival_cols]\n",
        "\n",
        "        X_train = train_df[feature_cols].values.astype(np.float32)\n",
        "        X_valid = valid_df[feature_cols].values.astype(np.float32)\n",
        "\n",
        "        scaler = StandardScaler().fit(X_train)\n",
        "        X_train = scaler.transform(X_train).astype(np.float32)\n",
        "        X_valid = scaler.transform(X_valid).astype(np.float32)\n",
        "\n",
        "        y_train_time = train_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_train_event = train_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        y_valid_time = valid_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_valid_event = valid_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        train_dataset = SurvivalDataset(X_train, y_train_time, y_train_event)\n",
        "        valid_dataset = SurvivalDataset(X_valid, y_valid_time, y_valid_event)\n",
        "\n",
        "        batch_size = 32\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "        train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        hidden_layer_configs = [\n",
        "            [32],\n",
        "            [64],\n",
        "            [32, 16],\n",
        "            [64, 32]\n",
        "        ]\n",
        "        dropout_rates = [0.0, 0.2, 0.5]\n",
        "        learning_rates = [1e-4, 3e-4, 1e-3] #[0.001, 0.01]\n",
        "        weight_decays = [0.0, 1e-4, 1e-3]\n",
        "\n",
        "        num_epochs = 100\n",
        "        best_ci = -np.inf\n",
        "        best_hyperparams = None\n",
        "        best_model_state = None\n",
        "        results = []\n",
        "\n",
        "        for layers in hidden_layer_configs:\n",
        "            for dropout in dropout_rates:\n",
        "                for lr in learning_rates:\n",
        "                    for wd in weight_decays:\n",
        "                        print(f\"Training with layers={layers}, dropout={dropout}, lr={lr}, weight_decay={wd}\")\n",
        "                        model = DeepSurvMLP(in_features=X_train.shape[1],\n",
        "                                            hidden_layers=layers,\n",
        "                                            dropout=dropout,\n",
        "                                            activation=nn.ReLU()).to(device)\n",
        "                        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "                        best_epoch_ci = -np.inf\n",
        "                        hist_train_ci, hist_val_ci, hist_train_loss = [], [], []\n",
        "\n",
        "                        for epoch in range(num_epochs):\n",
        "                            train_loss = train_model(model, optimizer, train_loader, device)\n",
        "                            # CI on full train (no drop_last, no shuffle) and valid\n",
        "                            train_ci = evaluate_model(model, train_eval_loader, device)\n",
        "                            val_ci = evaluate_model(model, valid_loader, device)\n",
        "\n",
        "                            hist_train_loss.append(train_loss)\n",
        "                            hist_train_ci.append(train_ci)\n",
        "                            hist_val_ci.append(val_ci)\n",
        "\n",
        "                            if (epoch + 1) % 10 == 0:\n",
        "                                print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}, Train CI: {train_ci:.4f}, Val CI: {val_ci:.4f}\")\n",
        "\n",
        "                            if val_ci > best_epoch_ci:\n",
        "                                best_epoch_ci = val_ci\n",
        "\n",
        "                            def preds_on(dl):\n",
        "                                return np.concatenate([model(x.to(device)).detach().cpu().numpy().ravel()\n",
        "                                                      for x,_,_ in dl])\n",
        "                            if epoch == 0: val_prev = preds_on(valid_loader)\n",
        "                            else:\n",
        "                                val_now = preds_on(valid_loader)\n",
        "                                rho = np.corrcoef(np.argsort(val_prev), np.argsort(val_now))[0,1]  # rank corr proxy\n",
        "                                print(f\"Val rank corr vs epoch1: {rho:.4f}\")\n",
        "\n",
        "                            vp = preds_on(valid_loader)\n",
        "                            print(\"Frac |pred|>=20:\", (np.abs(vp)>=20).mean())\n",
        "\n",
        "                        # After epochs: save CI plot for this config\n",
        "                        cfg = f\"layers-{'-'.join(map(str, layers))}_drop{dropout}_lr{lr}_wd{wd}\"\n",
        "                        plot_path = os.path.join(output_dir, f\"{current_date}_ci_{cfg}.png\")\n",
        "                        plt.figure()\n",
        "                        plt.plot(range(1, num_epochs+1), hist_train_ci, label='Train CI')\n",
        "                        plt.plot(range(1, num_epochs+1), hist_val_ci, label='Val CI')\n",
        "                        plt.xlabel('Epoch'); plt.ylabel('Concordance Index'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "                        plt.title(cfg)\n",
        "                        plt.savefig(plot_path, dpi=150, bbox_inches='tight'); plt.close()\n",
        "                        print(f\"Saved CI plot to {plot_path}\")\n",
        "                        results.append({\n",
        "                            'layers': layers,\n",
        "                            'dropout': dropout,\n",
        "                            'learning_rate': lr,\n",
        "                            'weight_decay': wd,\n",
        "                            'val_ci': best_epoch_ci\n",
        "                        })\n",
        "                        print(f\"Finished config: Val CI = {best_epoch_ci:.4f}\\n\")\n",
        "                        if best_epoch_ci > best_ci:\n",
        "                            best_ci = best_epoch_ci\n",
        "                            best_hyperparams = {\n",
        "                                'layers': layers,\n",
        "                                'dropout': dropout,\n",
        "                                'learning_rate': lr,\n",
        "                                'weight_decay': wd\n",
        "                            }\n",
        "                            best_model_state = model.state_dict()\n",
        "\n",
        "        print(\"Best Hyperparameters:\")\n",
        "        print(best_hyperparams)\n",
        "        print(\"Best Validation CI:\", best_ci)\n",
        "\n",
        "        current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "        output_dir = \"/content/drive/MyDrive/deepsurv_results\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_csv_path = os.path.join(output_dir, f\"{current_date}_deepsurv_hyperparam_search_results.csv\")\n",
        "        results_df.to_csv(results_csv_path, index=False)\n",
        "        print(f\"Hyperparameter search results saved to {results_csv_path}\")\n",
        "\n",
        "        best_model_path = os.path.join(output_dir, f\"{current_date}_best_deepsurv_model.pth\")\n",
        "        torch.save(best_model_state, best_model_path)\n",
        "        print(f\"Best model saved to {best_model_path}\")\n",
        "\n",
        "        # Perform eval on test set\n",
        "        test_df = pd.read_csv(\"/content/drive/MyDrive/affyTest.csv\")\n",
        "\n",
        "        test_df['Adjuvant Chemo'] = test_df['Adjuvant Chemo'].replace({'OBS': 0, 'ACT': 1})\n",
        "        for col in binary_columns:\n",
        "            if col in test_df.columns:\n",
        "                test_df[col] = test_df[col].astype(int)\n",
        "\n",
        "        X_test = test_df[feature_cols].values.astype(np.float32)\n",
        "        X_test = scaler.transform(X_test).astype(np.float32)\n",
        "        y_test_time = test_df['OS_MONTHS'].values.astype(np.float32)\n",
        "        y_test_event = test_df['OS_STATUS'].values.astype(np.float32)\n",
        "\n",
        "        test_dataset = SurvivalDataset(X_test, y_test_time, y_test_event)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Rebuild best model and load saved weights\n",
        "        best_model = DeepSurvMLP(\n",
        "            in_features=X_train.shape[1],\n",
        "            hidden_layers=best_hyperparams['layers'],\n",
        "            dropout=best_hyperparams['dropout'],\n",
        "            activation=nn.ReLU()\n",
        "        ).to(device)\n",
        "        best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "        test_ci = evaluate_model(best_model, test_loader, device)\n",
        "        print(f\"Test CI: {test_ci:.4f}\")\n",
        "\n",
        "        # Flush before exiting the with block\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Restore original stdout for subsequent prints to the console\n",
        "    sys.stdout = original_stdout\n",
        "    print(\"Training completed. Check your log file at:\", log_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}