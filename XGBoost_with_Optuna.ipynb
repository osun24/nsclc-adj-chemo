{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osun24/nsclc-adj-chemo/blob/main/XGBoost_with_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "81TEvMle6mqs",
        "outputId": "1c66e5de-5ed3-4032-c16e-967cf80344e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsurv\n",
            "  Downloading torchsurv-0.1.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting scikit-survival\n",
            "  Downloading scikit_survival-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.0.2)\n",
            "Collecting torchmetrics (from torchsurv)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ecos (from scikit-survival)\n",
            "  Downloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.14.1)\n",
            "Collecting osqp<1.0.0,>=0.6.3 (from scikit-survival)\n",
            "  Downloading osqp-0.6.7.post3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.6.1)\n",
            "Collecting qdldl (from osqp<1.0.0,>=0.6.3->scikit-survival)\n",
            "  Downloading qdldl-0.1.7.post5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8,>=1.6.1->scikit-survival) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.4.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (25.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->torchsurv)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchsurv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchsurv) (3.0.3)\n",
            "Downloading torchsurv-0.1.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_survival-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading osqp-0.6.7.post3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.1/222.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading qdldl-0.1.7.post5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, qdldl, ecos, osqp, torchmetrics, scikit-survival, torchsurv\n",
            "  Attempting uninstall: osqp\n",
            "    Found existing installation: osqp 1.0.5\n",
            "    Uninstalling osqp-1.0.5:\n",
            "      Successfully uninstalled osqp-1.0.5\n",
            "Successfully installed ecos-2.0.14 lightning-utilities-0.15.2 osqp-0.6.7.post3 qdldl-0.1.7.post5 scikit-survival-0.25.0 torchmetrics-1.8.2 torchsurv-0.1.5\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting optuna-dashboard\n",
            "  Downloading optuna_dashboard-0.19.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Collecting bottle>=0.13.0 (from optuna-dashboard)\n",
            "  Downloading bottle-0.13.4-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from optuna-dashboard) (1.6.1)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.14.1)\n",
            "Requirement already satisfied: osqp<1.0.0,>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (0.6.7.post3)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.16.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from portpicker) (5.9.5)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.12/dist-packages (from osqp<1.0.0,>=0.6.3->scikit-survival) (0.1.7.post5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->optuna-dashboard) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna_dashboard-0.19.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bottle-0.13.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: bottle, colorlog, optuna, optuna-dashboard\n",
            "Successfully installed bottle-0.13.4 colorlog-6.10.1 optuna-4.5.0 optuna-dashboard-0.19.0\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-4 (_serve):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\n",
            "    self.dialect.do_execute(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/default.py\", line 951, in do_execute\n",
            "    cursor.execute(statement, parameters)\n",
            "sqlite3.OperationalError: no such table: version_info\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 77, in _create_scoped_session\n",
            "    yield session\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 1046, in _init_version_info_model\n",
            "    version_info = models.VersionInfoModel.find(session)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/models.py\", line 591, in find\n",
            "    version_info = session.query(cls).one_or_none()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py\", line 2785, in one_or_none\n",
            "    return self._iter().one_or_none()  # type: ignore\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py\", line 2857, in _iter\n",
            "    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(\n",
            "                                                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/session.py\", line 2351, in execute\n",
            "    return self._execute_internal(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/session.py\", line 2249, in _execute_internal\n",
            "    result: Result[Any] = compile_state_cls.orm_execute_statement(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/context.py\", line 306, in orm_execute_statement\n",
            "    result = conn.execute(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1419, in execute\n",
            "    return meth(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/sql/elements.py\", line 526, in _execute_on_connection\n",
            "    return connection._execute_clauseelement(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1641, in _execute_clauseelement\n",
            "    ret = self._execute_context(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1846, in _execute_context\n",
            "    return self._exec_single_context(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1986, in _exec_single_context\n",
            "    self._handle_dbapi_exception(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 2355, in _handle_dbapi_exception\n",
            "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\n",
            "    self.dialect.do_execute(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/default.py\", line 951, in do_execute\n",
            "    cursor.execute(statement, parameters)\n",
            "sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: version_info\n",
            "[SQL: SELECT version_info.version_info_id AS version_info_version_info_id, version_info.schema_version AS version_info_schema_version, version_info.library_version AS version_info_library_version \n",
            "FROM version_info]\n",
            "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-3330911947.py\", line 29, in _serve\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_app.py\", line 658, in run_server\n",
            "    app = create_app(get_storage(storage), artifact_store=store)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_storage_url.py\", line 59, in get_storage\n",
            "    return guess_storage_from_url(storage)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_storage_url.py\", line 84, in guess_storage_from_url\n",
            "    return get_rdb_storage(storage_url)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_storage_url.py\", line 91, in get_rdb_storage\n",
            "    return RDBStorage(storage_url, skip_compatibility_check=True, skip_table_creation=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 240, in __init__\n",
            "    self._version_manager = _VersionManager(self.url, self.engine, self.scoped_session)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 1041, in __init__\n",
            "    self._init_version_info_model()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 1045, in _init_version_info_model\n",
            "    with _create_scoped_session(self.scoped_session, True) as session:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 95, in _create_scoped_session\n",
            "    raise optuna.exceptions.StorageInternalError(message) from e\n",
            "optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dashboard: https://44085-gpu-a100-s-9m8yuyfy4d1s-a.us-central1-0.prod.colab.dev\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip -q install xgboost optuna scikit-survival scikit-learn\n",
        "\n",
        "# Import required packages\n",
        "import os, math, gc, time, random, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "from sksurv.util import Surv\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import NSGAIISampler\n",
        "\n",
        "# (Optional) Mount Google Drive if you plan to load/save files there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab-ready SINGLE CELL\n",
        "# XGBoost (gbtree) Cox PH with IPTW, feature budgets,\n",
        "# emphasized gene×ACT interactions (incl. optional duplication),\n",
        "# early stopping on C-index, and conservative Pareto selection\n",
        "# ============================================================\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, math, gc, time, random, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "from sksurv.util import Surv\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import NSGAIISampler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\",\n",
        "    message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "\n",
        "np.random.seed(42); random.seed(42)\n",
        "\n",
        "# ---------- Paths ----------\n",
        "# Update these to your files if needed\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/affyfRMATrain.csv\"\n",
        "VALID_CSV = \"/content/drive/MyDrive/affyfRMAValidation.csv\"\n",
        "TEST_CSV  = \"/content/drive/MyDrive/affyfRMATest.csv\"\n",
        "\n",
        "GENES_CSV = \"/mnt/data/Genes.csv\"\n",
        "if not os.path.exists(GENES_CSV):\n",
        "    if os.path.exists(\"/content/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/Genes.csv\"\n",
        "    elif os.path.exists(\"/content/drive/MyDrive/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/drive/MyDrive/Genes.csv\"\n",
        "print(\"Genes.csv path:\", GENES_CSV)\n",
        "\n",
        "# ---------- Clinical columns ----------\n",
        "CLINICAL_VARS = [\n",
        "    \"Adjuvant Chemo\",\"Age\",\"IS_MALE\",\n",
        "    \"Stage_IA\",\"Stage_IB\",\"Stage_II\",\"Stage_III\",\n",
        "    \"Histology_Adenocarcinoma\",\"Histology_Large Cell Carcinoma\",\"Histology_Squamous Cell Carcinoma\",\n",
        "    \"Race_African American\",\"Race_Asian\",\"Race_Caucasian\",\"Race_Native Hawaiian or Other Pacific Islander\",\"Race_Unknown\",\n",
        "    \"Smoked?_No\",\"Smoked?_Unknown\",\"Smoked?_Yes\"\n",
        "]\n",
        "CLIN_FEATS_PRETX = [c for c in CLINICAL_VARS if c != \"Adjuvant Chemo\"]  # for IPTW only\n",
        "\n",
        "# ============================================================\n",
        "# Helpers: IO, preprocessing, ranking, features, IPTW, metrics\n",
        "# ============================================================\n",
        "def load_genes_list(genes_csv):\n",
        "    g = pd.read_csv(genes_csv)\n",
        "    if \"Prop\" not in g.columns or \"Gene\" not in g.columns:\n",
        "        raise ValueError(\"Genes.csv must have columns 'Gene' and 'Prop'.\")\n",
        "    g[\"Prop\"] = pd.to_numeric(g[\"Prop\"], errors=\"coerce\").fillna(0)\n",
        "    genes = g.loc[g[\"Prop\"] == 1, \"Gene\"].astype(str).tolist()\n",
        "    print(f\"[Genes] Selected {len(genes)} genes with Prop == 1\")\n",
        "    return genes\n",
        "\n",
        "def coerce_survival_cols(df):\n",
        "    if df[\"OS_STATUS\"].dtype == object:\n",
        "        df[\"OS_STATUS\"] = df[\"OS_STATUS\"].replace({\"DECEASED\":1,\"LIVING\":0,\"Dead\":1,\"Alive\":0}).astype(int)\n",
        "    else:\n",
        "        df[\"OS_STATUS\"] = pd.to_numeric(df[\"OS_STATUS\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"OS_MONTHS\"] = pd.to_numeric(df[\"OS_MONTHS\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def preprocess_split(df, clinical_vars, gene_names):\n",
        "    if \"Adjuvant Chemo\" in df.columns:\n",
        "        df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
        "    for col in [\"Adjuvant Chemo\",\"IS_MALE\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df = coerce_survival_cols(df)\n",
        "    keep_cols = [c for c in clinical_vars if c in df.columns] + [g for g in gene_names if g in df.columns]\n",
        "    cols = [\"OS_STATUS\",\"OS_MONTHS\"] + keep_cols\n",
        "    return df[cols].copy()\n",
        "\n",
        "def rank_genes_univariate(train_df, gene_cols):\n",
        "    y = Surv.from_arrays(event=train_df[\"OS_STATUS\"].astype(bool).values,\n",
        "                         time=train_df[\"OS_MONTHS\"].values.astype(float))\n",
        "    ranks = []\n",
        "    for g in gene_cols:\n",
        "        Xg = train_df[[g]].to_numpy(dtype=np.float32)\n",
        "        try:\n",
        "            model = CoxPHSurvivalAnalysis(alpha=1e-12)\n",
        "            model.fit(Xg, y)\n",
        "            pred = model.predict(Xg)\n",
        "            ci = concordance_index_censored(y[\"event\"], y[\"time\"], pred)[0]\n",
        "            ranks.append((g, float(ci)))\n",
        "        except Exception:\n",
        "            ranks.append((g, 0.5))\n",
        "    ranks.sort(key=lambda z: z[1], reverse=True)\n",
        "    return [g for g, _ in ranks]\n",
        "\n",
        "# === Emphasize interactions via optional duplication ===\n",
        "def build_features_with_interactions(df, main_genes, inter_genes, act_col=\"Adjuvant Chemo\", dup_inter=1):\n",
        "    \"\"\"\n",
        "    Build [clinical + genes_main + gene*ACT] features.\n",
        "    If dup_inter>1, duplicate interaction columns with unique names to bias column sampling.\n",
        "    \"\"\"\n",
        "    base_cols = CLINICAL_VARS + list(main_genes)  # keep ACT main effect in clinicals\n",
        "    X_base = df[base_cols].to_numpy(dtype=np.float32)\n",
        "    A = df[act_col].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
        "\n",
        "    names = list(base_cols)\n",
        "    blocks = [X_base]\n",
        "\n",
        "    if len(inter_genes) > 0:\n",
        "        X_int = df[list(inter_genes)].to_numpy(dtype=np.float32) * A\n",
        "        names_int = [f\"{g}*ACT\" for g in inter_genes]\n",
        "        blocks.append(X_int); names += names_int\n",
        "\n",
        "        # Duplicate interaction columns to increase selection chance\n",
        "        if int(dup_inter) > 1:\n",
        "            for d in range(1, int(dup_inter)):\n",
        "                blocks.append(X_int.copy())\n",
        "                names += [f\"{g}*ACT#dup{d}\" for g in inter_genes]\n",
        "\n",
        "    X = np.concatenate(blocks, axis=1) if len(blocks) > 1 else X_base\n",
        "    return X, names\n",
        "\n",
        "def compute_iptw(df, covariate_cols, act_col=\"Adjuvant Chemo\",\n",
        "                 ps_clip=(0.05, 0.95), w_clip=(0.1, 10.0),\n",
        "                 ref_prev=None, model=None):\n",
        "    A = df[act_col].astype(int).values\n",
        "    X = df[covariate_cols].astype(float).values\n",
        "    if model is None:\n",
        "        model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", class_weight=\"balanced\")\n",
        "        model.fit(X, A)\n",
        "    ps = model.predict_proba(X)[:, 1]\n",
        "    ps = np.clip(ps, ps_clip[0], ps_clip[1])\n",
        "    if ref_prev is None:\n",
        "        ref_prev = A.mean()\n",
        "    w = np.where(A == 1, ref_prev / ps, (1 - ref_prev) / (1 - ps))\n",
        "    w = np.clip(w, w_clip[0], w_clip[1])\n",
        "    return w.astype(np.float32), model, float(ref_prev)\n",
        "\n",
        "def pack_cox_labels(time, event):\n",
        "    \"\"\"XGBoost Cox: positive time => event, negative time => censored.\"\"\"\n",
        "    time = np.asarray(time, dtype=np.float32)\n",
        "    event = np.asarray(event, dtype=int)\n",
        "    return np.where(event == 1, time, -time).astype(np.float32)\n",
        "\n",
        "def cindex(pred, time, event):\n",
        "    return float(concordance_index_censored(event.astype(bool), time.astype(float), pred)[0])\n",
        "\n",
        "# ============================================================\n",
        "# Load data, rank genes (TRAIN only), set budgets\n",
        "# ============================================================\n",
        "train_raw = pd.read_csv(TRAIN_CSV)\n",
        "valid_raw = pd.read_csv(VALID_CSV)\n",
        "test_raw  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "GENE_LIST = load_genes_list(GENES_CSV)\n",
        "\n",
        "train_df = preprocess_split(train_raw, CLINICAL_VARS, GENE_LIST)\n",
        "valid_df = preprocess_split(valid_raw, CLINICAL_VARS, GENE_LIST)\n",
        "test_df  = preprocess_split(test_raw,  CLINICAL_VARS, GENE_LIST)\n",
        "\n",
        "# Keep only features present in all splits\n",
        "feat_candidates = [c for c in (CLINICAL_VARS + GENE_LIST)\n",
        "                   if c in train_df.columns and c in valid_df.columns and c in test_df.columns]\n",
        "CLIN_FEATS = [c for c in CLINICAL_VARS if c in feat_candidates]\n",
        "GENE_FEATS = [g for g in GENE_LIST if g in feat_candidates]\n",
        "\n",
        "# Sort by time/status for stability\n",
        "train_df = train_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "valid_df = valid_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "# Train-only univariate ranking for genes\n",
        "GENE_RANK = rank_genes_univariate(train_df, GENE_FEATS)\n",
        "MAX_GENES = len(GENE_RANK)\n",
        "print(f\"[Gene Ranking] Ranked {MAX_GENES} genes on TRAIN\")\n",
        "\n",
        "# ====== Capacity budgets tied to event count ======\n",
        "N_EVENTS_TR = int(train_df[\"OS_STATUS\"].sum())\n",
        "FEAT_EVENT_FRACTION = 0.50               # tune 0.35–0.60\n",
        "FEAT_BUDGET = max(24, int(FEAT_EVENT_FRACTION * N_EVENTS_TR))   # total inputs incl. clinical\n",
        "print(f\"[Budgets] events(train)={N_EVENTS_TR} → feature budget ≤ {FEAT_BUDGET}\")\n",
        "\n",
        "# ============================================================\n",
        "# XGBoost Cox setup: DMatrix, custom C-index eval, train helper\n",
        "# ============================================================\n",
        "def make_dmatrix(X, time, event, weight=None, feature_names=None):\n",
        "    y_signed = pack_cox_labels(time, event)  # +t if event, -t if censored\n",
        "    dm = xgb.DMatrix(X, label=y_signed, weight=weight, feature_names=feature_names)\n",
        "    return dm\n",
        "\n",
        "def xgb_cindex_eval(preds, dmatrix):\n",
        "    y_signed = dmatrix.get_label()\n",
        "    times = np.abs(y_signed)\n",
        "    events = (y_signed > 0.0).astype(int)\n",
        "    ci = cindex(preds, times, events)\n",
        "    # is_higher_better=True\n",
        "    return (\"cindex\", ci, True)\n",
        "\n",
        "def train_xgb_cox(dtrain, dvalid, params, num_boost_round, early_stopping_rounds):\n",
        "    evals_result = {}\n",
        "    # Requested: xgboost.config_context(verbosity=2) + gbtree\n",
        "    with xgb.config_context(verbosity=2):\n",
        "        booster = xgb.train(\n",
        "            params=params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=num_boost_round,\n",
        "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
        "            feval=xgb_cindex_eval,\n",
        "            evals_result=evals_result,\n",
        "            early_stopping_rounds=early_stopping_rounds,\n",
        "        )\n",
        "    return booster, evals_result\n",
        "\n",
        "# ============================================================\n",
        "# Optuna: NSGA-II (Val CI ↑, Gap ↓) + emphasized interactions\n",
        "# ============================================================\n",
        "def suggest_hparams(trial):\n",
        "    max_nonclin = max(8, FEAT_BUDGET - len(CLIN_FEATS))\n",
        "\n",
        "    # Larger menu; interactions can be as numerous as mains (within budget)\n",
        "    base_main  = [16, 32, 64, 96, 128, 192, 256, 384, 512, MAX_GENES]\n",
        "    TOPK_MAIN_CHOICES = tuple(sorted({k for k in base_main if k <= MAX_GENES}))\n",
        "    top_k_genes = int(trial.suggest_categorical(\"top_k_genes\", TOPK_MAIN_CHOICES))\n",
        "\n",
        "    base_inter = [0, 8, 16, 32, 64, 96, 128, 192, 256, 384, 512]\n",
        "    TOPK_INTER_CHOICES = tuple(sorted({k for k in base_inter if k <= MAX_GENES}))\n",
        "    # Ratio that biases interactions up relative to mains\n",
        "    inter_ratio = trial.suggest_float(\"inter_ratio\", 0.75, 1.25)\n",
        "    top_k_inter_raw = int(min(int(round(inter_ratio * top_k_genes)), max(base_inter)))\n",
        "\n",
        "    # Budget clamp: interactions get priority up to target ratio\n",
        "    k_main = int(min(top_k_genes, max_nonclin))\n",
        "    k_int  = int(min(top_k_inter_raw, k_main, max_nonclin - k_main))\n",
        "\n",
        "    # Optional duplication of interaction columns (1 = off)\n",
        "    dup_inter = trial.suggest_int(\"dup_inter\", 1, 3)\n",
        "\n",
        "    # XGBoost: allow more interaction capacity but keep strong regularization\n",
        "    params = {\n",
        "        \"objective\": \"survival:cox\",\n",
        "        \"booster\": \"gbtree\",\n",
        "        \"eval_metric\": \"cox-nloglik\",\n",
        "        \"tree_method\": \"hist\",            # switch to 'gpu_hist' if you have a GPU\n",
        "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.12, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),   # ↑ depth for interactions\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 2.0, 30.0, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 6.0),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.65, 0.95),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n",
        "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.6, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 3.0, 60.0, log=True),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 3.0),\n",
        "        \"max_bin\": trial.suggest_int(\"max_bin\", 128, 512),\n",
        "        \"seed\": 42,\n",
        "        \"nthread\": -1,\n",
        "    }\n",
        "    num_boost_round = trial.suggest_int(\"num_boost_round\", 600, 3200, step=100)\n",
        "    early_stopping_rounds = trial.suggest_int(\"early_stopping_rounds\", 75, 175, step=25)\n",
        "\n",
        "    return k_main, k_int, dup_inter, params, num_boost_round, early_stopping_rounds\n",
        "\n",
        "# Precompute IPTW on Train and Valid (prevalence anchored to TRAIN)\n",
        "w_tr, ps_model, pi_tr = compute_iptw(train_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\")\n",
        "w_va, _, _ = compute_iptw(valid_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\",\n",
        "                          ref_prev=pi_tr, model=ps_model)\n",
        "\n",
        "def build_trial_mats(k_main, k_int, dup_inter):\n",
        "    genes_main  = GENE_RANK[:k_main]\n",
        "    genes_inter = genes_main[:k_int]\n",
        "    Xtr_raw, feat_names = build_features_with_interactions(train_df, genes_main, genes_inter, dup_inter=dup_inter)\n",
        "    Xva_raw, _          = build_features_with_interactions(valid_df, genes_main, genes_inter, dup_inter=dup_inter)\n",
        "\n",
        "    # Impute (median). Trees don't need scaling; we impute to avoid NaNs.\n",
        "    med = np.nanmedian(Xtr_raw, axis=0)\n",
        "    Xtr = np.where(np.isnan(Xtr_raw), med, Xtr_raw).astype(np.float32)\n",
        "    Xva = np.where(np.isnan(Xva_raw), med, Xva_raw).astype(np.float32)\n",
        "\n",
        "    ytr = pack_cox_labels(train_df[\"OS_MONTHS\"].values, train_df[\"OS_STATUS\"].values)\n",
        "    yva = pack_cox_labels(valid_df[\"OS_MONTHS\"].values,  valid_df[\"OS_STATUS\"].values)\n",
        "\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr, weight=w_tr, feature_names=feat_names)\n",
        "    dva = xgb.DMatrix(Xva, label=yva, weight=w_va, feature_names=feat_names)\n",
        "    return dtr, dva, feat_names\n",
        "\n",
        "def objective(trial):\n",
        "    k_main, k_int, dup_inter, params, num_boost_round, esr = suggest_hparams(trial)\n",
        "    dtr, dva, feat_names = build_trial_mats(k_main, k_int, dup_inter)\n",
        "    booster, evr = train_xgb_cox(dtr, dva, params, num_boost_round, esr)\n",
        "\n",
        "    # Best iteration\n",
        "    best_ntree = booster.best_ntree_limit or booster.best_iteration + 1\n",
        "    tr_pred = booster.predict(dtr, iteration_range=(0, best_ntree), output_margin=True)\n",
        "    va_pred = booster.predict(dva, iteration_range=(0, best_ntree), output_margin=True)\n",
        "\n",
        "    ytr_signed = dtr.get_label(); ttr = np.abs(ytr_signed); etr = (ytr_signed > 0).astype(int)\n",
        "    yva_signed = dva.get_label(); tva = np.abs(yva_signed); eva = (yva_signed > 0).astype(int)\n",
        "\n",
        "    tr_ci = cindex(tr_pred, ttr, etr)\n",
        "    va_ci = cindex(va_pred, tva, eva)\n",
        "    gap = max(0.0, tr_ci - va_ci)\n",
        "\n",
        "    trial.set_user_attr(\"n_features\", int(len(dtr.feature_names)))\n",
        "    trial.set_user_attr(\"k_main\", int(k_main))\n",
        "    trial.set_user_attr(\"k_int\", int(k_int))\n",
        "    trial.set_user_attr(\"dup_inter\", int(dup_inter))\n",
        "    trial.set_user_attr(\"best_ntree\", int(best_ntree))\n",
        "    return va_ci, gap\n",
        "\n",
        "# ---- Run study ----\n",
        "storage = \"sqlite:///xgb_cox_optuna.db\"\n",
        "study_name = \"xgb_cox_mo_gap_bounded_interactions_emphasis\"\n",
        "sampler = NSGAIISampler(seed=42, population_size=24)\n",
        "study = optuna.create_study(\n",
        "    directions=[\"maximize\", \"minimize\"],\n",
        "    study_name=study_name, storage=storage, load_if_exists=True, sampler=sampler\n",
        ")\n",
        "N_TRIALS = 60  # adjust as needed\n",
        "print(f\"Starting multi-objective optimization: {N_TRIALS} trials\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)\n",
        "\n",
        "# ---- Choose a robust solution from Pareto front (conservative) ----\n",
        "pareto = study.best_trials\n",
        "best_val = max(tr.values[0] for tr in pareto)\n",
        "TOL = 0.015  # within 1.5pp absolute C-index of best\n",
        "cands = [tr for tr in pareto if (best_val - tr.values[0]) <= TOL]\n",
        "# prefer smaller gap, then fewer features\n",
        "cands.sort(key=lambda tr: (tr.values[1], tr.user_attrs.get(\"n_features\", 10**9)))\n",
        "chosen = cands[0]\n",
        "\n",
        "print(\"\\n[Chosen Pareto] Val CI=%.4f | Gap=%.4f | n_features=%d\" %\n",
        "      (chosen.values[0], chosen.values[1], chosen.user_attrs.get(\"n_features\", -1)))\n",
        "print(\"[Chosen Params]\", chosen.params)\n",
        "print(\"[Chosen Attrs] k_main=%s k_int=%s dup_inter=%s best_ntree=%s\" %\n",
        "      (str(chosen.user_attrs.get(\"k_main\")), str(chosen.user_attrs.get(\"k_int\")),\n",
        "       str(chosen.user_attrs.get(\"dup_inter\")), str(chosen.user_attrs.get(\"best_ntree\"))))\n",
        "\n",
        "# ============================================================\n",
        "# Final training on Train+Val with chosen hyperparams + IPTW\n",
        "# ============================================================\n",
        "best_hp = chosen.params\n",
        "k_main = int(chosen.user_attrs[\"k_main\"])\n",
        "k_int  = int(chosen.user_attrs[\"k_int\"])\n",
        "dup_inter = int(chosen.user_attrs.get(\"dup_inter\", 1))\n",
        "\n",
        "# Assemble Train+Val and Test\n",
        "trainval_df = pd.concat([train_df, valid_df], axis=0, ignore_index=True)\n",
        "trainval_df = trainval_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "genes_main  = GENE_RANK[:k_main]\n",
        "genes_inter = genes_main[:k_int]\n",
        "X_trv_raw, feat_names = build_features_with_interactions(trainval_df, genes_main, genes_inter, dup_inter=dup_inter)\n",
        "X_te_raw,  _          = build_features_with_interactions(test_df,      genes_main, genes_inter, dup_inter=dup_inter)\n",
        "\n",
        "# Impute (median) on Train+Val; apply to Test\n",
        "med_trv = np.nanmedian(X_trv_raw, axis=0)\n",
        "X_trv = np.where(np.isnan(X_trv_raw), med_trv, X_trv_raw).astype(np.float32)\n",
        "X_te  = np.where(np.isnan(X_te_raw),  med_trv, X_te_raw).astype(np.float32)\n",
        "\n",
        "# Labels and IPTW\n",
        "y_trv = pack_cox_labels(trainval_df[\"OS_MONTHS\"].values, trainval_df[\"OS_STATUS\"].values)\n",
        "y_te  = pack_cox_labels(test_df[\"OS_MONTHS\"].values,     test_df[\"OS_STATUS\"].values)\n",
        "\n",
        "w_trv, ps_model_fin, pi_fin = compute_iptw(trainval_df, covariate_cols=CLIN_FEATS_PRETX)\n",
        "w_te, _, _ = compute_iptw(test_df, covariate_cols=CLIN_FEATS_PRETX, ref_prev=pi_fin, model=ps_model_fin)\n",
        "\n",
        "# DMatrices\n",
        "d_trv = xgb.DMatrix(X_trv, label=y_trv, weight=w_trv, feature_names=feat_names)\n",
        "d_te  = xgb.DMatrix(X_te,  label=y_te,  weight=w_te,  feature_names=feat_names)\n",
        "\n",
        "# Params and rounds\n",
        "params_fin = {\n",
        "    \"objective\": \"survival:cox\",\n",
        "    \"booster\": \"gbtree\",\n",
        "    \"eval_metric\": \"cox-nloglik\",\n",
        "    \"tree_method\": \"hist\",  # switch to 'gpu_hist' if GPU available\n",
        "    \"seed\": 7,\n",
        "    \"nthread\": -1,\n",
        "    **{k: v for k, v in best_hp.items() if k not in [\"num_boost_round\", \"early_stopping_rounds\"]}\n",
        "}\n",
        "num_boost_round_fin = int(best_hp.get(\"num_boost_round\", 1600))\n",
        "early_stopping_rounds_fin = int(best_hp.get(\"early_stopping_rounds\", 125))\n",
        "\n",
        "# Split Train+Val for ES (drives ES on C-index while final metrics use full Train+Val)\n",
        "event_mask = (y_trv > 0).astype(int)\n",
        "idx = np.arange(len(y_trv))\n",
        "tr_idx, va_idx = train_test_split(idx, test_size=0.25, random_state=7, stratify=event_mask)\n",
        "d_tr_es = xgb.DMatrix(X_trv[tr_idx], label=y_trv[tr_idx], weight=w_trv[tr_idx], feature_names=feat_names)\n",
        "d_va_es = xgb.DMatrix(X_trv[va_idx], label=y_trv[va_idx], weight=w_trv[va_idx], feature_names=feat_names)\n",
        "\n",
        "booster_final, evr_final = train_xgb_cox(d_tr_es, d_va_es, params_fin,\n",
        "                                         num_boost_round_fin, early_stopping_rounds_fin)\n",
        "best_ntree_final = booster_final.best_ntree_limit or booster_final.best_iteration + 1\n",
        "\n",
        "# Evaluate Train+Val and Test at the best iteration\n",
        "pred_trv = booster_final.predict(d_trv, iteration_range=(0, best_ntree_final), output_margin=True)\n",
        "pred_te  = booster_final.predict(d_te,  iteration_range=(0, best_ntree_final), output_margin=True)\n",
        "\n",
        "t_trv = np.abs(y_trv); e_trv = (y_trv > 0).astype(int)\n",
        "t_te  = np.abs(y_te);  e_te  = (y_te  > 0).astype(int)\n",
        "\n",
        "ci_trv = cindex(pred_trv, t_trv, e_trv)\n",
        "ci_te  = cindex(pred_te,  t_te,  e_te)\n",
        "\n",
        "print(f\"\\n[Final XGB-Cox] Train+Val CI: {ci_trv:.4f}\")\n",
        "print(f\"[Final XGB-Cox] Test CI:      {ci_te:.4f}\")\n",
        "\n",
        "# Per-arm C-indices (sanity)\n",
        "act_trv = trainval_df[\"Adjuvant Chemo\"].to_numpy(int)\n",
        "act_te  = test_df[\"Adjuvant Chemo\"].to_numpy(int)\n",
        "def ci_by_arm(pred, t, e, arm):\n",
        "    out = {}\n",
        "    for label, mask in [(\"ACT=1\", arm==1), (\"ACT=0\", arm==0)]:\n",
        "        out[label] = cindex(pred[mask], t[mask], e[mask]) if mask.sum() >= 3 else np.nan\n",
        "    return out\n",
        "print(\"[Train+Val] CI by arm:\", ci_by_arm(pred_trv, t_trv, e_trv, act_trv))\n",
        "print(\"[Test]      CI by arm:\", ci_by_arm(pred_te,  t_te,  e_te,  act_te))\n",
        "\n",
        "# Save artifacts\n",
        "OUT_DIR = \"/content/drive/MyDrive/xgb_cox_interactions_iptw_bounded\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "booster_final.save_model(os.path.join(OUT_DIR, \"xgb_cox_final.json\"))\n",
        "with open(os.path.join(OUT_DIR, \"chosen_params.txt\"), \"w\") as f:\n",
        "    f.write(str(best_hp))\n",
        "with open(os.path.join(OUT_DIR, \"features_used.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(feat_names))\n",
        "print(\"Saved final model and parameters to:\", OUT_DIR)\n"
      ],
      "metadata": {
        "id": "szjS_EBRSj1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1EY87_PKv0dTOF6mK_Kx8YPzRYVzJ6o-8",
      "authorship_tag": "ABX9TyP9FvRFg+OG1hfaN4mjsPt1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}