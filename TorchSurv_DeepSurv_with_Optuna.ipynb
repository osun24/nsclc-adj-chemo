{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1EY87_PKv0dTOF6mK_Kx8YPzRYVzJ6o-8",
      "authorship_tag": "ABX9TyOKO6sXNugn5Xk4ik0O2t+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osun24/nsclc-adj-chemo/blob/main/TorchSurv_DeepSurv_with_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torchsurv scikit-survival\n",
        "!pip install optuna optuna-dashboard scikit-survival portpicker\n",
        "\n",
        "# Import required packages\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "\n",
        "# (Optional) Mount Google Drive if you plan to load/save files there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from optuna_dashboard import run_server\n",
        "import threading, time, portpicker\n",
        "from google.colab import output\n",
        "\n",
        "PORT = portpicker.pick_unused_port()\n",
        "\n",
        "def _serve():\n",
        "    run_server(\"sqlite:///deepsurv_optuna.db\", host=\"0.0.0.0\", port=PORT)\n",
        "\n",
        "threading.Thread(target=_serve, daemon=True).start()\n",
        "time.sleep(2)\n",
        "print(\"Dashboard:\", output.eval_js(f\"google.colab.kernel.proxyPort({PORT}, {{'cache': false}})\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "81TEvMle6mqs",
        "outputId": "c24f60e7-dbc6-4fba-db8f-55cbd3449cb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsurv in /usr/local/lib/python3.12/dist-packages (0.1.5)\n",
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.0.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.8.2)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.14.1)\n",
            "Requirement already satisfied: osqp<1.0.0,>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (0.6.7.post3)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.6.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.12/dist-packages (from osqp<1.0.0,>=0.6.3->scikit-survival) (0.1.7.post5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8,>=1.6.1->scikit-survival) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.4.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (25.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchsurv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchsurv) (3.0.3)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: optuna-dashboard in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: bottle>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from optuna-dashboard) (0.13.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from optuna-dashboard) (1.6.1)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.14.1)\n",
            "Requirement already satisfied: osqp<1.0.0,>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (0.6.7.post3)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.16.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from portpicker) (5.9.5)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.12/dist-packages (from osqp<1.0.0,>=0.6.3->scikit-survival) (0.1.7.post5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->optuna-dashboard) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-5 (_serve):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\n",
            "    self.dialect.do_execute(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/default.py\", line 951, in do_execute\n",
            "    cursor.execute(statement, parameters)\n",
            "sqlite3.OperationalError: no such table: version_info\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 77, in _create_scoped_session\n",
            "    yield session\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 1046, in _init_version_info_model\n",
            "    version_info = models.VersionInfoModel.find(session)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/models.py\", line 591, in find\n",
            "    version_info = session.query(cls).one_or_none()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py\", line 2785, in one_or_none\n",
            "    return self._iter().one_or_none()  # type: ignore\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py\", line 2857, in _iter\n",
            "    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(\n",
            "                                                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/session.py\", line 2351, in execute\n",
            "    return self._execute_internal(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/session.py\", line 2249, in _execute_internal\n",
            "    result: Result[Any] = compile_state_cls.orm_execute_statement(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/context.py\", line 306, in orm_execute_statement\n",
            "    result = conn.execute(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1419, in execute\n",
            "    return meth(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/sql/elements.py\", line 526, in _execute_on_connection\n",
            "    return connection._execute_clauseelement(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1641, in _execute_clauseelement\n",
            "    ret = self._execute_context(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1846, in _execute_context\n",
            "    return self._exec_single_context(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1986, in _exec_single_context\n",
            "    self._handle_dbapi_exception(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 2355, in _handle_dbapi_exception\n",
            "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\n",
            "    self.dialect.do_execute(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/default.py\", line 951, in do_execute\n",
            "    cursor.execute(statement, parameters)\n",
            "sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: version_info\n",
            "[SQL: SELECT version_info.version_info_id AS version_info_version_info_id, version_info.schema_version AS version_info_schema_version, version_info.library_version AS version_info_library_version \n",
            "FROM version_info]\n",
            "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-3330911947.py\", line 29, in _serve\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_app.py\", line 658, in run_server\n",
            "    app = create_app(get_storage(storage), artifact_store=store)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_storage_url.py\", line 59, in get_storage\n",
            "    return guess_storage_from_url(storage)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_storage_url.py\", line 84, in guess_storage_from_url\n",
            "    return get_rdb_storage(storage_url)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna_dashboard/_storage_url.py\", line 91, in get_rdb_storage\n",
            "    return RDBStorage(storage_url, skip_compatibility_check=True, skip_table_creation=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 240, in __init__\n",
            "    self._version_manager = _VersionManager(self.url, self.engine, self.scoped_session)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 1041, in __init__\n",
            "    self._init_version_info_model()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 1045, in _init_version_info_model\n",
            "    with _create_scoped_session(self.scoped_session, True) as session:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/storages/_rdb/storage.py\", line 95, in _create_scoped_session\n",
            "    raise optuna.exceptions.StorageInternalError(message) from e\n",
            "optuna.exceptions.StorageInternalError: An exception is raised during the commit. This typically happens due to invalid data in the commit, e.g. exceeding max length. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dashboard: https://40159-gpu-a100-s-ux6s319d4nv1-f.us-central1-1.prod.colab.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab-ready single cell: DeepSurv + Optuna HPO + Dashboard\n",
        "# - Keeps ONLY requested clinical vars + genes with Prop==1\n",
        "# - Sorts Train/Val by OS_MONTHS/OS_STATUS (desc)\n",
        "# - Standardizes using TRAIN-only (applies to VAL); after HPO\n",
        "#   restandardizes on TRAIN+VAL and evaluates TEST C-index\n",
        "# - Optuna + Successive Halving pruner\n",
        "# - Optuna Dashboard (proxied URL printed)  [fixed run_server args]\n",
        "# - Encodes architectures as strings to avoid Optuna warning\n",
        "# ============================================================\n",
        "\n",
        "# ---------- (Optional) Mount Google Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, math, copy, warnings, random, gc, time, threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "# Cox loss: Prefer torchsurv if available; fallback to Efron implementation\n",
        "try:\n",
        "    from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "    _HAS_TORCHSURV = True\n",
        "except Exception:\n",
        "    _HAS_TORCHSURV = False\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "\n",
        "import optuna\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# Cox loss fallback (Efron) if torchsurv isn't available\n",
        "# ============================================================\n",
        "def _cox_negloglik_efron(pred, event, time):\n",
        "    eta = pred.reshape(-1)\n",
        "    e = event.to(torch.float32).reshape(-1)\n",
        "    t = time.reshape(-1)\n",
        "\n",
        "    order = torch.argsort(t, descending=True)\n",
        "    t = t[order]; e = e[order]; eta = eta[order]\n",
        "    exp_eta = torch.exp(eta)\n",
        "    cum_exp = torch.cumsum(exp_eta, dim=0)\n",
        "\n",
        "    uniq_mask = torch.ones_like(t, dtype=torch.bool)\n",
        "    uniq_mask[1:] = t[1:] != t[:-1]\n",
        "    idxs = torch.nonzero(uniq_mask, as_tuple=False).reshape(-1)\n",
        "    idxs = torch.cat([idxs, torch.tensor([len(t)], device=t.device)])\n",
        "\n",
        "    nll = torch.tensor(0.0, device=t.device)\n",
        "    for k in range(len(idxs)-1):\n",
        "        start, end = idxs[k].item(), idxs[k+1].item()\n",
        "        e_slice = e[start:end]\n",
        "        d = int(e_slice.sum().item())\n",
        "        if d == 0: continue\n",
        "        eta_events = eta[start:end][e_slice.bool()]\n",
        "        exp_events = exp_eta[start:end][e_slice.bool()]\n",
        "        s_eta = eta_events.sum()\n",
        "        risk_sum = cum_exp[end-1]\n",
        "        s_exp = exp_events.sum()\n",
        "        eps = 1e-12\n",
        "        log_terms = 0.0\n",
        "        for j in range(d):\n",
        "            log_terms = log_terms + torch.log(risk_sum - (j / d) * s_exp + eps)\n",
        "        nll = nll - (s_eta - log_terms)\n",
        "    return nll / t.numel()\n",
        "\n",
        "def cox_negloglik(pred, event, time):\n",
        "    if _HAS_TORCHSURV:\n",
        "        return neg_partial_log_likelihood(pred, event, time, reduction='mean')\n",
        "    return _cox_negloglik_efron(pred, event, time)\n",
        "\n",
        "# ============================================================\n",
        "# Model, Dataset, Sampler, Utilities\n",
        "# ============================================================\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(d, units), activation]\n",
        "            if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "            d = units\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events.astype(bool), dtype=torch.bool)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.time[idx], self.event[idx]\n",
        "\n",
        "class EventBalancedBatchSampler(Sampler):\n",
        "    def __init__(self, events_numpy, batch_size, seed=0):\n",
        "        events = np.asarray(events_numpy).astype(bool)\n",
        "        self.pos_idx = np.where(events)[0]\n",
        "        self.neg_idx = np.where(~events)[0]\n",
        "        assert len(self.pos_idx) > 0, \"No events in training set — cannot balance batches.\"\n",
        "        self.bs = int(batch_size)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "    def __iter__(self):\n",
        "        pos = self.rng.permutation(self.pos_idx)\n",
        "        neg = self.rng.permutation(self.neg_idx)\n",
        "        n_total = len(pos) + len(neg)\n",
        "        n_batches = math.ceil(n_total / self.bs)\n",
        "        pi = ni = 0\n",
        "        for _ in range(n_batches):\n",
        "            take_pos = 1 if pi < len(pos) else 0\n",
        "            avail_neg = max(0, len(neg) - ni)\n",
        "            take_neg = min(self.bs - take_pos, avail_neg)\n",
        "            need = self.bs - (take_pos + take_neg)\n",
        "            extra_pos = min(need, max(0, len(pos) - (pi + take_pos)))\n",
        "            take_pos += extra_pos\n",
        "            batch = np.concatenate([pos[pi:pi+take_pos], neg[ni:ni+take_neg]])\n",
        "            pi += take_pos; ni += take_neg\n",
        "            if batch.size == 0: break\n",
        "            self.rng.shuffle(batch)\n",
        "            yield batch.tolist()\n",
        "    def __len__(self):\n",
        "        return math.ceil((len(self.pos_idx) + len(self.neg_idx)) / self.bs)\n",
        "\n",
        "def make_optimizer(model, lr, wd):\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight):\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "    param_groups = [{'params': decay, 'weight_decay': wd},\n",
        "                    {'params': no_decay, 'weight_decay': 0.0}]\n",
        "    return optim.AdamW(param_groups, lr=lr)\n",
        "\n",
        "def set_dropout_p(model, p):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout): m.p = float(p)\n",
        "\n",
        "def set_weight_decay(optimizer, wd):\n",
        "    for g in optimizer.param_groups: g['weight_decay'] = float(wd)\n",
        "\n",
        "def l1_penalty_first_layer(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear): return m.weight.abs().sum()\n",
        "    return torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_ci(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, times, events = [], [], []\n",
        "    for x, t, e in dataloader:\n",
        "        y = torch.clamp(model(x.to(device)), -20, 20)\n",
        "        preds.append(y.cpu().numpy().ravel())\n",
        "        times.append(t.numpy()); events.append(e.numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    times = np.concatenate(times); events = np.concatenate(events)\n",
        "    if np.isnan(preds).any(): return -np.inf\n",
        "    return concordance_index_censored(events.astype(bool), times, preds)[0]\n",
        "\n",
        "def train_one_epoch(model, optimizer, dataloader, device, l1_lambda=0.0, epoch=0, warmup_epochs=20):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))\n",
        "    loss_sum, n_seen = 0.0, 0\n",
        "    for x, t, e in dataloader:\n",
        "        if e.sum().item() == 0:  # safety (shouldn't happen with balanced sampler)\n",
        "            continue\n",
        "        x, t, e = x.to(device), t.to(device), e.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = cox_negloglik(out, e, t)\n",
        "        if l1_lambda > 0:\n",
        "            loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        n_seen += x.size(0)\n",
        "    return {'avg_loss': loss_sum / max(n_seen, 1), 'warm': warm}\n",
        "\n",
        "def full_risk_set_step(model, optimizer, ds, device, l1_lambda=0.0, warm=1.0):\n",
        "    model.train()\n",
        "    X_all = ds.x.to(device); t_all = ds.time.to(device); e_all = ds.event.to(device)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(X_all), -20, 20)\n",
        "    loss_full = cox_negloglik(out_all, e_all, t_all)\n",
        "    if l1_lambda > 0:\n",
        "        loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "# ============================================================\n",
        "# Data loading & preprocessing\n",
        "# ============================================================\n",
        "# Default CSV paths (adjust if needed)\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/affyfRMATrain.csv\"\n",
        "VALID_CSV = \"/content/drive/MyDrive/affyfRMAValidation.csv\"\n",
        "TEST_CSV  = \"/content/drive/MyDrive/affyfRMATest.csv\"\n",
        "\n",
        "# Genes list path (uploaded or Drive)\n",
        "GENES_CSV = \"/mnt/data/Genes.csv\"\n",
        "if not os.path.exists(GENES_CSV):\n",
        "    if os.path.exists(\"/content/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/Genes.csv\"\n",
        "    elif os.path.exists(\"/content/drive/MyDrive/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/drive/MyDrive/Genes.csv\"\n",
        "print(\"Genes.csv path:\", GENES_CSV)\n",
        "\n",
        "# Clinical variables to KEEP (exact names)\n",
        "CLINICAL_VARS = [\n",
        "    \"Adjuvant Chemo\",\"Age\",\"IS_MALE\",\n",
        "    \"Stage_IA\",\"Stage_IB\",\"Stage_II\",\"Stage_III\",\n",
        "    \"Histology_Adenocarcinoma\",\"Histology_Large Cell Carcinoma\",\"Histology_Squamous Cell Carcinoma\",\n",
        "    \"Race_African American\",\"Race_Asian\",\"Race_Caucasian\",\"Race_Native Hawaiian or Other Pacific Islander\",\"Race_Unknown\",\n",
        "    \"Smoked?_No\",\"Smoked?_Unknown\",\"Smoked?_Yes\"\n",
        "]\n",
        "\n",
        "def load_genes_list(genes_csv):\n",
        "    g = pd.read_csv(genes_csv)\n",
        "    if not {\"Gene\",\"Prop\"}.issubset(set(g.columns)):\n",
        "        raise ValueError(f\"Genes.csv must contain 'Gene' and 'Prop' columns. Found: {list(g.columns)}\")\n",
        "    genes = g.loc[g[\"Prop\"] == 1, \"Gene\"].astype(str).tolist()\n",
        "    print(f\"[Genes] Selected {len(genes)} genes with Prop == 1\")\n",
        "    return genes\n",
        "\n",
        "def coerce_survival_cols(df):\n",
        "    # Map to integers {0,1}\n",
        "    if df[\"OS_STATUS\"].dtype == object:\n",
        "        df[\"OS_STATUS\"] = df[\"OS_STATUS\"].replace({\"DECEASED\":1,\"LIVING\":0,\"Dead\":1,\"Alive\":0}).astype(int)\n",
        "    else:\n",
        "        df[\"OS_STATUS\"] = pd.to_numeric(df[\"OS_STATUS\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"OS_MONTHS\"] = pd.to_numeric(df[\"OS_MONTHS\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def preprocess_split(df, clinical_vars, gene_names):\n",
        "    if \"Adjuvant Chemo\" in df.columns:\n",
        "        df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
        "    for col in [\"Adjuvant Chemo\",\"IS_MALE\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df = coerce_survival_cols(df)\n",
        "    keep_cols = [c for c in clinical_vars if c in df.columns] + [g for g in gene_names if g in df.columns]\n",
        "    missing_clin = [c for c in clinical_vars if c not in df.columns]\n",
        "    if missing_clin:\n",
        "        print(f\"[WARN] Missing clinical columns: {missing_clin}\")\n",
        "    if len(keep_cols) == 0:\n",
        "        raise ValueError(\"No feature columns found after filtering clinical+genes.\")\n",
        "    cols = [\"OS_STATUS\",\"OS_MONTHS\"] + keep_cols\n",
        "    return df[cols].copy()\n",
        "\n",
        "# Load CSVs\n",
        "train_raw = pd.read_csv(TRAIN_CSV)\n",
        "valid_raw = pd.read_csv(VALID_CSV)\n",
        "test_raw  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# Load genes (Prop==1)\n",
        "GENE_LIST = load_genes_list(GENES_CSV)\n",
        "\n",
        "# Reduce to requested columns per split\n",
        "train_df = preprocess_split(train_raw, CLINICAL_VARS, GENE_LIST)\n",
        "valid_df = preprocess_split(valid_raw, CLINICAL_VARS, GENE_LIST)\n",
        "test_df  = preprocess_split(test_raw,  CLINICAL_VARS, GENE_LIST)\n",
        "\n",
        "# Ensure consistent columns across splits (intersection)\n",
        "feat_candidates = [c for c in (CLINICAL_VARS + GENE_LIST)\n",
        "                   if c in train_df.columns and c in valid_df.columns and c in test_df.columns]\n",
        "if len(feat_candidates) == 0:\n",
        "    raise ValueError(\"After filtering, no common features across train/val/test.\")\n",
        "print(f\"[Features] Using {len(feat_candidates)} common features.\")\n",
        "\n",
        "# Sort Train/Val by event time & status (descending)\n",
        "train_df = train_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "valid_df = valid_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "# Build arrays & TRAIN-only standardization (apply to VAL)\n",
        "X_train = train_df[feat_candidates].values.astype(np.float32)\n",
        "X_valid = valid_df[feat_candidates].values.astype(np.float32)\n",
        "\n",
        "train_medians = np.nanmedian(X_train, axis=0)\n",
        "X_train = np.where(np.isnan(X_train), train_medians, X_train)\n",
        "X_valid = np.where(np.isnan(X_valid), train_medians, X_valid)\n",
        "\n",
        "scaler_tv = StandardScaler().fit(X_train)\n",
        "X_train = scaler_tv.transform(X_train).astype(np.float32)\n",
        "X_valid = scaler_tv.transform(X_valid).astype(np.float32)\n",
        "\n",
        "ytr_time = train_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "ytr_event = train_df[\"OS_STATUS\"].values.astype(int)\n",
        "yva_time = valid_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "yva_event = valid_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_ds = SurvivalDataset(X_train, ytr_time, ytr_event)\n",
        "valid_ds = SurvivalDataset(X_valid, yva_time, yva_event)\n",
        "\n",
        "train_sampler = EventBalancedBatchSampler(ytr_event, BATCH_SIZE, seed=42)\n",
        "train_loader  = DataLoader(train_ds, batch_sampler=train_sampler, num_workers=0)\n",
        "train_eval_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "valid_loader      = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "in_dim = X_train.shape[1]\n",
        "print(\"Input dim:\", in_dim)\n",
        "\n",
        "# ============================================================\n",
        "# Optuna Objective & Study  (arch encoded as strings -> parsed)\n",
        "# ============================================================# ============================================================\n",
        "# Optuna Objective & Study — Expanded Space to Reduce Overfitting\n",
        "#   * Architectures include smaller, narrower and bottlenecks\n",
        "#   * Stronger regularizers: higher dropout, input dropout, L1/L2\n",
        "#   * Optionally apply WD to final layer\n",
        "#   * Batch size, grad clip, scheduler, epochs per trial\n",
        "#   * Input Gaussian noise\n",
        "# ============================================================\n",
        "\n",
        "# Wider set of architectures (strings -> parsed lists)\n",
        "ARCH_CHOICES = (\n",
        "    # very small\n",
        "    \"16\", \"32\",\n",
        "    # small / medium singles\n",
        "    \"64\", \"128\", \"256\",\n",
        "    # conservative big single\n",
        "    \"512\",\n",
        "    # 2-layer bottlenecks and symmetric\n",
        "    \"32-16\", \"32-32\", \"64-32\", \"64-64\",\n",
        "    \"128-64\", \"128-128\", \"256-128\", \"256-256\",\n",
        "    \"512-256\", \"512-512\",\n",
        "    # 3-layer (narrowing)\n",
        "    \"64-32-16\", \"128-64-32\", \"256-128-64\"\n",
        ")\n",
        "\n",
        "def layers_from_arch(arch_str: str):\n",
        "    return [int(x) for x in arch_str.split(\"-\") if x.strip()]\n",
        "\n",
        "def suggest_hparams(trial):\n",
        "    # ----- STATIC spaces only -----\n",
        "    # Main-gene budget: choose from a fixed set (truncate to MAX_GENES at runtime)\n",
        "    TOPK_MAIN_CHOICES = tuple([k for k in (32, 64, 128, 256, 512, 800, MAX_GENES) if k <= MAX_GENES])\n",
        "    top_k_genes = trial.suggest_categorical(\"top_k_genes\", TOPK_MAIN_CHOICES)\n",
        "\n",
        "    # Interaction-gene budget: FIXED choices; we’ll clamp to <= top_k_genes after sampling\n",
        "    TOPK_INTER_CHOICES = tuple([k for k in (0, 16, 32, 64, 128, 256, 512) if k <= MAX_GENES])\n",
        "    top_k_inter_raw = trial.suggest_categorical(\"top_k_inter\", TOPK_INTER_CHOICES)\n",
        "\n",
        "    # net & regularizers\n",
        "    arch = trial.suggest_categorical(\"arch\", ARCH_CHOICES)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.10, 0.70)\n",
        "    input_dropout = trial.suggest_float(\"input_dropout\", 0.00, 0.30)\n",
        "    noise_std = trial.suggest_float(\"noise_std\", 0.0, 0.10)\n",
        "\n",
        "    wd = trial.suggest_float(\"wd\", 1e-6, 1e-1, log=True)\n",
        "    apply_final_wd = trial.suggest_categorical(\"apply_final_wd\", (0, 1))\n",
        "    use_l1 = trial.suggest_categorical(\"use_l1\", (0, 1))\n",
        "    l1 = 0.0 if use_l1 == 0 else trial.suggest_float(\"l1\", 1e-8, 3e-3, log=True)\n",
        "\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
        "    sched = trial.suggest_categorical(\"sched\", (\"cosine\", \"cawr\", \"none\"))\n",
        "    if sched == \"cawr\":\n",
        "        cawr_T0 = trial.suggest_int(\"cawr_T0\", 16, 80, step=8)\n",
        "        cawr_Tmult = trial.suggest_categorical(\"cawr_Tmult\", (1, 2, 3))\n",
        "    else:\n",
        "        cawr_T0 = cawr_Tmult = None\n",
        "\n",
        "    epochs = trial.suggest_int(\"epochs\", 64, 384, step=32)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", (32, 64, 128))\n",
        "    grad_clip = trial.suggest_float(\"grad_clip\", 1.0, 10.0)\n",
        "\n",
        "    # Clamp interactions to main-gene budget (no dynamic space in Optuna, just post-hoc enforcement)\n",
        "    top_k_inter = int(min(top_k_inter_raw, top_k_genes))\n",
        "\n",
        "    return dict(\n",
        "        arch=arch, top_k_genes=int(top_k_genes), top_k_inter=int(top_k_inter),\n",
        "        dropout=dropout, input_dropout=input_dropout, noise_std=noise_std,\n",
        "        wd=wd, apply_final_wd=int(apply_final_wd), use_l1=int(use_l1), l1=float(l1),\n",
        "        lr=lr, sched=sched, cawr_T0=cawr_T0, cawr_Tmult=cawr_Tmult,\n",
        "        epochs=int(epochs), batch_size=int(batch_size), grad_clip=float(grad_clip)\n",
        "    )\n",
        "\n",
        "\n",
        "# Warmups (as before)\n",
        "MAX_EPOCHS_CAP = 512  # absolute cap (safety)\n",
        "WARMUP_EPOCHS_L1 = 30\n",
        "WARMUP_EPOCHS_DROPOUT = 30\n",
        "WARMUP_EPOCHS_WD = 30\n",
        "DROPOUT_START = 0.15\n",
        "WD_START = 0.0\n",
        "\n",
        "# Local helpers that add input dropout/noise and variable grad clip\n",
        "def _apply_input_dropout(x, p):\n",
        "    if p <= 0.0: return x\n",
        "    # inverted dropout on features\n",
        "    keep = 1.0 - p\n",
        "    mask = torch.bernoulli(torch.full_like(x, keep))\n",
        "    return x * mask / max(keep, 1e-6)\n",
        "\n",
        "def train_one_epoch_reg(model, optimizer, dataloader, device,\n",
        "                        l1_lambda=0.0, epoch=0, warmup_epochs=20,\n",
        "                        input_dropout=0.0, noise_std=0.0, grad_clip=5.0):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))\n",
        "    loss_sum, n_seen = 0.0, 0\n",
        "    for x, t, e in dataloader:\n",
        "        if e.sum().item() == 0:\n",
        "            continue\n",
        "        x, t, e = x.to(device), t.to(device), e.to(device)\n",
        "\n",
        "        # data-level regularization\n",
        "        if input_dropout > 0.0:\n",
        "            x = _apply_input_dropout(x, input_dropout)\n",
        "        if noise_std > 0.0:\n",
        "            x = x + noise_std * torch.randn_like(x)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = cox_negloglik(out, e, t)\n",
        "        if l1_lambda > 0:\n",
        "            loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        n_seen += x.size(0)\n",
        "    return {'avg_loss': loss_sum / max(n_seen, 1), 'warm': warm}\n",
        "\n",
        "def full_risk_set_step_reg(model, optimizer, ds, device,\n",
        "                           l1_lambda=0.0, warm=1.0,\n",
        "                           input_dropout=0.0, noise_std=0.0, grad_clip=5.0):\n",
        "    model.train()\n",
        "    X_all = ds.x.to(device); t_all = ds.time.to(device); e_all = ds.event.to(device)\n",
        "    XX = X_all\n",
        "    if input_dropout > 0.0:\n",
        "        XX = _apply_input_dropout(XX, input_dropout)\n",
        "    if noise_std > 0.0:\n",
        "        XX = XX + noise_std * torch.randn_like(XX)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(XX), -20, 20)\n",
        "    loss_full = cox_negloglik(out_all, e_all, t_all)\n",
        "    if l1_lambda > 0:\n",
        "        loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "# Optimizer that can optionally apply WD to the final layer\n",
        "def make_optimizer_hpo(model, lr, wd, apply_final_wd=False):\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight) and not apply_final_wd:\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "    param_groups = [{'params': decay, 'weight_decay': float(wd)},\n",
        "                    {'params': no_decay, 'weight_decay': 0.0}]\n",
        "    return optim.AdamW(param_groups, lr=float(lr))\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_hparams(trial)\n",
        "    layers = layers_from_arch(hp[\"arch\"])\n",
        "\n",
        "    # Build loaders with TRIAL-SPECIFIC batch size (smaller batches often regularize more)\n",
        "    bs = int(hp[\"batch_size\"])\n",
        "    tr_sampler = EventBalancedBatchSampler(ytr_event, bs, seed=42)\n",
        "    tr_loader = DataLoader(train_ds, batch_sampler=tr_sampler, num_workers=0)\n",
        "    tr_eval_loader = DataLoader(train_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "    va_loader = DataLoader(valid_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Model / optimizer\n",
        "    model = DeepSurvMLP(in_dim, layers, dropout=hp[\"dropout\"]).to(device)\n",
        "    optimizer = make_optimizer_hpo(model, lr=hp[\"lr\"], wd=hp[\"wd\"],\n",
        "                                   apply_final_wd=bool(hp[\"apply_final_wd\"]))\n",
        "\n",
        "    # Scheduler per trial\n",
        "    epochs = int(min(hp[\"epochs\"], MAX_EPOCHS_CAP))\n",
        "    if hp[\"sched\"] == \"cosine\":\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "        def step_sched(epoch_idx): scheduler.step()\n",
        "    elif hp[\"sched\"] == \"cawr\":\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=int(hp[\"cawr_T0\"]), T_mult=int(hp[\"cawr_Tmult\"])\n",
        "        )\n",
        "        def step_sched(epoch_idx): scheduler.step(epoch_idx + 1)\n",
        "    else:\n",
        "        scheduler = None\n",
        "        def step_sched(epoch_idx): pass\n",
        "\n",
        "    best_val_ci = -np.inf\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Warm-up schedules for dropout & WD\n",
        "        frac_d = min(1.0, epoch / float(WARMUP_EPOCHS_DROPOUT))\n",
        "        frac_w = min(1.0, epoch / float(WARMUP_EPOCHS_WD))\n",
        "        set_dropout_p(model, DROPOUT_START + (hp['dropout'] - DROPOUT_START) * frac_d)\n",
        "        set_weight_decay(optimizer, WD_START + (hp['wd'] - WD_START) * frac_w)\n",
        "\n",
        "        # One epoch + full risk-set step with extra regularizers\n",
        "        stats = train_one_epoch_reg(\n",
        "            model, optimizer, tr_loader, device,\n",
        "            l1_lambda=hp[\"l1\"], epoch=epoch, warmup_epochs=WARMUP_EPOCHS_L1,\n",
        "            input_dropout=hp[\"input_dropout\"], noise_std=hp[\"noise_std\"],\n",
        "            grad_clip=hp[\"grad_clip\"]\n",
        "        )\n",
        "        _ = full_risk_set_step_reg(\n",
        "            model, optimizer, train_ds, device,\n",
        "            l1_lambda=hp[\"l1\"], warm=stats['warm'],\n",
        "            input_dropout=hp[\"input_dropout\"], noise_std=hp[\"noise_std\"],\n",
        "            grad_clip=hp[\"grad_clip\"]\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_ci = evaluate_ci(model, va_loader, device)\n",
        "        step_sched(epoch)\n",
        "\n",
        "        # Report for pruning\n",
        "        trial.report(val_ci, step=epoch)\n",
        "        if val_ci > best_val_ci:\n",
        "            best_val_ci = val_ci\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "        if trial.should_prune():\n",
        "            # Clean up GPU mem before pruning\n",
        "            del model, optimizer, scheduler\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    trial.set_user_attr(\"best_epoch\", int(best_epoch))\n",
        "\n",
        "    # Clean up\n",
        "    del model, optimizer, scheduler\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return best_val_ci\n",
        "\n",
        "# ---- Study / Sampler / Pruner (tune TPE a bit for broader exploration) ----\n",
        "storage = \"sqlite:///deepsurv_optuna.db\"\n",
        "study_name = \"deepsurv_cox_hpo_overfit_reducer\"\n",
        "\n",
        "sampler = optuna.samplers.TPESampler(\n",
        "    seed=42,\n",
        "    multivariate=True,\n",
        "    group=True,\n",
        "    n_startup_trials=40,       # explore more before exploitation\n",
        "    constant_liar=True,        # better parallel behavior if you run parallel\n",
        "    consider_prior=True\n",
        ")\n",
        "\n",
        "# Hyperband/SH both work; Hyperband gives a bit more flexibility across resource levels\n",
        "pruner = optuna.pruners.HyperbandPruner(min_resource=16, reduction_factor=3)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",       # single objective (Val CI)\n",
        "    study_name=study_name,\n",
        "    storage=storage,\n",
        "    load_if_exists=True,\n",
        "    sampler=sampler,\n",
        "    pruner=pruner\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Launch Optuna Dashboard (Colab proxied URL printed)\n",
        "#   (No unsupported args; previous TypeError fixed)\n",
        "# ============================================================\n",
        "try:\n",
        "    from optuna_dashboard import run_server\n",
        "    from google.colab import output\n",
        "    import portpicker\n",
        "    PORT = portpicker.pick_unused_port()\n",
        "    def _start_dashboard():\n",
        "        # NOTE: do NOT pass unsupported kwargs like reload/quiet\n",
        "        run_server(storage, host=\"0.0.0.0\", port=PORT)\n",
        "    t = threading.Thread(target=_start_dashboard, daemon=True)\n",
        "    t.start()\n",
        "    time.sleep(2)\n",
        "    dash_url = output.eval_js(f\"google.colab.kernel.proxyPort({PORT}, {{'cache': false}})\")\n",
        "    print(\"Optuna Dashboard:\", dash_url)\n",
        "except Exception as ex:\n",
        "    print(\"[Optuna Dashboard] Could not start dashboard automatically.\", ex)\n",
        "    print(\"You can run it locally with:  optuna-dashboard sqlite:///deepsurv_optuna.db\")\n",
        "\n",
        "# ============================================================\n",
        "# Run Optimization\n",
        "# ============================================================\n",
        "N_TRIALS = 100  # adjust as needed\n",
        "print(f\"Starting optimization: {N_TRIALS} trials × up to {MAX_EPOCHS_CAP} epochs\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)\n",
        "\n",
        "print(\"\\n[Best] Val CI:\", study.best_value)\n",
        "print(\"[Best] Params:\", study.best_params)\n",
        "print(\"[Best] Best epoch:\", study.best_trial.user_attrs.get(\"best_epoch\", MAX_EPOCHS_CAP))\n",
        "\n",
        "# ============================================================\n",
        "# Retrain on Train+Val with best hyperparams; evaluate Test\n",
        "# - Combine Train+Val, sort, restandardize; apply to Test\n",
        "# ============================================================\n",
        "# Prepare Train+Val\n",
        "trainval_df = pd.concat([train_df, valid_df], axis=0, ignore_index=True)\n",
        "trainval_df = trainval_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "X_trv = trainval_df[feat_candidates].values.astype(np.float32)\n",
        "y_trv_time = trainval_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "y_trv_event = trainval_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "# Median impute by Train+Val medians (for retraining phase)\n",
        "trv_medians = np.nanmedian(X_trv, axis=0)\n",
        "X_trv = np.where(np.isnan(X_trv), trv_medians, X_trv)\n",
        "\n",
        "scaler_trv = StandardScaler().fit(X_trv)\n",
        "X_trv = scaler_trv.transform(X_trv).astype(np.float32)\n",
        "\n",
        "# Test set standardized with Train+Val scaler\n",
        "X_test = test_df[feat_candidates].values.astype(np.float32)\n",
        "X_test = np.where(np.isnan(X_test), trv_medians, X_test)\n",
        "X_test = scaler_trv.transform(X_test).astype(np.float32)\n",
        "y_te_time = test_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "y_te_event = test_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "# Loaders\n",
        "BATCH_SIZE = 64\n",
        "trv_ds = SurvivalDataset(X_trv, y_trv_time, y_trv_event)\n",
        "te_ds  = SurvivalDataset(X_test, y_te_time, y_te_event)\n",
        "\n",
        "trv_sampler = EventBalancedBatchSampler(y_trv_event, BATCH_SIZE, seed=7)\n",
        "trv_loader  = DataLoader(trv_ds, batch_sampler=trv_sampler, num_workers=0)\n",
        "trv_eval_loader = DataLoader(trv_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "te_loader  = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# Build & train final model\n",
        "best_hp = study.best_params\n",
        "l1_final = float(best_hp.get(\"l1\", 0.0))\n",
        "best_layers = layers_from_arch(best_hp[\"arch\"])\n",
        "best_n_epochs = int(study.best_trial.user_attrs.get(\"best_epoch\", MAX_EPOCHS_CAP))\n",
        "best_n_epochs = max(16, min(best_n_epochs, MAX_EPOCHS_CAP))\n",
        "\n",
        "model_final = DeepSurvMLP(in_dim, best_layers, dropout=best_hp[\"dropout\"]).to(device)\n",
        "opt_final = make_optimizer(model_final, lr=best_hp[\"lr\"], wd=best_hp[\"wd\"])\n",
        "sched_final = torch.optim.lr_scheduler.CosineAnnealingLR(opt_final, T_max=best_n_epochs)\n",
        "\n",
        "for epoch in range(best_n_epochs):\n",
        "    frac_d = min(1.0, epoch / float(WARMUP_EPOCHS_DROPOUT))\n",
        "    frac_w = min(1.0, epoch / float(WARMUP_EPOCHS_WD))\n",
        "    set_dropout_p(model_final, DROPOUT_START + (best_hp['dropout'] - DROPOUT_START) * frac_d)\n",
        "    set_weight_decay(opt_final, WD_START + (best_hp['wd'] - WD_START) * frac_w)\n",
        "\n",
        "    stats = train_one_epoch(model_final, opt_final, trv_loader, device, l1_lambda=best_hp[\"l1\"],\n",
        "                            epoch=epoch, warmup_epochs=WARMUP_EPOCHS_L1)\n",
        "    _ = full_risk_set_step(model_final, opt_final, trv_ds, device, l1_lambda=best_hp[\"l1\"], warm=stats['warm'])\n",
        "    sched_final.step()\n",
        "\n",
        "# Evaluate\n",
        "trainval_ci = evaluate_ci(model_final, trv_eval_loader, device)\n",
        "test_ci = evaluate_ci(model_final, te_loader, device)\n",
        "print(f\"\\n[Final] Train+Val CI: {trainval_ci:.4f}\")\n",
        "print(f\"[Final] Test CI:      {test_ci:.4f}\")\n",
        "\n",
        "# (Optional) Save artifacts to Drive\n",
        "OUT_DIR = \"/content/drive/MyDrive/deepsurv_results_optuna\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "torch.save(model_final.state_dict(), os.path.join(OUT_DIR, \"deepsurv_best.pt\"))\n",
        "with open(os.path.join(OUT_DIR, \"best_params.txt\"), \"w\") as f:\n",
        "    f.write(str(study.best_params))\n",
        "print(\"Saved final model and best params to:\", OUT_DIR)\n"
      ],
      "metadata": {
        "id": "lbcrdoE4tyWq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "100b3983-19b4-41e6-b7c3-f8f754836f04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "Genes.csv path: /content/drive/MyDrive/Genes.csv\n",
            "[Genes] Selected 1555 genes with Prop == 1\n",
            "[Features] Using 1573 common features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3928505025.py:260: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-3928505025.py:260: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-3928505025.py:260: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim: 1573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.\n",
            "  warnings.warn(\n",
            "[I 2025-10-19 19:17:47,670] A new study created in RDB with name: deepsurv_cox_hpo_overfit_reducer\n",
            "Bottle v0.13.4 server starting up (using WSGIRefServer())...\n",
            "Listening on http://0.0.0.0:38099/\n",
            "Hit Ctrl-C to quit.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna Dashboard: https://38099-gpu-a100-s-2cm35yjghnl2l-b.asia-southeast1-0.prod.colab.dev\n",
            "Starting optimization: 100 trials × up to 512 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-10-19 19:18:28,622] Trial 0 failed with parameters: {'arch': '128-128', 'dropout': 0.27473748411882515, 'input_dropout': 0.18355586841671384, 'wd': 4.982752357076453e-06, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 1.240616395320886e-07, 'lr': 7.475992999956501e-05, 'sched': 'none', 'epochs': 128, 'batch_size': 128, 'grad_clip': 8.275576133048151, 'noise_std': 0.03046137691733707} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3928505025.py\", line 536, in objective\n",
            "    _ = full_risk_set_step_reg(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3928505025.py\", line 467, in full_risk_set_step_reg\n",
            "    loss_full.backward()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 647, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 829, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-10-19 19:18:28,625] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3928505025.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0mN_TRIALS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m  \u001b[0;31m# adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting optimization: {N_TRIALS} trials × up to {MAX_EPOCHS_CAP} epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_TRIALS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n[Best] Val CI:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     ):\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3928505025.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mgrad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"grad_clip\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         )\n\u001b[0;32m--> 536\u001b[0;31m         _ = full_risk_set_step_reg(\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0ml1_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'warm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3928505025.py\u001b[0m in \u001b[0;36mfull_risk_set_step_reg\u001b[0;34m(model, optimizer, ds, device, l1_lambda, warm, input_dropout, noise_std, grad_clip)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ml1_lambda\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mloss_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_full\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml1_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwarm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml1_penalty_first_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0mloss_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Ready-to-run: Retrain on Train+Val with given best params, then eval on Test ====\n",
        "\n",
        "# --- Use the reported best params & epoch (handles missing 'l1') ---\n",
        "best_hp = {\n",
        "    'arch': '512',\n",
        "    'dropout': 0.385912835673872,\n",
        "    'input_dropout': 0.061771953713056,  # not used in final fit below\n",
        "    'wd': 3.3415205446076416e-06,\n",
        "    'apply_final_wd': 0,                 # not used in final fit below\n",
        "    'use_l1': 0,\n",
        "    'lr': 6.945321990789577e-05,\n",
        "    'sched': 'cosine',\n",
        "    'epochs': 192,\n",
        "    'batch_size': 32,\n",
        "    'grad_clip': 8.251469896233349,      # not used in final fit below\n",
        "    'noise_std': 0.09246940755563608     # not used in final fit below\n",
        "}\n",
        "best_n_epochs = 26\n",
        "l1_final = float(best_hp.get(\"l1\", 0.0))  # <- fix: default L1 when use_l1==0\n",
        "\n",
        "# --- Build Train+Val, restandardize; transform Test ---\n",
        "trainval_df = pd.concat([train_df, valid_df], axis=0, ignore_index=True)\n",
        "trainval_df = trainval_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "X_trv = trainval_df[feat_candidates].values.astype(np.float32)\n",
        "y_trv_time = trainval_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "y_trv_event = trainval_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "trv_medians = np.nanmedian(X_trv, axis=0)\n",
        "X_trv = np.where(np.isnan(X_trv), trv_medians, X_trv)\n",
        "scaler_trv = StandardScaler().fit(X_trv)\n",
        "X_trv = scaler_trv.transform(X_trv).astype(np.float32)\n",
        "\n",
        "X_test = test_df[feat_candidates].values.astype(np.float32)\n",
        "X_test = np.where(np.isnan(X_test), trv_medians, X_test)\n",
        "X_test = scaler_trv.transform(X_test).astype(np.float32)\n",
        "y_te_time = test_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "y_te_event = test_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "# --- Datasets & Loaders ---\n",
        "BATCH_SIZE = 64\n",
        "trv_ds = SurvivalDataset(X_trv, y_trv_time, y_trv_event)\n",
        "te_ds  = SurvivalDataset(X_test, y_te_time, y_te_event)\n",
        "\n",
        "trv_sampler = EventBalancedBatchSampler(y_trv_event, BATCH_SIZE, seed=7)\n",
        "trv_loader  = DataLoader(trv_ds, batch_sampler=trv_sampler, num_workers=0)\n",
        "trv_eval_loader = DataLoader(trv_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "te_loader  = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# --- Model (arch parsed from string), Optim, Scheduler ---\n",
        "def layers_from_arch(arch_str: str):\n",
        "    return [int(x) for x in arch_str.split(\"-\") if x.strip()]\n",
        "\n",
        "in_dim = X_trv.shape[1]\n",
        "best_layers = layers_from_arch(best_hp[\"arch\"])\n",
        "\n",
        "model_final = DeepSurvMLP(in_dim, best_layers, dropout=best_hp[\"dropout\"]).to(device)\n",
        "opt_final = make_optimizer(model_final, lr=best_hp[\"lr\"], wd=best_hp[\"wd\"])\n",
        "sched_final = torch.optim.lr_scheduler.CosineAnnealingLR(opt_final, T_max=best_n_epochs)\n",
        "\n",
        "# --- Warmups (reuse constants from earlier cell if present; else define sane defaults) ---\n",
        "try:\n",
        "    WARMUP_EPOCHS_DROPOUT\n",
        "except NameError:\n",
        "    WARMUP_EPOCHS_DROPOUT = 30\n",
        "    WARMUP_EPOCHS_WD = 30\n",
        "    WARMUP_EPOCHS_L1 = 30\n",
        "    DROPOUT_START = 0.15\n",
        "    WD_START = 0.0\n",
        "\n",
        "# --- Train ---\n",
        "for epoch in range(best_n_epochs):\n",
        "    frac_d = min(1.0, epoch / float(WARMUP_EPOCHS_DROPOUT))\n",
        "    frac_w = min(1.0, epoch / float(WARMUP_EPOCHS_WD))\n",
        "    set_dropout_p(model_final, DROPOUT_START + (best_hp['dropout'] - DROPOUT_START) * frac_d)\n",
        "    set_weight_decay(opt_final, WD_START + (best_hp['wd'] - WD_START) * frac_w)\n",
        "\n",
        "    stats = train_one_epoch(model_final, opt_final, trv_loader, device, l1_lambda=l1_final,\n",
        "                            epoch=epoch, warmup_epochs=WARMUP_EPOCHS_L1)\n",
        "    _ = full_risk_set_step(model_final, opt_final, trv_ds, device, l1_lambda=l1_final, warm=stats['warm'])\n",
        "    sched_final.step()\n",
        "\n",
        "# --- Evaluate ---\n",
        "trainval_ci = evaluate_ci(model_final, trv_eval_loader, device)\n",
        "test_ci = evaluate_ci(model_final, te_loader, device)\n",
        "print(f\"\\n[Final] Train+Val CI: {trainval_ci:.4f}\")\n",
        "print(f\"[Final] Test CI:      {test_ci:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4PSFDyjMEea",
        "outputId": "5724b04b-5c1f-4a90-bb0e-7d50609a1142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Final] Train+Val CI: 0.7981\n",
            "[Final] Test CI:      0.6103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiobjective"
      ],
      "metadata": {
        "id": "G4S7y_fu6MS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab-ready SINGLE CELL\n",
        "# DeepSurv + Multi-Objective Optuna (Val CI ↑, Gap ↓, Params ↓)\n",
        "# with:\n",
        "#   • ONLY clinical vars (given list) + genes with Prop==1 from Genes.csv\n",
        "#   • Train/Val sorted by OS_MONTHS & OS_STATUS (desc)\n",
        "#   • Train-only standardization applied to Val; after HPO, restandardize on Train+Val\n",
        "#   • Treatment × Gene interactions (for Top-K_inter genes)\n",
        "#   • Stabilized IPTW (propensity weights) to address ACT imbalance\n",
        "#   • Event-balanced batches; dropout & WD warm-ups; gradient clipping; LR schedulers\n",
        "#   • Optuna Dashboard (link printed)\n",
        "#   • Final retraining on Train+Val with selected hyperparams + interactions + IPTW; Test CI reported\n",
        "# ============================================================\n",
        "\n",
        "# ---------- Installs (Colab) ----------\n",
        "!pip -q install optuna optuna-dashboard scikit-survival portpicker\n",
        "\n",
        "# ---------- (Optional) Mount Google Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, math, copy, warnings, random, gc, time, threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "# Prefer torchsurv if available; fallback to custom Efron\n",
        "try:\n",
        "    from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "    _HAS_TORCHSURV = True\n",
        "except Exception:\n",
        "    _HAS_TORCHSURV = False\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "from sksurv.util import Surv\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import NSGAIISampler\n",
        "\n",
        "import portpicker\n",
        "from google.colab import output\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# Cox Losses\n",
        "# ============================================================\n",
        "def _cox_negloglik_efron(pred, event, time):\n",
        "    eta = pred.reshape(-1)\n",
        "    e = event.to(torch.float32).reshape(-1)\n",
        "    t = time.reshape(-1)\n",
        "    order = torch.argsort(t, descending=True)\n",
        "    t = t[order]; e = e[order]; eta = eta[order]\n",
        "    exp_eta = torch.exp(eta)\n",
        "    cum_exp = torch.cumsum(exp_eta, dim=0)\n",
        "    uniq_mask = torch.ones_like(t, dtype=torch.bool)\n",
        "    uniq_mask[1:] = t[1:] != t[:-1]\n",
        "    idxs = torch.nonzero(uniq_mask, as_tuple=False).reshape(-1)\n",
        "    idxs = torch.cat([idxs, torch.tensor([len(t)], device=t.device)])\n",
        "    nll = torch.tensor(0.0, device=t.device)\n",
        "    for k in range(len(idxs)-1):\n",
        "        start, end = idxs[k].item(), idxs[k+1].item()\n",
        "        e_slice = e[start:end]\n",
        "        d = int(e_slice.sum().item())\n",
        "        if d == 0: continue\n",
        "        eta_events = eta[start:end][e_slice.bool()]\n",
        "        exp_events = torch.exp(eta[start:end])[e_slice.bool()]\n",
        "        s_eta = eta_events.sum()\n",
        "        risk_sum = cum_exp[end-1]\n",
        "        s_exp = exp_events.sum()\n",
        "        eps = 1e-12\n",
        "        log_terms = 0.0\n",
        "        for j in range(d):\n",
        "            log_terms = log_terms + torch.log(risk_sum - (j / d) * s_exp + eps)\n",
        "        nll = nll - (s_eta - log_terms)\n",
        "    return nll / t.numel()\n",
        "\n",
        "def cox_negloglik(pred, event, time):\n",
        "    if _HAS_TORCHSURV:\n",
        "        return neg_partial_log_likelihood(pred, event, time, reduction='mean')\n",
        "    return _cox_negloglik_efron(pred, event, time)\n",
        "\n",
        "# Weighted Breslow negative partial log-likelihood (IPTW)\n",
        "def cox_negloglik_breslow_weighted(pred, event, time, weight=None):\n",
        "    eta = pred.reshape(-1)\n",
        "    e = event.to(torch.float32).reshape(-1)\n",
        "    t = time.reshape(-1)\n",
        "    if weight is None:\n",
        "        weight = torch.ones_like(eta)\n",
        "    else:\n",
        "        weight = weight.reshape(-1)\n",
        "\n",
        "    order = torch.argsort(t, descending=True)\n",
        "    t = t[order]; e = e[order]; eta = eta[order]; w = weight[order]\n",
        "    exp_eta = torch.exp(eta)\n",
        "    w_exp = w * exp_eta\n",
        "    cum_w_exp = torch.cumsum(w_exp, dim=0)\n",
        "\n",
        "    uniq = torch.ones_like(t, dtype=torch.bool)\n",
        "    uniq[1:] = t[1:] != t[:-1]\n",
        "    idxs = torch.nonzero(uniq, as_tuple=False).reshape(-1)\n",
        "    idxs = torch.cat([idxs, torch.tensor([len(t)], device=t.device)])\n",
        "\n",
        "    nll = torch.tensor(0.0, device=t.device)\n",
        "    eps = 1e-12\n",
        "    for k in range(len(idxs)-1):\n",
        "        s, eidx = idxs[k].item(), idxs[k+1].item()\n",
        "        emask = e[s:eidx] > 0.5\n",
        "        if emask.sum() == 0:\n",
        "            continue\n",
        "        w_events = w[s:eidx][emask]\n",
        "        eta_events = eta[s:eidx][emask]\n",
        "        s_eta = (w_events * eta_events).sum()\n",
        "        denom = cum_w_exp[eidx-1]\n",
        "        nll = nll - (s_eta - w_events.sum() * torch.log(denom + eps))\n",
        "    return nll / (w.sum() + 1e-9)\n",
        "\n",
        "def cox_loss(pred, event, time, weight=None):\n",
        "    \"\"\"Use weighted Breslow if weights provided; else Efron/torchsurv.\"\"\"\n",
        "    if weight is None:\n",
        "        return cox_negloglik(pred, event, time)\n",
        "    return cox_negloglik_breslow_weighted(pred, event, time, weight)\n",
        "\n",
        "# ============================================================\n",
        "# Model, Dataset (with weights), Sampler, Utils\n",
        "# ============================================================\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(d, units), activation]\n",
        "            if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "            d = units\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events, weights=None):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events.astype(bool), dtype=torch.bool)\n",
        "        if weights is None:\n",
        "            self.w = torch.ones(len(self.x), dtype=torch.float32)\n",
        "        else:\n",
        "            self.w = torch.tensor(weights, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.time[idx], self.event[idx], self.w[idx]\n",
        "\n",
        "class EventBalancedBatchSampler(Sampler):\n",
        "    def __init__(self, events_numpy, batch_size, seed=0):\n",
        "        events = np.asarray(events_numpy).astype(bool)\n",
        "        self.pos_idx = np.where(events)[0]\n",
        "        self.neg_idx = np.where(~events)[0]\n",
        "        assert len(self.pos_idx) > 0, \"No events in training set — cannot balance batches.\"\n",
        "        self.bs = int(batch_size)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "    def __iter__(self):\n",
        "        pos = self.rng.permutation(self.pos_idx)\n",
        "        neg = self.rng.permutation(self.neg_idx)\n",
        "        n_total = len(pos) + len(neg)\n",
        "        n_batches = math.ceil(n_total / self.bs)\n",
        "        pi = ni = 0\n",
        "        for _ in range(n_batches):\n",
        "            take_pos = 1 if pi < len(pos) else 0\n",
        "            avail_neg = max(0, len(neg) - ni)\n",
        "            take_neg = min(self.bs - take_pos, avail_neg)\n",
        "            need = self.bs - (take_pos + take_neg)\n",
        "            extra_pos = min(need, max(0, len(pos) - (pi + take_pos)))\n",
        "            take_pos += extra_pos\n",
        "            batch = np.concatenate([pos[pi:pi+take_pos], neg[ni:ni+take_neg]])\n",
        "            pi += take_pos; ni += take_neg\n",
        "            if batch.size == 0: break\n",
        "            self.rng.shuffle(batch)\n",
        "            yield batch.tolist()\n",
        "    def __len__(self):\n",
        "        return math.ceil((len(self.pos_idx) + len(self.neg_idx)) / self.bs)\n",
        "\n",
        "def make_optimizer_groups(model, lr, wd, apply_final_wd=False):\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight) and not apply_final_wd:\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "    return optim.AdamW(\n",
        "        [{'params': decay, 'weight_decay': float(wd)},\n",
        "         {'params': no_decay, 'weight_decay': 0.0}],\n",
        "        lr=float(lr)\n",
        "    )\n",
        "\n",
        "def set_dropout_p(model, p):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout): m.p = float(p)\n",
        "\n",
        "def set_weight_decay(optimizer, wd):\n",
        "    for g in optimizer.param_groups: g['weight_decay'] = float(wd)\n",
        "\n",
        "def l1_penalty_first_layer(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear): return m.weight.abs().sum()\n",
        "    return torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_ci(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, times, events = [], [], []\n",
        "    for batch in dataloader:\n",
        "        if len(batch) == 4:\n",
        "            x, t, e, _ = batch\n",
        "        else:\n",
        "            x, t, e = batch\n",
        "        y = torch.clamp(model(x.to(device)), -20, 20)\n",
        "        preds.append(y.cpu().numpy().ravel())\n",
        "        times.append(t.numpy()); events.append(e.numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    times = np.concatenate(times); events = np.concatenate(events)\n",
        "    if np.isnan(preds).any(): return -np.inf\n",
        "    return concordance_index_censored(events.astype(bool), times, preds)[0]\n",
        "\n",
        "def evaluate_ci_grouped(model, X, t, e, group_mask):\n",
        "    \"\"\"Compute C-index within group_mask==1 and group_mask==0.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(torch.tensor(X, dtype=torch.float32, device=device)).cpu().numpy().ravel()\n",
        "    res = {}\n",
        "    for label, mask in [(\"ACT=1\", group_mask.astype(bool)), (\"ACT=0\", ~group_mask.astype(bool))]:\n",
        "        if mask.sum() >= 3:  # need a few samples\n",
        "            ci = concordance_index_censored(e[mask].astype(bool), t[mask], preds[mask])[0]\n",
        "            res[label] = float(ci)\n",
        "        else:\n",
        "            res[label] = np.nan\n",
        "    return res\n",
        "\n",
        "# ============================================================\n",
        "# Data loading & preprocessing\n",
        "# ============================================================\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/affyfRMATrain.csv\"\n",
        "VALID_CSV = \"/content/drive/MyDrive/affyfRMAValidation.csv\"\n",
        "TEST_CSV  = \"/content/drive/MyDrive/affyfRMATest.csv\"\n",
        "\n",
        "GENES_CSV = \"/mnt/data/Genes.csv\"\n",
        "if not os.path.exists(GENES_CSV):\n",
        "    if os.path.exists(\"/content/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/Genes.csv\"\n",
        "    elif os.path.exists(\"/content/drive/MyDrive/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/drive/MyDrive/Genes.csv\"\n",
        "print(\"Genes.csv path:\", GENES_CSV)\n",
        "\n",
        "CLINICAL_VARS = [\n",
        "    \"Adjuvant Chemo\",\"Age\",\"IS_MALE\",\n",
        "    \"Stage_IA\",\"Stage_IB\",\"Stage_II\",\"Stage_III\",\n",
        "    \"Histology_Adenocarcinoma\",\"Histology_Large Cell Carcinoma\",\"Histology_Squamous Cell Carcinoma\",\n",
        "    \"Race_African American\",\"Race_Asian\",\"Race_Caucasian\",\"Race_Native Hawaiian or Other Pacific Islander\",\"Race_Unknown\",\n",
        "    \"Smoked?_No\",\"Smoked?_Unknown\",\"Smoked?_Yes\"\n",
        "]\n",
        "\n",
        "def load_genes_list(genes_csv):\n",
        "    g = pd.read_csv(genes_csv)\n",
        "    if \"Prop\" not in g.columns or \"Gene\" not in g.columns:\n",
        "        raise ValueError(f\"Genes.csv must have columns 'Gene' and 'Prop'. Found: {list(g.columns)}\")\n",
        "    g[\"Prop\"] = pd.to_numeric(g[\"Prop\"], errors=\"coerce\").fillna(0)\n",
        "    genes = g.loc[g[\"Prop\"] == 1, \"Gene\"].astype(str).tolist()\n",
        "    print(f\"[Genes] Selected {len(genes)} genes with Prop == 1\")\n",
        "    return genes\n",
        "\n",
        "def coerce_survival_cols(df):\n",
        "    if df[\"OS_STATUS\"].dtype == object:\n",
        "        df[\"OS_STATUS\"] = df[\"OS_STATUS\"].replace({\"DECEASED\":1,\"LIVING\":0,\"Dead\":1,\"Alive\":0}).astype(int)\n",
        "    else:\n",
        "        df[\"OS_STATUS\"] = pd.to_numeric(df[\"OS_STATUS\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"OS_MONTHS\"] = pd.to_numeric(df[\"OS_MONTHS\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def preprocess_split(df, clinical_vars, gene_names):\n",
        "    if \"Adjuvant Chemo\" in df.columns:\n",
        "        df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
        "    for col in [\"Adjuvant Chemo\",\"IS_MALE\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df = coerce_survival_cols(df)\n",
        "    keep_cols = [c for c in clinical_vars if c in df.columns] + [g for g in gene_names if g in df.columns]\n",
        "    missing_clin = [c for c in clinical_vars if c not in df.columns]\n",
        "    if missing_clin:\n",
        "        print(f\"[WARN] Missing clinical columns: {missing_clin}\")\n",
        "    cols = [\"OS_STATUS\",\"OS_MONTHS\"] + keep_cols\n",
        "    return df[cols].copy()\n",
        "\n",
        "# Load CSVs\n",
        "train_raw = pd.read_csv(TRAIN_CSV)\n",
        "valid_raw = pd.read_csv(VALID_CSV)\n",
        "test_raw  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "GENE_LIST = load_genes_list(GENES_CSV)\n",
        "\n",
        "# Reduce to requested columns on each split\n",
        "train_df = preprocess_split(train_raw, CLINICAL_VARS, GENE_LIST)\n",
        "valid_df = preprocess_split(valid_raw, CLINICAL_VARS, GENE_LIST)\n",
        "test_df  = preprocess_split(test_raw,  CLINICAL_VARS, GENE_LIST)\n",
        "\n",
        "# Intersect features that exist everywhere\n",
        "feat_candidates = [c for c in (CLINICAL_VARS + GENE_LIST)\n",
        "                   if c in train_df.columns and c in valid_df.columns and c in test_df.columns]\n",
        "CLIN_FEATS = [c for c in CLINICAL_VARS if c in feat_candidates]\n",
        "GENE_FEATS = [g for g in GENE_LIST if g in feat_candidates]\n",
        "CLIN_FEATS_PRETX = [c for c in CLIN_FEATS if c != \"Adjuvant Chemo\"]  # PS excludes treatment itself\n",
        "print(f\"[Features] Using {len(feat_candidates)} common features → Clinical={len(CLIN_FEATS)}, Genes={len(GENE_FEATS)}\")\n",
        "\n",
        "# Sort Train/Val by event time & status (desc)\n",
        "train_df = train_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "valid_df = valid_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "# ---- Train-only gene ranking (univariate Cox CI on TRAIN) ----\n",
        "def rank_genes_univariate(train_df, gene_cols):\n",
        "    y = Surv.from_arrays(event=train_df[\"OS_STATUS\"].astype(bool).values,\n",
        "                         time=train_df[\"OS_MONTHS\"].values.astype(float))\n",
        "    ranks = []\n",
        "    for g in gene_cols:\n",
        "        Xg = train_df[[g]].to_numpy(dtype=np.float32)\n",
        "        try:\n",
        "            model = CoxPHSurvivalAnalysis(alpha=1e-12)\n",
        "            model.fit(Xg, y)\n",
        "            pred = model.predict(Xg)\n",
        "            ci = concordance_index_censored(y[\"event\"], y[\"time\"], pred)[0]\n",
        "            ranks.append((g, float(ci)))\n",
        "        except Exception:\n",
        "            ranks.append((g, 0.5))\n",
        "    ranks.sort(key=lambda z: z[1], reverse=True)\n",
        "    return [g for g, _ in ranks]\n",
        "\n",
        "GENE_RANK = rank_genes_univariate(train_df, GENE_FEATS)\n",
        "MAX_GENES = len(GENE_RANK)\n",
        "print(f\"[Gene Ranking] Ranked {MAX_GENES} genes on TRAIN\")\n",
        "\n",
        "# ============================================================\n",
        "# Feature construction (main effects + interactions) & IPTW\n",
        "# ============================================================\n",
        "def build_features_with_interactions(df, main_genes, inter_genes, act_col=\"Adjuvant Chemo\"):\n",
        "    base_cols = CLIN_FEATS + list(main_genes)  # keep ACT main effect via CLIN_FEATS\n",
        "    X_base = df[base_cols].to_numpy(dtype=np.float32)\n",
        "    A = df[act_col].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
        "    if len(inter_genes) > 0:\n",
        "        X_int = df[list(inter_genes)].to_numpy(dtype=np.float32) * A\n",
        "        X = np.concatenate([X_base, X_int], axis=1)\n",
        "        names = base_cols + [f\"{g}*ACT\" for g in inter_genes]\n",
        "    else:\n",
        "        X = X_base\n",
        "        names = base_cols\n",
        "    return X, names\n",
        "\n",
        "def compute_iptw(df, covariate_cols, act_col=\"Adjuvant Chemo\",\n",
        "                 ps_clip=(0.05, 0.95), w_clip=(0.1, 10.0),\n",
        "                 ref_prevalence=None, model=None):\n",
        "    A = df[act_col].astype(int).values\n",
        "    X = df[covariate_cols].astype(float).values\n",
        "    if model is None:\n",
        "        model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", class_weight=\"balanced\")\n",
        "        model.fit(X, A)\n",
        "    ps = model.predict_proba(X)[:, 1]\n",
        "    ps = np.clip(ps, ps_clip[0], ps_clip[1])\n",
        "    if ref_prevalence is None:\n",
        "        ref_prevalence = A.mean()\n",
        "    w = np.where(A == 1, ref_prevalence / ps, (1 - ref_prevalence) / (1 - ps))\n",
        "    w = np.clip(w, w_clip[0], w_clip[1])\n",
        "    return w.astype(np.float32), model, float(ref_prevalence)\n",
        "\n",
        "# ============================================================\n",
        "# Training helpers (with input dropout/noise, weights, warm-ups)\n",
        "# ============================================================\n",
        "def _apply_input_dropout(x, p):\n",
        "    if p <= 0.0: return x\n",
        "    keep = 1.0 - p\n",
        "    mask = torch.bernoulli(torch.full_like(x, keep))\n",
        "    return x * mask / max(keep, 1e-6)\n",
        "\n",
        "def train_one_epoch_reg(model, optimizer, dataloader, device,\n",
        "                        l1_lambda=0.0, epoch=0, warmup_epochs=20,\n",
        "                        input_dropout=0.0, noise_std=0.0, grad_clip=5.0):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))\n",
        "    loss_sum, w_sum = 0.0, 0.0\n",
        "    for x, t, e, w in dataloader:\n",
        "        if e.sum().item() == 0:\n",
        "            continue\n",
        "        x, t, e, w = x.to(device), t.to(device), e.to(device), w.to(device)\n",
        "        if input_dropout > 0.0: x = _apply_input_dropout(x, input_dropout)\n",
        "        if noise_std > 0.0: x = x + noise_std * torch.randn_like(x)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = cox_loss(out, e, t, weight=w)\n",
        "        if l1_lambda > 0:\n",
        "            loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * float(w.sum().item())\n",
        "        w_sum += float(w.sum().item())\n",
        "    return {'avg_loss': (loss_sum / max(w_sum, 1e-9)), 'warm': warm}\n",
        "\n",
        "def full_risk_set_step_reg(model, optimizer, ds, device,\n",
        "                           l1_lambda=0.0, warm=1.0,\n",
        "                           input_dropout=0.0, noise_std=0.0, grad_clip=5.0):\n",
        "    model.train()\n",
        "    X_all = ds.x.to(device); t_all = ds.time.to(device); e_all = ds.event.to(device); w_all = ds.w.to(device)\n",
        "    XX = X_all\n",
        "    if input_dropout > 0.0: XX = _apply_input_dropout(XX, input_dropout)\n",
        "    if noise_std > 0.0: XX = XX + noise_std * torch.randn_like(XX)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(XX), -20, 20)\n",
        "    loss_full = cox_loss(out_all, e_all, t_all, weight=w_all)\n",
        "    if l1_lambda > 0:\n",
        "        loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "def count_params(in_dim, layers):\n",
        "    params, d = 0, in_dim\n",
        "    for h in layers:\n",
        "        params += d*h + h\n",
        "        d = h\n",
        "    params += d*1 + 1\n",
        "    return int(params)\n",
        "\n",
        "# ============================================================\n",
        "# Optuna: Multi-Objective (Val CI ↑, Train–Val GAP ↓, Params ↓)\n",
        "# + Feature budget (Top-K genes & Top-K interactions)\n",
        "# ============================================================\n",
        "ARCH_CHOICES = (\n",
        "    \"16\",\"32\",\"64\",\"128\",\"256\",\"512\",\n",
        "    \"32-16\",\"64-32\",\"64-64\",\"128-64\",\"128-128\",\"256-128\",\"256-256\",\"512-256\",\n",
        "    \"64-32-16\",\"128-64-32\",\"256-128-64\"\n",
        ")\n",
        "def layers_from_arch(arch_str: str):\n",
        "    return [int(x) for x in arch_str.split(\"-\") if x.strip()]\n",
        "\n",
        "# ---------- STATIC SPACES (no dynamic value space); clamp later ----------\n",
        "def suggest_hparams(trial):\n",
        "    # Fixed main-gene choices (dedup + bounded by MAX_GENES)\n",
        "    base_main = [32, 64, 128, 256, 512, 800, MAX_GENES]\n",
        "    TOPK_MAIN_CHOICES = tuple(sorted({k for k in base_main if k <= MAX_GENES}))\n",
        "    top_k_genes = trial.suggest_categorical(\"top_k_genes\", TOPK_MAIN_CHOICES)\n",
        "\n",
        "    # Fixed interaction choices; NOT conditioned on top_k_genes\n",
        "    base_inter = [0, 16, 32, 64, 128, 256, 512]\n",
        "    TOPK_INTER_CHOICES = tuple(sorted({k for k in base_inter if k <= MAX_GENES}))\n",
        "    top_k_inter_raw = trial.suggest_categorical(\"top_k_inter\", TOPK_INTER_CHOICES)\n",
        "\n",
        "    # net & regularizers\n",
        "    arch = trial.suggest_categorical(\"arch\", ARCH_CHOICES)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.10, 0.70)\n",
        "    input_dropout = trial.suggest_float(\"input_dropout\", 0.00, 0.30)\n",
        "    noise_std = trial.suggest_float(\"noise_std\", 0.0, 0.10)\n",
        "\n",
        "    wd = trial.suggest_float(\"wd\", 1e-6, 1e-1, log=True)\n",
        "    apply_final_wd = trial.suggest_categorical(\"apply_final_wd\", (0, 1))\n",
        "    use_l1 = trial.suggest_categorical(\"use_l1\", (0, 1))\n",
        "    l1 = 0.0 if use_l1 == 0 else trial.suggest_float(\"l1\", 1e-8, 3e-3, log=True)\n",
        "\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
        "    sched = trial.suggest_categorical(\"sched\", (\"cosine\", \"cawr\", \"none\"))\n",
        "    if sched == \"cawr\":\n",
        "        cawr_T0 = trial.suggest_int(\"cawr_T0\", 16, 80, step=8)\n",
        "        cawr_Tmult = trial.suggest_categorical(\"cawr_Tmult\", (1, 2, 3))\n",
        "    else:\n",
        "        cawr_T0 = cawr_Tmult = None\n",
        "\n",
        "    epochs = trial.suggest_int(\"epochs\", 64, 384, step=32)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", (32, 64, 128))\n",
        "    grad_clip = trial.suggest_float(\"grad_clip\", 1.0, 10.0)\n",
        "\n",
        "    # Clamp interactions to <= main set (no dynamic space; enforced post-hoc)\n",
        "    top_k_inter = int(min(top_k_inter_raw, top_k_genes))\n",
        "\n",
        "    return dict(\n",
        "        arch=arch, top_k_genes=int(top_k_genes), top_k_inter=int(top_k_inter),\n",
        "        dropout=dropout, input_dropout=input_dropout, noise_std=noise_std,\n",
        "        wd=wd, apply_final_wd=int(apply_final_wd), use_l1=int(use_l1), l1=float(l1),\n",
        "        lr=lr, sched=sched, cawr_T0=cawr_T0, cawr_Tmult=cawr_Tmult,\n",
        "        epochs=int(epochs), batch_size=int(batch_size), grad_clip=float(grad_clip)\n",
        "    )\n",
        "\n",
        "# Warm-ups and caps\n",
        "MAX_EPOCHS_CAP = 384\n",
        "WARMUP_EPOCHS_L1 = 30\n",
        "WARMUP_EPOCHS_DROPOUT = 30\n",
        "WARMUP_EPOCHS_WD = 30\n",
        "DROPOUT_START = 0.15\n",
        "WD_START = 0.0\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_hparams(trial)\n",
        "    layers = layers_from_arch(hp[\"arch\"])\n",
        "\n",
        "    # --- Trial-specific features (main + interactions)\n",
        "    genes_main = GENE_RANK[:hp[\"top_k_genes\"]]\n",
        "    # interactions must be a subset of the selected mains\n",
        "    genes_inter = genes_main[:hp[\"top_k_inter\"]]\n",
        "\n",
        "    Xtr_raw, feat_names = build_features_with_interactions(train_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "    Xva_raw, _          = build_features_with_interactions(valid_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "\n",
        "    # --- TRAIN-only impute & standardize\n",
        "    med = np.nanmedian(Xtr_raw, axis=0)\n",
        "    Xtr = np.where(np.isnan(Xtr_raw), med, Xtr_raw)\n",
        "    Xva = np.where(np.isnan(Xva_raw), med, Xva_raw)\n",
        "    sc = StandardScaler().fit(Xtr)\n",
        "    Xtr = sc.transform(Xtr).astype(np.float32)\n",
        "    Xva = sc.transform(Xva).astype(np.float32)\n",
        "\n",
        "    ytr_t = train_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "    ytr_e = train_df[\"OS_STATUS\"].to_numpy(int)\n",
        "    yva_t = valid_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "    yva_e = valid_df[\"OS_STATUS\"].to_numpy(int)\n",
        "\n",
        "    # --- IPTW on clinical pre-treatment covariates ONLY (exclude ACT)\n",
        "    w_tr, ps_model, pi_tr = compute_iptw(train_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\")\n",
        "    w_va, _, _ = compute_iptw(valid_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\",\n",
        "                              ref_prevalence=pi_tr, model=ps_model)\n",
        "\n",
        "    # --- Datasets/loaders\n",
        "    bs = hp[\"batch_size\"]\n",
        "    tr_ds = SurvivalDataset(Xtr, ytr_t, ytr_e, weights=w_tr)\n",
        "    va_ds = SurvivalDataset(Xva, yva_t, yva_e, weights=w_va)\n",
        "    tr_sampler = EventBalancedBatchSampler(ytr_e, bs, seed=42)\n",
        "    tr_loader = DataLoader(tr_ds, batch_sampler=tr_sampler, num_workers=0)\n",
        "    tr_eval_loader = DataLoader(tr_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "    va_loader = DataLoader(va_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "\n",
        "    # --- Model/opt/sched\n",
        "    in_dim_trial = Xtr.shape[1]\n",
        "    model = DeepSurvMLP(in_dim_trial, layers, dropout=hp[\"dropout\"]).to(device)\n",
        "    opt = make_optimizer_groups(model, lr=hp[\"lr\"], wd=hp[\"wd\"], apply_final_wd=bool(hp[\"apply_final_wd\"]))\n",
        "\n",
        "    epochs = int(min(hp[\"epochs\"], MAX_EPOCHS_CAP))\n",
        "    if hp[\"sched\"] == \"cosine\":\n",
        "        sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "        def sched_step(i): sched.step()\n",
        "    elif hp[\"sched\"] == \"cawr\":\n",
        "        sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            opt, T_0=int(hp[\"cawr_T0\"]), T_mult=int(hp[\"cawr_Tmult\"]))\n",
        "        def sched_step(i): sched.step(i+1)\n",
        "    else:\n",
        "        sched = None\n",
        "        def sched_step(i): pass\n",
        "\n",
        "    # --- Manual early-stopping on Val CI (MO: no trial.report/prune)\n",
        "    PATIENCE = 40\n",
        "    MIN_DELTA = 1e-4\n",
        "    no_improve = 0\n",
        "    best_val_ci = -np.inf\n",
        "    best_tr_ci_at_best = float(\"nan\")\n",
        "    best_epoch = 0\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        # warm-ups\n",
        "        frac_d = min(1.0, ep / float(WARMUP_EPOCHS_DROPOUT))\n",
        "        frac_w = min(1.0, ep / float(WARMUP_EPOCHS_WD))\n",
        "        set_dropout_p(model, DROPOUT_START + (hp['dropout'] - DROPOUT_START) * frac_d)\n",
        "        set_weight_decay(opt, WD_START + (hp['wd'] - WD_START) * frac_w)\n",
        "\n",
        "        # one epoch + full-risk-set correction (with regularizers + weights)\n",
        "        st = train_one_epoch_reg(\n",
        "            model, opt, tr_loader, device,\n",
        "            l1_lambda=float(hp.get(\"l1\", 0.0)), epoch=ep, warmup_epochs=WARMUP_EPOCHS_L1,\n",
        "            input_dropout=float(hp.get(\"input_dropout\", 0.0)),\n",
        "            noise_std=float(hp.get(\"noise_std\", 0.0)),\n",
        "            grad_clip=float(hp.get(\"grad_clip\", 5.0))\n",
        "        )\n",
        "        _ = full_risk_set_step_reg(\n",
        "            model, opt, tr_ds, device,\n",
        "            l1_lambda=float(hp.get(\"l1\", 0.0)), warm=st['warm'],\n",
        "            input_dropout=float(hp.get(\"input_dropout\", 0.0)),\n",
        "            noise_std=float(hp.get(\"noise_std\", 0.0)),\n",
        "            grad_clip=float(hp.get(\"grad_clip\", 5.0))\n",
        "        )\n",
        "        sched_step(ep)\n",
        "\n",
        "        # eval both splits so we can compute the gap at the best Val epoch\n",
        "        va_ci = evaluate_ci(model, va_loader, device)\n",
        "        tr_ci = evaluate_ci(model, tr_eval_loader, device)\n",
        "\n",
        "        if va_ci > best_val_ci + MIN_DELTA:\n",
        "            best_val_ci = va_ci\n",
        "            best_tr_ci_at_best = tr_ci\n",
        "            best_epoch = ep + 1\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE:\n",
        "                break\n",
        "\n",
        "    gap = max(0.0, best_tr_ci_at_best - best_val_ci)\n",
        "    param_cnt = count_params(in_dim_trial, layers)\n",
        "\n",
        "    # annotate & cleanup\n",
        "    trial.set_user_attr(\"best_epoch\", int(best_epoch))\n",
        "    trial.set_user_attr(\"n_features\", int(in_dim_trial))\n",
        "    del model, opt, sched\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Multi-objective return: (Val CI ↑, Gap ↓, Params ↓)\n",
        "    return float(best_val_ci), float(gap), int(param_cnt)\n",
        "\n",
        "# ---- Optuna Study: NSGA-II (no pruner for multi-objective) ----\n",
        "storage = \"sqlite:///deepsurv_optuna.db\"\n",
        "study_name = f\"deepsurv_cox_mo_gap_size_interactions_static_M{MAX_GENES}\"  # new name to avoid old dynamic-space schema\n",
        "\n",
        "sampler = NSGAIISampler(seed=42, population_size=24)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    directions=[\"maximize\", \"minimize\", \"minimize\"],\n",
        "    study_name=study_name,\n",
        "    storage=storage,\n",
        "    load_if_exists=True,\n",
        "    sampler=sampler\n",
        ")\n",
        "\n",
        "# ---- Launch Optuna Dashboard (proxied URL printed) ----\n",
        "try:\n",
        "    from optuna_dashboard import run_server\n",
        "    PORT = portpicker.pick_unused_port()\n",
        "    def _start_dashboard():\n",
        "        run_server(storage, host=\"0.0.0.0\", port=PORT)  # no unsupported kwargs\n",
        "    t = threading.Thread(target=_start_dashboard, daemon=True)\n",
        "    t.start(); time.sleep(2)\n",
        "    dash_url = output.eval_js(f\"google.colab.kernel.proxyPort({PORT}, {{'cache': false}})\")\n",
        "    print(\"Optuna Dashboard:\", dash_url)\n",
        "except Exception as ex:\n",
        "    print(\"[Optuna Dashboard] Could not start dashboard automatically.\", ex)\n",
        "    print(\"Run locally:  optuna-dashboard sqlite:///deepsurv_optuna.db\")\n",
        "\n",
        "# ---- Optimize ----\n",
        "N_TRIALS = 100\n",
        "print(f\"Starting multi-objective optimization: {N_TRIALS} trials\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)\n",
        "\n",
        "# ---- Choose a robust solution from Pareto front ----\n",
        "pareto = study.best_trials\n",
        "best_val = max(tr.values[0] for tr in pareto)\n",
        "TOL = 0.005  # within 0.5% absolute C-index of best\n",
        "cands = [tr for tr in pareto if (best_val - tr.values[0]) <= TOL]\n",
        "cands.sort(key=lambda tr: (tr.values[1], tr.values[2]))  # gap, then params\n",
        "chosen = cands[0]\n",
        "print(\"\\n[Chosen Pareto] Val CI=%.4f | Gap=%.4f | Params=%d\" % (chosen.values[0], chosen.values[1], chosen.values[2]))\n",
        "print(\"[Chosen Params]\", chosen.params)\n",
        "\n",
        "# ============================================================\n",
        "# Final training on Train+Val with chosen hyperparams + interactions + IPTW\n",
        "# ============================================================\n",
        "best_hp = chosen.params\n",
        "best_layers = layers_from_arch(best_hp[\"arch\"])\n",
        "k_main = int(best_hp[\"top_k_genes\"])\n",
        "k_int  = int(best_hp[\"top_k_inter\"])\n",
        "genes_main = GENE_RANK[:k_main]\n",
        "# interactions must be a subset of the selected main genes:\n",
        "genes_inter = genes_main[:k_int]\n",
        "\n",
        "print(f\"[Final] Using features: {len(CLIN_FEATS)} clinical + {k_main} genes (main) + {k_int} interactions\")\n",
        "\n",
        "# Build Train+Val & Test features (same construction)\n",
        "trainval_df = pd.concat([train_df, valid_df], axis=0, ignore_index=True)\n",
        "trainval_df = trainval_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "X_trv_raw, feat_names = build_features_with_interactions(trainval_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "X_te_raw,  _          = build_features_with_interactions(test_df,      genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "\n",
        "# Impute + standardize on Train+Val; apply to Test\n",
        "med_trv = np.nanmedian(X_trv_raw, axis=0)\n",
        "X_trv = np.where(np.isnan(X_trv_raw), med_trv, X_trv_raw)\n",
        "X_te  = np.where(np.isnan(X_te_raw),  med_trv, X_te_raw)\n",
        "sc_trv = StandardScaler().fit(X_trv)\n",
        "X_trv = sc_trv.transform(X_trv).astype(np.float32)\n",
        "X_te  = sc_trv.transform(X_te).astype(np.float32)\n",
        "\n",
        "y_trv_t = trainval_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "y_trv_e = trainval_df[\"OS_STATUS\"].to_numpy(int)\n",
        "y_te_t  = test_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "y_te_e  = test_df[\"OS_STATUS\"].to_numpy(int)\n",
        "\n",
        "# IPTW on Train+Val, apply to Test with same prevalence\n",
        "w_trv, ps_model_fin, pi_fin = compute_iptw(trainval_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\")\n",
        "w_te, _, _ = compute_iptw(test_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\",\n",
        "                          ref_prevalence=pi_fin, model=ps_model_fin)\n",
        "\n",
        "# Loaders\n",
        "bs_fin = int(best_hp[\"batch_size\"])\n",
        "ds_trv = SurvivalDataset(X_trv, y_trv_t, y_trv_e, weights=w_trv)\n",
        "ds_te  = SurvivalDataset(X_te,  y_te_t,  y_te_e,  weights=w_te)\n",
        "sam_trv = EventBalancedBatchSampler(y_trv_e, bs_fin, seed=7)\n",
        "dl_trv  = DataLoader(ds_trv, batch_sampler=sam_trv, num_workers=0)\n",
        "dl_trv_eval = DataLoader(ds_trv, batch_size=bs_fin, shuffle=False, num_workers=0)\n",
        "dl_te   = DataLoader(ds_te,  batch_size=bs_fin, shuffle=False, num_workers=0)\n",
        "\n",
        "# Model / optimizer / scheduler\n",
        "in_dim_final = X_trv.shape[1]\n",
        "model_final = DeepSurvMLP(in_dim_final, best_layers, dropout=float(best_hp[\"dropout\"])).to(device)\n",
        "opt_final = make_optimizer_groups(model_final, lr=float(best_hp[\"lr\"]), wd=float(best_hp[\"wd\"]),\n",
        "                                  apply_final_wd=bool(best_hp[\"apply_final_wd\"]))\n",
        "epochs_fin = int(min(best_hp[\"epochs\"], MAX_EPOCHS_CAP))\n",
        "if best_hp[\"sched\"] == \"cosine\":\n",
        "    sched_final = torch.optim.lr_scheduler.CosineAnnealingLR(opt_final, T_max=epochs_fin)\n",
        "    def sched_step(i): sched_final.step()\n",
        "elif best_hp[\"sched\"] == \"cawr\":\n",
        "    sched_final = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        opt_final, T_0=int(best_hp[\"cawr_T0\"]), T_mult=int(best_hp[\"cawr_Tmult\"]))\n",
        "    def sched_step(i): sched_final.step(i+1)\n",
        "else:\n",
        "    sched_final = None\n",
        "    def sched_step(i): pass\n",
        "\n",
        "# Train\n",
        "for ep in range(epochs_fin):\n",
        "    frac_d = min(1.0, ep / float(WARMUP_EPOCHS_DROPOUT))\n",
        "    frac_w = min(1.0, ep / float(WARMUP_EPOCHS_WD))\n",
        "    set_dropout_p(model_final, DROPOUT_START + (float(best_hp['dropout']) - DROPOUT_START) * frac_d)\n",
        "    set_weight_decay(opt_final, WD_START + (float(best_hp['wd']) - WD_START) * frac_w)\n",
        "\n",
        "    st = train_one_epoch_reg(\n",
        "        model_final, opt_final, dl_trv, device,\n",
        "        l1_lambda=float(best_hp.get(\"l1\", 0.0)), epoch=ep, warmup_epochs=WARMUP_EPOCHS_L1,\n",
        "        input_dropout=float(best_hp.get(\"input_dropout\", 0.0)),\n",
        "        noise_std=float(best_hp.get(\"noise_std\", 0.0)),\n",
        "        grad_clip=float(best_hp.get(\"grad_clip\", 5.0))\n",
        "    )\n",
        "    _ = full_risk_set_step_reg(\n",
        "        model_final, opt_final, ds_trv, device,\n",
        "        l1_lambda=float(best_hp.get(\"l1\", 0.0)), warm=st['warm'],\n",
        "        input_dropout=float(best_hp.get(\"input_dropout\", 0.0)),\n",
        "        noise_std=float(best_hp.get(\"noise_std\", 0.0)),\n",
        "        grad_clip=float(best_hp.get(\"grad_clip\", 5.0))\n",
        "    )\n",
        "    sched_step(ep)\n",
        "\n",
        "# Evaluate\n",
        "trainval_ci = evaluate_ci(model_final, dl_trv_eval, device)\n",
        "test_ci = evaluate_ci(model_final, dl_te, device)\n",
        "print(f\"\\n[Final] Train+Val CI: {trainval_ci:.4f}\")\n",
        "print(f\"[Final] Test CI:      {test_ci:.4f}\")\n",
        "\n",
        "# Per-arm C-indices (helpful for treatment recommendation sanity checks)\n",
        "act_trv = trainval_df[\"Adjuvant Chemo\"].to_numpy(int)\n",
        "act_te  = test_df[\"Adjuvant Chemo\"].to_numpy(int)\n",
        "trv_grouped = evaluate_ci_grouped(model_final, X_trv, y_trv_t, y_trv_e, act_trv == 1)\n",
        "te_grouped  = evaluate_ci_grouped(model_final, X_te,  y_te_t,  y_te_e,  act_te == 1)\n",
        "print(\"[Train+Val] CI by arm:\", trv_grouped)\n",
        "print(\"[Test]      CI by arm:\", te_grouped)\n",
        "\n",
        "# Save artifacts\n",
        "OUT_DIR = \"/content/drive/MyDrive/deepsurv_results_optuna_interactions_iptw\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "torch.save(model_final.state_dict(), os.path.join(OUT_DIR, \"deepsurv_best.pt\"))\n",
        "with open(os.path.join(OUT_DIR, \"chosen_params.txt\"), \"w\") as f:\n",
        "    f.write(str(best_hp))\n",
        "with open(os.path.join(OUT_DIR, \"features_used.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(feat_names))\n",
        "print(\"Saved final model and parameters to:\", OUT_DIR)\n"
      ],
      "metadata": {
        "id": "-X3mDpg-Rb6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd00f8d-507e-4097-9929-172012a39dd4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "Genes.csv path: /content/drive/MyDrive/Genes.csv\n",
            "[Genes] Selected 1555 genes with Prop == 1\n",
            "[Features] Using 1573 common features → Clinical=18, Genes=1555\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3193099072.py:296: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-3193099072.py:296: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-3193099072.py:296: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "[I 2025-10-19 19:57:30,215] A new study created in RDB with name: deepsurv_cox_mo_gap_size_interactions_static_M1555\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Gene Ranking] Ranked 1555 genes on TRAIN\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Bottle v0.13.4 server starting up (using WSGIRefServer())...\n",
            "Listening on http://0.0.0.0:36243/\n",
            "Hit Ctrl-C to quit.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna Dashboard: https://36243-gpu-a100-s-2cm35yjghnl2l-b.asia-southeast1-0.prod.colab.dev\n",
            "Starting multi-objective optimization: 100 trials\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-19 19:59:10,153] Trial 0 finished with values: [0.6152409964133052, 0.08393829126825181, 70657.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '256-128', 'dropout': 0.20231447421237492, 'input_dropout': 0.019515477895583853, 'noise_std': 0.09488855372533334, 'wd': 0.0673224892077534, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 2.5749486860432668e-06, 'lr': 1.6119044727609182e-05, 'sched': 'none', 'epochs': 128, 'batch_size': 32, 'grad_clip': 5.920392514089517}.\n",
            "[I 2025-10-19 20:00:11,063] Trial 1 finished with values: [0.6322409472804992, 0.06179018547983173, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'none', 'epochs': 320, 'batch_size': 64, 'grad_clip': 5.704595464437946}.\n",
            "[I 2025-10-19 20:01:49,567] Trial 2 finished with values: [0.628998182086179, 0.11489080839887811, 17025.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 0, 'arch': '32', 'dropout': 0.10417127831871442, 'input_dropout': 0.1532241907732697, 'noise_std': 0.041741100314877905, 'wd': 1.2898495377182665e-05, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 7.610438922351127e-05, 'sched': 'none', 'epochs': 384, 'batch_size': 64, 'grad_clip': 3.5635644493972087}.\n",
            "[I 2025-10-19 20:03:28,953] Trial 3 finished with values: [0.600648553038864, 0.07246854670455949, 27777.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 32, 'arch': '32-16', 'dropout': 0.5145626428614796, 'input_dropout': 0.11602060389016122, 'noise_std': 0.09367299887367346, 'wd': 4.870849610200306e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.7430586177787164e-05, 'sched': 'cawr', 'cawr_T0': 48, 'cawr_Tmult': 3, 'epochs': 352, 'batch_size': 32, 'grad_clip': 7.533601109832154}.\n",
            "[I 2025-10-19 20:07:46,533] Trial 4 finished with values: [0.5986832407998821, 0.07542580698541379, 220033.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 64, 'arch': '128-64-32', 'dropout': 0.6352279331062679, 'input_dropout': 0.18934158779917887, 'noise_std': 0.07948113035416485, 'wd': 0.00032597587933173606, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 3.450041141715295e-07, 'lr': 1.0997959032388697e-05, 'sched': 'none', 'epochs': 384, 'batch_size': 32, 'grad_clip': 9.354867063289529}.\n",
            "[I 2025-10-19 20:08:19,163] Trial 5 finished with values: [0.6156831916670761, 0.0, 4737.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 64, 'arch': '32', 'dropout': 0.48997835846665905, 'input_dropout': 0.210590063177311, 'noise_std': 0.07957926694361012, 'wd': 0.02818556266996745, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 1.5734762488527392e-08, 'lr': 6.18069149877641e-05, 'sched': 'none', 'epochs': 64, 'batch_size': 64, 'grad_clip': 2.143544613866963}.\n",
            "[I 2025-10-19 20:09:05,346] Trial 6 finished with values: [0.5997641625313221, 0.03576974380439968, 37889.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 64, 'arch': '256', 'dropout': 0.43261243150684037, 'input_dropout': 0.18351622387030567, 'noise_std': 0.04196000624277899, 'wd': 1.7324270154949117e-05, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 1.7863314662226938e-08, 'lr': 1.1727272411645968e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 32, 'grad_clip': 4.904664843141758}.\n",
            "[I 2025-10-19 20:10:08,428] Trial 7 finished with values: [0.6497322262074388, 0.07753193676020376, 13633.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 0, 'arch': '64-64', 'dropout': 0.29138537817625676, 'input_dropout': 0.25346259329083637, 'noise_std': 0.0023271935735825868, 'wd': 0.011812516086413106, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00030959834513535173, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 3, 'epochs': 192, 'batch_size': 64, 'grad_clip': 9.376815930432082}.\n",
            "[I 2025-10-19 20:11:13,752] Trial 8 finished with values: [0.6602957794919668, 0.06534469198814152, 68865.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.6326225096549798, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011706701642760588, 'wd': 5.187503589897551e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'cawr', 'cawr_T0': 16, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 128, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 20:12:54,218] Trial 9 finished with values: [0.6418218444455362, 0.09436430928978545, 813057.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '512-256', 'dropout': 0.41385870650208934, 'input_dropout': 0.18881959144057875, 'noise_std': 0.06957486889846172, 'wd': 0.00018737227240512905, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 3.0015669271531814e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 20:14:28,045] Trial 10 finished with values: [0.6558738269542573, 0.09285852994753985, 38209.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 256, 'arch': '64-64', 'dropout': 0.26404424315842373, 'input_dropout': 0.016912648995278133, 'noise_std': 0.08647223762550532, 'wd': 0.01160125694455748, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.001494846021349967, 'lr': 0.0002776677435509801, 'sched': 'cawr', 'cawr_T0': 80, 'cawr_Tmult': 3, 'epochs': 256, 'batch_size': 128, 'grad_clip': 5.68276930813414}.\n",
            "[I 2025-10-19 20:15:41,846] Trial 11 finished with values: [0.6022208028300496, 0.08023390836898836, 283905.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 32, 'arch': '256-256', 'dropout': 0.3754814537429568, 'input_dropout': 0.2940097725856431, 'noise_std': 0.04926180939928696, 'wd': 4.4030949559517743e-05, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 5.027158591793137e-08, 'lr': 1.811665941459424e-05, 'sched': 'cawr', 'cawr_T0': 40, 'cawr_Tmult': 1, 'epochs': 96, 'batch_size': 32, 'grad_clip': 3.5073130512876274}.\n",
            "[I 2025-10-19 20:17:21,478] Trial 12 finished with values: [0.5959809364712819, 0.09047481655028045, 277377.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 512, 'arch': '128-64-32', 'dropout': 0.4192680911591894, 'input_dropout': 0.032151603401932814, 'noise_std': 0.04474123668234547, 'wd': 0.00046034807588787966, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 3.525354876320808e-05, 'sched': 'cawr', 'cawr_T0': 80, 'cawr_Tmult': 3, 'epochs': 224, 'batch_size': 128, 'grad_clip': 7.70895526765897}.\n",
            "[I 2025-10-19 20:18:29,049] Trial 13 finished with values: [0.5708249398123126, 0.08744618895805967, 36097.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 0, 'arch': '64-32', 'dropout': 0.4340577358083578, 'input_dropout': 0.17148380684096995, 'noise_std': 0.027997909366028417, 'wd': 0.007038249626652292, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 2.1267062028723338e-07, 'lr': 1.567127460068538e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 4.029438503745285}.\n",
            "[I 2025-10-19 20:19:53,788] Trial 14 finished with values: [0.6341571267135067, 0.0710902230119066, 4417.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 256, 'arch': '16', 'dropout': 0.3152982904741015, 'input_dropout': 0.26316016224393246, 'noise_std': 0.03924451074226354, 'wd': 0.012105903211961774, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00018627986386722937, 'sched': 'none', 'epochs': 192, 'batch_size': 64, 'grad_clip': 2.0520823663752767}.\n",
            "[I 2025-10-19 20:21:35,920] Trial 15 finished with values: [0.6474229843266349, 0.053917530932185986, 3329.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 0, 'arch': '64', 'dropout': 0.25107943534417165, 'input_dropout': 0.10370879805117583, 'noise_std': 0.01815977168014257, 'wd': 0.03485401443038997, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 6.91710278982827e-08, 'lr': 9.908020585782568e-05, 'sched': 'cawr', 'cawr_T0': 72, 'cawr_Tmult': 1, 'epochs': 384, 'batch_size': 32, 'grad_clip': 6.460435417445703}.\n",
            "[I 2025-10-19 20:22:38,220] Trial 16 finished with values: [0.6282611899965607, 0.15765536445252026, 104897.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 32, 'arch': '64-32', 'dropout': 0.3513552697027288, 'input_dropout': 0.27445377032043, 'noise_std': 0.03623938991166331, 'wd': 0.0007997269865263961, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00042936796766455975, 'sched': 'cawr', 'cawr_T0': 80, 'cawr_Tmult': 2, 'epochs': 320, 'batch_size': 64, 'grad_clip': 2.716189485972999}.\n",
            "[I 2025-10-19 20:24:05,404] Trial 17 finished with values: [0.5922959760231907, 0.05688137209515587, 69377.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 512, 'arch': '64-32-16', 'dropout': 0.24290966862803592, 'input_dropout': 0.029832417827803548, 'noise_std': 0.02431721909994541, 'wd': 0.004086341509440911, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 1.3265506639155056e-07, 'lr': 3.1481049378577344e-05, 'sched': 'cosine', 'epochs': 128, 'batch_size': 128, 'grad_clip': 4.792820955332481}.\n",
            "[I 2025-10-19 20:28:05,732] Trial 18 finished with values: [0.6346975875792267, 0.07104850697170906, 174081.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 256, 'arch': '512-256', 'dropout': 0.11103440592848005, 'input_dropout': 0.2742896419681497, 'noise_std': 0.011775108289014114, 'wd': 0.0007631016235941566, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 1.3508102113295584e-07, 'lr': 1.0439545297949767e-05, 'sched': 'cawr', 'cawr_T0': 56, 'cawr_Tmult': 2, 'epochs': 384, 'batch_size': 128, 'grad_clip': 1.4226937009754954}.\n",
            "[I 2025-10-19 20:28:40,324] Trial 19 finished with values: [0.6257554168918587, 0.06548274484554661, 27137.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '128-64', 'dropout': 0.5914613906045199, 'input_dropout': 0.23982476369069378, 'noise_std': 0.06946964708544268, 'wd': 2.2946987874230535e-05, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 5.615290533484884e-08, 'lr': 0.0004115520422559035, 'sched': 'none', 'epochs': 352, 'batch_size': 64, 'grad_clip': 7.230489080660672}.\n",
            "[I 2025-10-19 20:30:29,456] Trial 20 finished with values: [0.597602319068442, 0.09596331852140139, 35073.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 16, 'arch': '64', 'dropout': 0.13921380447936543, 'input_dropout': 0.09458710134918309, 'noise_std': 0.05394912923753373, 'wd': 0.008987022400646503, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 2.4876607835597055e-05, 'sched': 'cawr', 'cawr_T0': 72, 'cawr_Tmult': 2, 'epochs': 384, 'batch_size': 64, 'grad_clip': 5.318880582735179}.\n",
            "[I 2025-10-19 20:32:08,322] Trial 21 finished with values: [0.6373998919078269, 0.07701327938885427, 33281.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 32, 'arch': '128-64-32', 'dropout': 0.6656390644810753, 'input_dropout': 0.19489399469771054, 'noise_std': 0.060773679487885916, 'wd': 0.0003659675301686172, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00021110116810555505, 'sched': 'none', 'epochs': 352, 'batch_size': 64, 'grad_clip': 4.566178170123929}.\n",
            "[I 2025-10-19 20:33:55,482] Trial 22 finished with values: [0.61907335527932, 0.10580237413498739, 8513.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 512, 'arch': '16', 'dropout': 0.3883519463959888, 'input_dropout': 0.17772233539785529, 'noise_std': 0.08246809659251492, 'wd': 5.4833509078339605e-05, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.00023313045538540643, 'lr': 0.00013142861048341892, 'sched': 'cawr', 'cawr_T0': 72, 'cawr_Tmult': 1, 'epochs': 320, 'batch_size': 64, 'grad_clip': 1.2632255473110323}.\n",
            "[I 2025-10-19 20:35:09,683] Trial 23 finished with values: [0.5740185722006583, 0.07903236072924591, 55009.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 128, 'arch': '32-16', 'dropout': 0.449302276105764, 'input_dropout': 0.10376491708588702, 'noise_std': 0.062091551776747717, 'wd': 1.6932074437214245e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 1.6634609873972044e-05, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 5.013124357390735}.\n",
            "[I 2025-10-19 20:37:44,039] Trial 24 finished with values: [0.5984375767700093, 0.11261737936815408, 80385.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 16, 'arch': '128-64-32', 'dropout': 0.13921380447936543, 'input_dropout': 0.19489399469771054, 'noise_std': 0.060773679487885916, 'wd': 0.008987022400646503, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.9166445050164936e-05, 'sched': 'cawr', 'cawr_T0': 80, 'cawr_Tmult': 2, 'epochs': 384, 'batch_size': 64, 'grad_clip': 4.566178170123929}.\n",
            "[I 2025-10-19 20:38:31,308] Trial 25 finished with values: [0.5898884685304377, 0.048737655723523066, 43137.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.5145626428614796, 'input_dropout': 0.11602060389016122, 'noise_std': 0.06957486889846172, 'wd': 4.870849610200306e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.7430586177787164e-05, 'sched': 'cosine', 'epochs': 352, 'batch_size': 32, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 20:39:33,611] Trial 26 finished with values: [0.6371050950719795, 0.12133616109998768, 115201.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 0, 'arch': '128-64-32', 'dropout': 0.25107943534417165, 'input_dropout': 0.1052745037656236, 'noise_std': 0.01815977168014257, 'wd': 0.03485401443038997, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.0007890231621162025, 'lr': 0.0001552120833503202, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 3, 'epochs': 384, 'batch_size': 32, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 20:40:14,451] Trial 27 finished with values: [0.6014838107404313, 0.054814505164541116, 27137.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '128-64', 'dropout': 0.5914613906045199, 'input_dropout': 0.18881959144057875, 'noise_std': 0.06946964708544268, 'wd': 2.2946987874230535e-05, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 3.0015669271531814e-05, 'sched': 'none', 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 20:40:43,354] Trial 28 finished with values: [0.6114086375472904, 0.03408908208053518, 9473.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '64', 'dropout': 0.20231447421237492, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011706701642760588, 'wd': 5.187503589897551e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'cawr', 'cawr_T0': 40, 'cawr_Tmult': 3, 'epochs': 128, 'batch_size': 64, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 20:43:06,349] Trial 29 finished with values: [0.6323883456984228, 0.07155672587145967, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.3883519463959888, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 5.4833509078339605e-05, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.00016689080943341078, 'lr': 0.00013142861048341892, 'sched': 'none', 'epochs': 320, 'batch_size': 64, 'grad_clip': 1.2632255473110323}.\n",
            "[I 2025-10-19 20:44:28,026] Trial 30 finished with values: [0.651992335282268, 0.09948865976907506, 68865.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.3883519463959888, 'input_dropout': 0.17772233539785529, 'noise_std': 0.011706701642760588, 'wd': 5.4833509078339605e-05, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.0004236236270849814, 'lr': 0.00013142861048341892, 'sched': 'cawr', 'cawr_T0': 64, 'cawr_Tmult': 1, 'epochs': 192, 'batch_size': 128, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 20:47:35,784] Trial 31 finished with values: [0.6283594556085098, 0.0816924692694363, 37377.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 64, 'arch': '128-64-32', 'dropout': 0.48997835846665905, 'input_dropout': 0.210590063177311, 'noise_std': 0.060773679487885916, 'wd': 0.02818556266996745, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 0.0011055931000633079, 'lr': 6.18069149877641e-05, 'sched': 'none', 'epochs': 352, 'batch_size': 64, 'grad_clip': 4.566178170123929}.\n",
            "[I 2025-10-19 20:48:58,916] Trial 32 finished with values: [0.6659460521790399, 0.08397220908566616, 31233.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 16, 'arch': '128-64-32', 'dropout': 0.22031483602020202, 'input_dropout': 0.25346259329083637, 'noise_std': 0.060773679487885916, 'wd': 0.0003659675301686172, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00021110116810555505, 'sched': 'none', 'epochs': 352, 'batch_size': 128, 'grad_clip': 3.4209406399158793}.\n",
            "[I 2025-10-19 20:53:26,066] Trial 33 finished with values: [0.6147988011595342, 0.06706295785804928, 25089.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 32, 'arch': '128-64-32', 'dropout': 0.6656390644810753, 'input_dropout': 0.19489399469771054, 'noise_std': 0.060773679487885916, 'wd': 0.0003659675301686172, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.00023352205402988653, 'lr': 3.3674073748740536e-05, 'sched': 'none', 'epochs': 352, 'batch_size': 32, 'grad_clip': 4.566178170123929}.\n",
            "[I 2025-10-19 20:55:39,349] Trial 34 finished with values: [0.6288507836682553, 0.10842706707404326, 13633.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 0, 'arch': '64-64', 'dropout': 0.25107943534417165, 'input_dropout': 0.11774806692200544, 'noise_std': 0.01815977168014257, 'wd': 0.011812516086413106, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 4.2528806251225656e-06, 'lr': 9.908020585782568e-05, 'sched': 'cawr', 'cawr_T0': 56, 'cawr_Tmult': 3, 'epochs': 192, 'batch_size': 64, 'grad_clip': 6.460435417445703}.\n",
            "[I 2025-10-19 20:58:17,555] Trial 35 finished with values: [0.6325357441163465, 0.15122512436464297, 822785.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 32, 'arch': '512', 'dropout': 0.3513552697027288, 'input_dropout': 0.27445377032043, 'noise_std': 0.03623938991166331, 'wd': 0.0007997269865263961, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 5.332124451932883e-08, 'lr': 1.0997959032388697e-05, 'sched': 'none', 'epochs': 384, 'batch_size': 32, 'grad_clip': 2.8013521982032907}.\n",
            "[I 2025-10-19 20:58:51,544] Trial 36 finished with values: [0.5687122291554071, 0.012391991888299536, 119297.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 32, 'arch': '128-64-32', 'dropout': 0.5145626428614796, 'input_dropout': 0.11602060389016122, 'noise_std': 0.07948113035416485, 'wd': 4.870849610200306e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 1.0997959032388697e-05, 'sched': 'none', 'epochs': 384, 'batch_size': 32, 'grad_clip': 9.354867063289529}.\n",
            "[I 2025-10-19 21:01:26,447] Trial 37 finished with values: [0.5775070014248513, 0.07008544647016846, 119297.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 32, 'arch': '128-64-32', 'dropout': 0.3754814537429568, 'input_dropout': 0.2940097725856431, 'noise_std': 0.060773679487885916, 'wd': 0.0003659675301686172, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 1.811665941459424e-05, 'sched': 'cawr', 'cawr_T0': 56, 'cawr_Tmult': 3, 'epochs': 352, 'batch_size': 64, 'grad_clip': 3.5073130512876274}.\n",
            "[I 2025-10-19 21:02:03,378] Trial 38 finished with values: [0.652532796147988, 0.05872719952955685, 174081.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 256, 'arch': '512-256', 'dropout': 0.3883519463959888, 'input_dropout': 0.17772233539785529, 'noise_std': 0.011775108289014114, 'wd': 0.0007631016235941566, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 1.9259855406086172e-08, 'lr': 0.00013142861048341892, 'sched': 'cawr', 'cawr_T0': 48, 'cawr_Tmult': 2, 'epochs': 320, 'batch_size': 128, 'grad_clip': 1.2632255473110323}.\n",
            "[I 2025-10-19 21:03:39,205] Trial 39 finished with values: [0.5917555151574706, 0.10100658910236437, 34049.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 0, 'arch': '64', 'dropout': 0.4340577358083578, 'input_dropout': 0.17148380684096995, 'noise_std': 0.027997909366028417, 'wd': 0.03485401443038997, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 7.231191526754465e-07, 'lr': 1.567127460068538e-05, 'sched': 'cosine', 'epochs': 384, 'batch_size': 32, 'grad_clip': 4.029438503745285}.\n",
            "[I 2025-10-19 21:04:36,405] Trial 40 finished with values: [0.5951948115756891, 0.05303492137416488, 40705.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 64, 'arch': '64-32-16', 'dropout': 0.6352279331062679, 'input_dropout': 0.029832417827803548, 'noise_std': 0.02431721909994541, 'wd': 0.004086341509440911, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.00029033269763593, 'lr': 3.1481049378577344e-05, 'sched': 'none', 'epochs': 384, 'batch_size': 32, 'grad_clip': 9.354867063289529}.\n",
            "[I 2025-10-19 21:06:45,852] Trial 41 finished with values: [0.6582813344470103, 0.07563275952759807, 3201.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 0, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'none', 'epochs': 192, 'batch_size': 64, 'grad_clip': 9.376815930432082}.\n",
            "[I 2025-10-19 21:07:12,990] Trial 42 finished with values: [0.5968653269788238, 0.0, 35073.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 16, 'arch': '64', 'dropout': 0.41385870650208934, 'input_dropout': 0.18881959144057875, 'noise_std': 0.06957486889846172, 'wd': 0.00018737227240512905, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.4876607835597055e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 21:08:48,429] Trial 43 finished with values: [0.6309143615191863, 0.12223465789307064, 250881.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 0, 'arch': '256-128-64', 'dropout': 0.6326225096549798, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011706701642760588, 'wd': 0.07481106041623166, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 3.646805819620637e-05, 'lr': 9.908020585782568e-05, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 1, 'epochs': 192, 'batch_size': 32, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 21:10:04,568] Trial 44 finished with values: [0.6182380975777526, 0.12277479774679179, 33281.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 32, 'arch': '128-64-32', 'dropout': 0.3513552697027288, 'input_dropout': 0.19489399469771054, 'noise_std': 0.03623938991166331, 'wd': 7.164630786905331e-05, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00047641670771215815, 'sched': 'cawr', 'cawr_T0': 16, 'cawr_Tmult': 1, 'epochs': 320, 'batch_size': 64, 'grad_clip': 2.716189485972999}.\n",
            "[I 2025-10-19 21:10:35,288] Trial 45 finished with values: [0.6083132707708937, 0.0, 43137.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 2.7430586177787164e-05, 'sched': 'none', 'epochs': 320, 'batch_size': 32, 'grad_clip': 7.533601109832154}.\n",
            "[I 2025-10-19 21:11:13,331] Trial 46 finished with values: [0.5962757333071291, 0.02059957494488107, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '32-16', 'dropout': 0.20231447421237492, 'input_dropout': 0.019515477895583853, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 1.6119044727609182e-05, 'sched': 'none', 'epochs': 128, 'batch_size': 128, 'grad_clip': 5.704595464437946}.\n",
            "[I 2025-10-19 21:12:18,739] Trial 47 finished with values: [0.5676804402299416, 0.029012327415927408, 3393.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 64, 'arch': '16', 'dropout': 0.3152982904741015, 'input_dropout': 0.26316016224393246, 'noise_std': 0.04196000624277899, 'wd': 0.012105903211961774, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 0.0008575379003843558, 'lr': 1.1727272411645968e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 4.904664843141758}.\n",
            "[I 2025-10-19 21:13:22,914] Trial 48 finished with values: [0.6687466221195892, 0.08222454488788644, 68865.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.11103440592848005, 'input_dropout': 0.2742896419681497, 'noise_std': 0.011706701642760588, 'wd': 5.187503589897551e-06, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 2.0408761578124986e-06, 'lr': 0.0001552120833503202, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 2, 'epochs': 384, 'batch_size': 128, 'grad_clip': 6.606699252071345}.\n",
            "[I 2025-10-19 21:15:01,208] Trial 49 finished with values: [0.6326831425342702, 0.07647658326690798, 4417.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 256, 'arch': '16', 'dropout': 0.47893613205551233, 'input_dropout': 0.26316016224393246, 'noise_std': 0.03924451074226354, 'wd': 0.012105903211961774, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00018627986386722937, 'sched': 'none', 'epochs': 192, 'batch_size': 64, 'grad_clip': 2.0520823663752767}.\n",
            "[I 2025-10-19 21:18:44,419] Trial 50 finished with values: [0.6106225126516975, 0.10013873820032493, 59105.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 256, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.26316016224393246, 'noise_std': 0.03308980248526492, 'wd': 0.012105903211961774, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.7430586177787164e-05, 'sched': 'none', 'epochs': 320, 'batch_size': 64, 'grad_clip': 2.0520823663752767}.\n",
            "[I 2025-10-19 21:19:54,281] Trial 51 finished with values: [0.6441310863263401, 0.07308059760243912, 25089.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 32, 'arch': '128-64-32', 'dropout': 0.6656390644810753, 'input_dropout': 0.19489399469771054, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'none', 'epochs': 320, 'batch_size': 32, 'grad_clip': 5.704595464437946}.\n",
            "[I 2025-10-19 21:20:24,972] Trial 52 finished with values: [0.6144548715177124, 0.009147950269678184, 92801.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 64, 'arch': '128-128', 'dropout': 0.25107943534417165, 'input_dropout': 0.18351622387030567, 'noise_std': 0.04196000624277899, 'wd': 1.7324270154949117e-05, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 0.00034665279082617026, 'lr': 9.908020585782568e-05, 'sched': 'cawr', 'cawr_T0': 40, 'cawr_Tmult': 3, 'epochs': 384, 'batch_size': 32, 'grad_clip': 6.460435417445703}.\n",
            "[I 2025-10-19 21:22:02,601] Trial 53 finished with values: [0.6722841841497568, 0.1212077500380927, 68865.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.2002251447405699, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011775108289014114, 'wd': 0.0007631016235941566, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 2, 'epochs': 384, 'batch_size': 128, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 21:22:39,412] Trial 54 finished with values: [0.5947034835159436, 0.011687100181750565, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '32-16', 'dropout': 0.20231447421237492, 'input_dropout': 0.019515477895583853, 'noise_std': 0.06957486889846172, 'wd': 2.078699690689779e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.4876607835597055e-05, 'sched': 'none', 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 21:24:16,757] Trial 55 finished with values: [0.6487004372819731, 0.14661468632361951, 534529.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 256, 'arch': '512-256', 'dropout': 0.41385870650208934, 'input_dropout': 0.17772233539785529, 'noise_std': 0.06957486889846172, 'wd': 0.00018737227240512905, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 3.977506130697732e-06, 'lr': 0.00013142861048341892, 'sched': 'cawr', 'cawr_T0': 72, 'cawr_Tmult': 3, 'epochs': 320, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 21:25:40,048] Trial 56 finished with values: [0.6441310863263401, 0.09495332900618414, 31233.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 16, 'arch': '128-64-32', 'dropout': 0.3152982904741015, 'input_dropout': 0.26316016224393246, 'noise_std': 0.03924451074226354, 'wd': 0.0003659675301686172, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00021110116810555505, 'sched': 'none', 'epochs': 128, 'batch_size': 128, 'grad_clip': 2.0520823663752767}.\n",
            "[I 2025-10-19 21:26:44,896] Trial 57 finished with values: [0.6722841841497568, 0.07810511389460939, 38209.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 256, 'arch': '64-64', 'dropout': 0.20231447421237492, 'input_dropout': 0.019515477895583853, 'noise_std': 0.08647223762550532, 'wd': 0.01160125694455748, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 4.102015775325905e-08, 'lr': 0.0002776677435509801, 'sched': 'none', 'epochs': 128, 'batch_size': 128, 'grad_clip': 5.68276930813414}.\n",
            "[I 2025-10-19 21:27:45,725] Trial 58 finished with values: [0.6315530879968555, 0.07944645204958323, 25921.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 64, 'arch': '64-64', 'dropout': 0.3152982904741015, 'input_dropout': 0.28016250525837144, 'noise_std': 0.03161561049767505, 'wd': 0.011812516086413106, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 6.000487726924257e-06, 'lr': 0.00030959834513535173, 'sched': 'cawr', 'cawr_T0': 16, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 64, 'grad_clip': 9.376815930432082}.\n",
            "[I 2025-10-19 21:29:08,217] Trial 59 finished with values: [0.6441310863263401, 0.10288337173199091, 117249.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 16, 'arch': '128-64-32', 'dropout': 0.5001185545276626, 'input_dropout': 0.25346259329083637, 'noise_std': 0.011706701642760588, 'wd': 4.991789621744231e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00021110116810555505, 'sched': 'none', 'epochs': 352, 'batch_size': 128, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 21:32:43,286] Trial 60 finished with values: [0.6053653024124208, 0.08664314299995801, 54529.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 32, 'arch': '64', 'dropout': 0.6326225096549798, 'input_dropout': 0.1052745037656236, 'noise_std': 0.0029973589872677956, 'wd': 5.187503589897551e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 1.0997959032388697e-05, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 1, 'epochs': 384, 'batch_size': 32, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 21:33:15,647] Trial 61 finished with values: [0.6068392865916572, 0.027625089173777595, 82433.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 32, 'arch': '128-64-32', 'dropout': 0.5145626428614796, 'input_dropout': 0.11602060389016122, 'noise_std': 0.06957486889846172, 'wd': 4.870849610200306e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0004444792315132321, 'sched': 'cosine', 'epochs': 96, 'batch_size': 32, 'grad_clip': 9.354867063289529}.\n",
            "[I 2025-10-19 21:35:00,665] Trial 62 finished with values: [0.6011398810986095, 0.08451788020211704, 54529.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 32, 'arch': '64', 'dropout': 0.5145626428614796, 'input_dropout': 0.11602060389016122, 'noise_std': 0.011706701642760588, 'wd': 4.870849610200306e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 1.0997959032388697e-05, 'sched': 'cawr', 'cawr_T0': 64, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 32, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 21:35:45,998] Trial 63 finished with values: [0.6363189701763867, 0.08769780981618747, 174081.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 64, 'arch': '512-256', 'dropout': 0.43261243150684037, 'input_dropout': 0.17772233539785529, 'noise_std': 0.011775108289014114, 'wd': 0.0007631016235941566, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 6.217221275551168e-05, 'lr': 0.00013142861048341892, 'sched': 'cosine', 'epochs': 320, 'batch_size': 64, 'grad_clip': 5.474572876075846}.\n",
            "[I 2025-10-19 21:37:19,963] Trial 64 finished with values: [0.589642804500565, 0.07449688838345458, 18049.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 16, 'arch': '32-16', 'dropout': 0.41385870650208934, 'input_dropout': 0.18881959144057875, 'noise_std': 0.06957486889846172, 'wd': 0.00018737227240512905, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.4876607835597055e-05, 'sched': 'none', 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 21:37:56,952] Trial 65 finished with values: [0.5908219918439542, 0.06084353879283211, 403457.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 256, 'arch': '512-256', 'dropout': 0.41385870650208934, 'input_dropout': 0.016912648995278133, 'noise_std': 0.06957486889846172, 'wd': 1.1977764279576476e-05, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 3.0015669271531814e-05, 'sched': 'cawr', 'cawr_T0': 56, 'cawr_Tmult': 2, 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 21:39:23,164] Trial 66 finished with values: [0.6298825725937208, 0.08822131041182635, 4737.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.011706701642760588, 'wd': 2.078699690689779e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.00024517478425066546, 'sched': 'none', 'epochs': 128, 'batch_size': 64, 'grad_clip': 9.376815930432082}.\n",
            "[I 2025-10-19 21:40:30,972] Trial 67 finished with values: [0.5907237262320051, 0.10061972607967717, 37889.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 64, 'arch': '256', 'dropout': 0.48997835846665905, 'input_dropout': 0.18351622387030567, 'noise_std': 0.04196000624277899, 'wd': 1.7324270154949117e-05, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 4.5540686406553345e-08, 'lr': 6.18069149877641e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 32, 'grad_clip': 2.143544613866963}.\n",
            "[I 2025-10-19 21:41:59,375] Trial 68 finished with values: [0.6260993465336805, 0.07046984322764238, 12033.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '64-32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.002564206580643569, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'none', 'epochs': 128, 'batch_size': 64, 'grad_clip': 5.704595464437946}.\n",
            "[I 2025-10-19 21:42:50,314] Trial 69 finished with values: [0.6333710018179138, 0.05714120911789766, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'none', 'epochs': 320, 'batch_size': 64, 'grad_clip': 6.221551323045991}.\n",
            "[I 2025-10-19 21:44:59,348] Trial 70 finished with values: [0.5878248906795067, 0.05787786848770027, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.20231447421237492, 'input_dropout': 0.18881959144057875, 'noise_std': 0.06957486889846172, 'wd': 0.00018737227240512905, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 0.00015465300719004436, 'lr': 1.6119044727609182e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 5.704595464437946}.\n",
            "[I 2025-10-19 21:48:09,188] Trial 71 finished with values: [0.6042352478750062, 0.09723826600395402, 206849.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 64, 'arch': '512-256', 'dropout': 0.11103440592848005, 'input_dropout': 0.2742896419681497, 'noise_std': 0.08129857387160935, 'wd': 0.0007631016235941566, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 8.57525738724858e-05, 'lr': 1.0439545297949767e-05, 'sched': 'none', 'epochs': 384, 'batch_size': 128, 'grad_clip': 2.143544613866963}.\n",
            "[I 2025-10-19 21:49:09,864] Trial 72 finished with values: [0.6317496192207537, 0.08986210239096792, 13633.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 0, 'arch': '64-64', 'dropout': 0.6656390644810753, 'input_dropout': 0.19489399469771054, 'noise_std': 0.0023271935735825868, 'wd': 0.011812516086413106, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'cawr', 'cawr_T0': 80, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 64, 'grad_clip': 5.704595464437946}.\n",
            "[I 2025-10-19 21:49:37,644] Trial 73 finished with values: [0.6067901537856827, 0.0, 85249.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '64', 'dropout': 0.6326225096549798, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011706701642760588, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 2.7430586177787164e-05, 'sched': 'none', 'epochs': 320, 'batch_size': 128, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 21:53:04,537] Trial 74 finished with values: [0.584090797425441, 0.03717900690033915, 12033.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 64, 'arch': '64-32-16', 'dropout': 0.3152982904741015, 'input_dropout': 0.18698943804826737, 'noise_std': 0.04196000624277899, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 1.1727272411645968e-05, 'sched': 'cosine', 'epochs': 320, 'batch_size': 64, 'grad_clip': 4.904664843141758}.\n",
            "[I 2025-10-19 21:53:39,739] Trial 75 finished with values: [0.6276224635188916, 0.0, 27777.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 32, 'arch': '32-16', 'dropout': 0.5145626428614796, 'input_dropout': 0.11602060389016122, 'noise_std': 0.03308980248526492, 'wd': 4.870849610200306e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 1.0997959032388697e-05, 'sched': 'none', 'epochs': 384, 'batch_size': 32, 'grad_clip': 9.354867063289529}.\n",
            "[I 2025-10-19 21:54:22,240] Trial 76 finished with values: [0.6321426816685501, 0.0, 4353.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 16, 'arch': '64', 'dropout': 0.3883519463959888, 'input_dropout': 0.17772233539785529, 'noise_std': 0.06957486889846172, 'wd': 0.0007631016235941566, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 1.0319472283946684e-05, 'lr': 1.1851043470018121e-05, 'sched': 'cawr', 'cawr_T0': 56, 'cawr_Tmult': 1, 'epochs': 256, 'batch_size': 128, 'grad_clip': 1.2632255473110323}.\n",
            "[I 2025-10-19 21:55:45,005] Trial 77 finished with values: [0.633027072176092, 0.07620469454432832, 9473.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.6305238780090446, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011706701642760588, 'wd': 5.187503589897551e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 2, 'epochs': 128, 'batch_size': 128, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 21:57:04,656] Trial 78 finished with values: [0.6476195155505331, 0.0684173472336822, 42625.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '32', 'dropout': 0.41385870650208934, 'input_dropout': 0.210590063177311, 'noise_std': 0.08398613028667204, 'wd': 0.00018737227240512905, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 6.18069149877641e-05, 'sched': 'none', 'epochs': 256, 'batch_size': 128, 'grad_clip': 9.1002081105821}.\n",
            "[I 2025-10-19 21:59:32,491] Trial 79 finished with values: [0.6279663931607135, 0.08109358367536024, 43137.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.10602641379081543, 'noise_std': 0.03308980248526492, 'wd': 1.5288074657312176e-05, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 2.7430586177787164e-05, 'sched': 'none', 'epochs': 320, 'batch_size': 32, 'grad_clip': 6.221551323045991}.\n",
            "[I 2025-10-19 22:01:04,843] Trial 80 finished with values: [0.6538102491033263, 0.08677594231493768, 38209.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 256, 'arch': '64-64', 'dropout': 0.20231447421237492, 'input_dropout': 0.016912648995278133, 'noise_std': 0.08647223762550532, 'wd': 0.01160125694455748, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 0.0001883791744816948, 'lr': 0.0002776677435509801, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 1, 'epochs': 256, 'batch_size': 128, 'grad_clip': 5.68276930813414}.\n",
            "[I 2025-10-19 22:03:29,356] Trial 81 finished with values: [0.6402004618483762, 0.12403177590882375, 43137.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.1052745037656236, 'noise_std': 0.03308980248526492, 'wd': 0.0007631016235941566, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'none', 'epochs': 320, 'batch_size': 64, 'grad_clip': 6.221551323045991}.\n",
            "[I 2025-10-19 22:04:04,855] Trial 82 finished with values: [0.6352871812509212, 0.07924790544755433, 85249.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '64', 'dropout': 0.11103440592848005, 'input_dropout': 0.18698943804826737, 'noise_std': 0.011706701642760588, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 6.417871422674918e-05, 'lr': 0.0001552120833503202, 'sched': 'none', 'epochs': 384, 'batch_size': 128, 'grad_clip': 6.606699252071345}.\n",
            "[I 2025-10-19 22:05:29,204] Trial 83 finished with values: [0.6155357932491524, 0.14092156683906998, 92801.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 64, 'arch': '128-128', 'dropout': 0.25107943534417165, 'input_dropout': 0.18351622387030567, 'noise_std': 0.04196000624277899, 'wd': 1.7324270154949117e-05, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 1.6387130734265401e-06, 'lr': 9.908020585782568e-05, 'sched': 'cawr', 'cawr_T0': 40, 'cawr_Tmult': 3, 'epochs': 384, 'batch_size': 32, 'grad_clip': 6.460435417445703}.\n",
            "[I 2025-10-19 22:06:11,271] Trial 84 finished with values: [0.6388247432810887, 0.09163692817906988, 73025.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 256, 'arch': '64-64', 'dropout': 0.5145626428614796, 'input_dropout': 0.019515477895583853, 'noise_std': 0.08647223762550532, 'wd': 0.01160125694455748, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 5.049299035503888e-08, 'lr': 0.0002776677435509801, 'sched': 'none', 'epochs': 96, 'batch_size': 32, 'grad_clip': 5.68276930813414}.\n",
            "[I 2025-10-19 22:07:33,091] Trial 85 finished with values: [0.6585761312828575, 0.1299893123506648, 117249.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 16, 'arch': '128-64-32', 'dropout': 0.22031483602020202, 'input_dropout': 0.18698943804826737, 'noise_std': 0.060773679487885916, 'wd': 0.0003659675301686172, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 0.00017007086605340253, 'sched': 'none', 'epochs': 320, 'batch_size': 128, 'grad_clip': 3.4209406399158793}.\n",
            "[I 2025-10-19 22:11:50,879] Trial 86 finished with values: [0.6016803419643296, 0.06921457250093277, 3329.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 0, 'arch': '64', 'dropout': 0.25107943534417165, 'input_dropout': 0.10370879805117583, 'noise_std': 0.04196000624277899, 'wd': 0.012105903211961774, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 1.471957519794018e-05, 'lr': 1.1727272411645968e-05, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 3, 'epochs': 384, 'batch_size': 64, 'grad_clip': 4.904664843141758}.\n",
            "[I 2025-10-19 22:14:00,491] Trial 87 finished with values: [0.6658969193730654, 0.0636614697919804, 3201.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 0, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'none', 'epochs': 192, 'batch_size': 64, 'grad_clip': 9.376815930432082}.\n",
            "[I 2025-10-19 22:16:09,230] Trial 88 finished with values: [0.614602269935636, 0.06473251529930046, 5377.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 0, 'arch': '64', 'dropout': 0.6178620555253561, 'input_dropout': 0.2742896419681497, 'noise_std': 0.01918673247187429, 'wd': 5.187503589897551e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 3.543223798586142e-05, 'sched': 'cawr', 'cawr_T0': 48, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 64, 'grad_clip': 5.796620078962018}.\n",
            "[I 2025-10-19 22:17:32,404] Trial 89 finished with values: [0.5914607183216234, 0.05106669878975745, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '32-16', 'dropout': 0.20231447421237492, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 1.6119044727609182e-05, 'sched': 'none', 'epochs': 128, 'batch_size': 128, 'grad_clip': 6.221551323045991}.\n",
            "[I 2025-10-19 22:17:59,576] Trial 90 finished with values: [0.6139144106519924, 0.01126776974955268, 206849.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 64, 'arch': '512-256', 'dropout': 0.48997835846665905, 'input_dropout': 0.210590063177311, 'noise_std': 0.011775108289014114, 'wd': 4.184742749494778e-05, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 6.666407476287439e-07, 'lr': 6.18069149877641e-05, 'sched': 'none', 'epochs': 64, 'batch_size': 128, 'grad_clip': 7.025382596783711}.\n",
            "[I 2025-10-19 22:19:32,562] Trial 91 finished with values: [0.6804893627475065, 0.17769531691977525, 813057.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '512-256', 'dropout': 0.2002251447405699, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011775108289014114, 'wd': 0.00018737227240512905, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}.\n",
            "[I 2025-10-19 22:20:40,638] Trial 92 finished with values: [0.6114086375472904, 0.018966030648860488, 27265.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 16, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.06957486889846172, 'wd': 0.00018737227240512905, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 2.7430586177787164e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 7.533601109832154}.\n",
            "[I 2025-10-19 22:22:02,092] Trial 93 finished with values: [0.6183854959956763, 0.0367878732472845, 4737.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 256, 'arch': '32', 'dropout': 0.48997835846665905, 'input_dropout': 0.210590063177311, 'noise_std': 0.07957926694361012, 'wd': 0.01160125694455748, 'apply_final_wd': 1, 'use_l1': 1, 'l1': 0.0027862611888515294, 'lr': 6.18069149877641e-05, 'sched': 'none', 'epochs': 128, 'batch_size': 128, 'grad_clip': 2.143544613866963}.\n",
            "[I 2025-10-19 22:23:24,057] Trial 94 finished with values: [0.6519432024762934, 0.10602147530684125, 38209.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 256, 'arch': '64-64', 'dropout': 0.3883519463959888, 'input_dropout': 0.19855175695481359, 'noise_std': 0.08647223762550532, 'wd': 0.01160125694455748, 'apply_final_wd': 0, 'use_l1': 1, 'l1': 1.135134728636729e-05, 'lr': 0.0002776677435509801, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 1.2632255473110323}.\n",
            "[I 2025-10-19 22:24:09,129] Trial 95 finished with values: [0.6170097774283889, 0.07706014659614935, 9345.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 512, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03924451074226354, 'wd': 0.0006478643450593017, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 0.0004236201735117859, 'sched': 'none', 'epochs': 320, 'batch_size': 64, 'grad_clip': 2.0520823663752767}.\n",
            "[I 2025-10-19 22:25:13,339] Trial 96 finished with values: [0.6525819289539626, 0.12291640025087192, 101825.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 16, 'arch': '64', 'dropout': 0.22031483602020202, 'input_dropout': 0.1052745037656236, 'noise_std': 0.060773679487885916, 'wd': 5.187503589897551e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'cawr', 'cawr_T0': 40, 'cawr_Tmult': 3, 'epochs': 320, 'batch_size': 128, 'grad_clip': 7.780403667330475}.\n",
            "[I 2025-10-19 22:26:16,628] Trial 97 finished with values: [0.6742494963887388, 0.12841934258872356, 206849.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '512-256', 'dropout': 0.2002251447405699, 'input_dropout': 0.07039262455663606, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 9.376815930432082}.\n",
            "[I 2025-10-19 22:28:06,301] Trial 98 finished with values: [0.6488969685058714, 0.10122079068904355, 4417.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 256, 'arch': '16', 'dropout': 0.3152982904741015, 'input_dropout': 0.019515477895583853, 'noise_std': 0.08647223762550532, 'wd': 0.012105903211961774, 'apply_final_wd': 1, 'use_l1': 0, 'lr': 0.0002776677435509801, 'sched': 'none', 'epochs': 192, 'batch_size': 64, 'grad_clip': 2.0520823663752767}.\n",
            "[I 2025-10-19 22:30:15,734] Trial 99 finished with values: [0.655922959760232, 0.06770590759026929, 3201.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 0, 'arch': '32-16', 'dropout': 0.6178620555253561, 'input_dropout': 0.18698943804826737, 'noise_std': 0.03308980248526492, 'wd': 2.078699690689779e-06, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0003216235469207422, 'sched': 'none', 'epochs': 192, 'batch_size': 64, 'grad_clip': 9.376815930432082}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Chosen Pareto] Val CI=0.6805 | Gap=0.1777 | Params=813057\n",
            "[Chosen Params] {'top_k_genes': 800, 'top_k_inter': 512, 'arch': '512-256', 'dropout': 0.2002251447405699, 'input_dropout': 0.1052745037656236, 'noise_std': 0.011775108289014114, 'wd': 0.00018737227240512905, 'apply_final_wd': 0, 'use_l1': 0, 'lr': 0.0001552120833503202, 'sched': 'cosine', 'epochs': 256, 'batch_size': 128, 'grad_clip': 4.180170052234475}\n",
            "[Final] Using features: 18 clinical + 800 genes (main) + 512 interactions\n",
            "\n",
            "[Final] Train+Val CI: 0.8692\n",
            "[Final] Test CI:      0.6233\n",
            "[Train+Val] CI by arm: {'ACT=1': 0.6878566537320246, 'ACT=0': 0.9276072855401425}\n",
            "[Test]      CI by arm: {'ACT=1': 0.5584642233856894, 'ACT=0': 0.6499927818680525}\n",
            "Saved final model and parameters to: /content/drive/MyDrive/deepsurv_results_optuna_interactions_iptw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab-ready SINGLE CELL\n",
        "# DeepSurv + Multi-Objective Optuna (Val CI ↑, Gap ↓, Params ↓)\n",
        "# with capacity budgets, conservative Pareto selector, column-dropout,\n",
        "# grouped L1 (stronger on interactions), and WD applied to last layer.\n",
        "# ============================================================\n",
        "\n",
        "# ---------- Installs (Colab) ----------\n",
        "!pip -q install optuna optuna-dashboard scikit-survival portpicker\n",
        "\n",
        "# ---------- (Optional) Mount Google Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, math, copy, warnings, random, gc, time, threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "# Prefer torchsurv if available; fallback to custom Efron\n",
        "try:\n",
        "    from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "    _HAS_TORCHSURV = True\n",
        "except Exception:\n",
        "    _HAS_TORCHSURV = False\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "from sksurv.util import Surv\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import NSGAIISampler\n",
        "\n",
        "import portpicker\n",
        "from google.colab import output\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# Cox Losses\n",
        "# ============================================================\n",
        "def _cox_negloglik_efron(pred, event, time):\n",
        "    eta = pred.reshape(-1)\n",
        "    e = event.to(torch.float32).reshape(-1)\n",
        "    t = time.reshape(-1)\n",
        "    order = torch.argsort(t, descending=True)\n",
        "    t = t[order]; e = e[order]; eta = eta[order]\n",
        "    exp_eta = torch.exp(eta)\n",
        "    cum_exp = torch.cumsum(exp_eta, dim=0)\n",
        "    uniq_mask = torch.ones_like(t, dtype=torch.bool)\n",
        "    uniq_mask[1:] = t[1:] != t[:-1]\n",
        "    idxs = torch.nonzero(uniq_mask, as_tuple=False).reshape(-1)\n",
        "    idxs = torch.cat([idxs, torch.tensor([len(t)], device=t.device)])\n",
        "    nll = torch.tensor(0.0, device=t.device)\n",
        "    for k in range(len(idxs)-1):\n",
        "        start, end = idxs[k].item(), idxs[k+1].item()\n",
        "        e_slice = e[start:end]\n",
        "        d = int(e_slice.sum().item())\n",
        "        if d == 0: continue\n",
        "        eta_events = eta[start:end][e_slice.bool()]\n",
        "        exp_events = torch.exp(eta[start:end])[e_slice.bool()]\n",
        "        s_eta = eta_events.sum()\n",
        "        risk_sum = cum_exp[end-1]\n",
        "        s_exp = exp_events.sum()\n",
        "        eps = 1e-12\n",
        "        log_terms = 0.0\n",
        "        for j in range(d):\n",
        "            log_terms = log_terms + torch.log(risk_sum - (j / d) * s_exp + eps)\n",
        "        nll = nll - (s_eta - log_terms)\n",
        "    return nll / t.numel()\n",
        "\n",
        "def cox_negloglik(pred, event, time):\n",
        "    if _HAS_TORCHSURV:\n",
        "        return neg_partial_log_likelihood(pred, event, time, reduction='mean')\n",
        "    return _cox_negloglik_efron(pred, event, time)\n",
        "\n",
        "# Weighted Breslow negative partial log-likelihood (IPTW)\n",
        "def cox_negloglik_breslow_weighted(pred, event, time, weight=None):\n",
        "    eta = pred.reshape(-1)\n",
        "    e = event.to(torch.float32).reshape(-1)\n",
        "    t = time.reshape(-1)\n",
        "    if weight is None:\n",
        "        weight = torch.ones_like(eta)\n",
        "    else:\n",
        "        weight = weight.reshape(-1)\n",
        "\n",
        "    order = torch.argsort(t, descending=True)\n",
        "    t = t[order]; e = e[order]; eta = eta[order]; w = weight[order]\n",
        "    exp_eta = torch.exp(eta)\n",
        "    w_exp = w * exp_eta\n",
        "    cum_w_exp = torch.cumsum(w_exp, dim=0)\n",
        "\n",
        "    uniq = torch.ones_like(t, dtype=torch.bool)\n",
        "    uniq[1:] = t[1:] != t[:-1]\n",
        "    idxs = torch.nonzero(uniq, as_tuple=False).reshape(-1)\n",
        "    idxs = torch.cat([idxs, torch.tensor([len(t)], device=t.device)])\n",
        "\n",
        "    nll = torch.tensor(0.0, device=t.device)\n",
        "    eps = 1e-12\n",
        "    for k in range(len(idxs)-1):\n",
        "        s, eidx = idxs[k].item(), idxs[k+1].item()\n",
        "        emask = e[s:eidx] > 0.5\n",
        "        if emask.sum() == 0:\n",
        "            continue\n",
        "        w_events = w[s:eidx][emask]\n",
        "        eta_events = eta[s:eidx][emask]\n",
        "        s_eta = (w_events * eta_events).sum()\n",
        "        denom = cum_w_exp[eidx-1]\n",
        "        nll = nll - (s_eta - w_events.sum() * torch.log(denom + eps))\n",
        "    return nll / (w.sum() + 1e-9)\n",
        "\n",
        "def cox_loss(pred, event, time, weight=None):\n",
        "    \"\"\"Use weighted Breslow if weights provided; else Efron/torchsurv.\"\"\"\n",
        "    if weight is None:\n",
        "        return cox_negloglik(pred, event, time)\n",
        "    return cox_negloglik_breslow_weighted(pred, event, time, weight)\n",
        "\n",
        "# ============================================================\n",
        "# Model, Dataset (with weights), Sampler, Utils\n",
        "# ============================================================\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(d, units), activation]\n",
        "            if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "            d = units\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events, weights=None):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events.astype(bool), dtype=torch.bool)\n",
        "        if weights is None:\n",
        "            self.w = torch.ones(len(self.x), dtype=torch.float32)\n",
        "        else:\n",
        "            self.w = torch.tensor(weights, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.time[idx], self.event[idx], self.w[idx]\n",
        "\n",
        "class EventBalancedBatchSampler(Sampler):\n",
        "    def __init__(self, events_numpy, batch_size, seed=0):\n",
        "        events = np.asarray(events_numpy).astype(bool)\n",
        "        self.pos_idx = np.where(events)[0]\n",
        "        self.neg_idx = np.where(~events)[0]\n",
        "        assert len(self.pos_idx) > 0, \"No events in training set — cannot balance batches.\"\n",
        "        self.bs = int(batch_size)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "    def __iter__(self):\n",
        "        pos = self.rng.permutation(self.pos_idx)\n",
        "        neg = self.rng.permutation(self.neg_idx)\n",
        "        n_total = len(pos) + len(neg)\n",
        "        n_batches = math.ceil(n_total / self.bs)\n",
        "        pi = ni = 0\n",
        "        for _ in range(n_batches):\n",
        "            take_pos = 1 if pi < len(pos) else 0\n",
        "            avail_neg = max(0, len(neg) - ni)\n",
        "            take_neg = min(self.bs - take_pos, avail_neg)\n",
        "            need = self.bs - (take_pos + take_neg)\n",
        "            extra_pos = min(need, max(0, len(pos) - (pi + take_pos)))\n",
        "            take_pos += extra_pos\n",
        "            batch = np.concatenate([pos[pi:pi+take_pos], neg[ni:ni+take_neg]])\n",
        "            pi += take_pos; ni += take_neg\n",
        "            if batch.size == 0: break\n",
        "            self.rng.shuffle(batch)\n",
        "            yield batch.tolist()\n",
        "    def __len__(self):\n",
        "        return math.ceil((len(self.pos_idx) + len(self.neg_idx)) / self.bs)\n",
        "\n",
        "def make_optimizer_groups(model, lr, wd, apply_final_wd=True):\n",
        "    # apply_final_wd now defaults to True\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight) and not apply_final_wd:\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "    return optim.AdamW(\n",
        "        [{'params': decay, 'weight_decay': float(wd)},\n",
        "         {'params': no_decay, 'weight_decay': 0.0}],\n",
        "        lr=float(lr)\n",
        "    )\n",
        "\n",
        "def set_dropout_p(model, p):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout): m.p = float(p)\n",
        "\n",
        "def set_weight_decay(optimizer, wd):\n",
        "    for g in optimizer.param_groups: g['weight_decay'] = float(wd)\n",
        "\n",
        "def l1_penalty_first_layer(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear): return m.weight.abs().sum()\n",
        "    return torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "\n",
        "# ---- NEW: grouped L1 that shrinks interaction columns more than mains\n",
        "def l1_first_layer_grouped(model, inter_start_idx, lam_main=0.0, lam_inter=0.0):\n",
        "    W = None\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            W = m.weight   # [hidden, in_dim]\n",
        "            break\n",
        "    if W is None or (lam_main == 0 and lam_inter == 0):\n",
        "        return torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "    main_sum = W[:, :inter_start_idx].abs().sum()\n",
        "    inter_sum = W[:, inter_start_idx:].abs().sum()\n",
        "    return lam_main * main_sum + lam_inter * inter_sum\n",
        "\n",
        "# ---- NEW: column-wise dropout (drop entire gene/interaction columns)\n",
        "def column_dropout(x, p, col_start):\n",
        "    \"\"\"Drop whole columns from col_start onward with prob p (shared across batch).\"\"\"\n",
        "    if p <= 0.0: return x\n",
        "    keep = 1.0 - float(p)\n",
        "    B, D = x.shape\n",
        "    device = x.device\n",
        "    if col_start >= D: return x\n",
        "    # One Bernoulli per column, scale by 1/keep to keep expectation\n",
        "    m = torch.bernoulli(torch.full((D - col_start,), keep, device=device)) / max(keep, 1e-6)\n",
        "    x = x.clone()\n",
        "    x[:, col_start:] = x[:, col_start:] * m\n",
        "    return x\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_ci(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, times, events = [], [], []\n",
        "    for batch in dataloader:\n",
        "        if len(batch) == 4:\n",
        "            x, t, e, _ = batch\n",
        "        else:\n",
        "            x, t, e = batch\n",
        "        y = torch.clamp(model(x.to(device)), -20, 20)\n",
        "        preds.append(y.cpu().numpy().ravel())\n",
        "        times.append(t.numpy()); events.append(e.numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    times = np.concatenate(times); events = np.concatenate(events)\n",
        "    if np.isnan(preds).any(): return -np.inf\n",
        "    return concordance_index_censored(events.astype(bool), times, preds)[0]\n",
        "\n",
        "def evaluate_ci_grouped(model, X, t, e, group_mask):\n",
        "    \"\"\"Compute C-index within group_mask==1 and group_mask==0.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(torch.tensor(X, dtype=torch.float32, device=device)).cpu().numpy().ravel()\n",
        "    res = {}\n",
        "    for label, mask in [(\"ACT=1\", group_mask.astype(bool)), (\"ACT=0\", ~group_mask.astype(bool))]:\n",
        "        if mask.sum() >= 3:\n",
        "            ci = concordance_index_censored(e[mask].astype(bool), t[mask], preds[mask])[0]\n",
        "            res[label] = float(ci)\n",
        "        else:\n",
        "            res[label] = np.nan\n",
        "    return res\n",
        "\n",
        "# ============================================================\n",
        "# Data loading & preprocessing\n",
        "# ============================================================\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/affyfRMATrain.csv\"\n",
        "VALID_CSV = \"/content/drive/MyDrive/affyfRMAValidation.csv\"\n",
        "TEST_CSV  = \"/content/drive/MyDrive/affyfRMATest.csv\"\n",
        "\n",
        "GENES_CSV = \"/mnt/data/Genes.csv\"\n",
        "if not os.path.exists(GENES_CSV):\n",
        "    if os.path.exists(\"/content/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/Genes.csv\"\n",
        "    elif os.path.exists(\"/content/drive/MyDrive/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/drive/MyDrive/Genes.csv\"\n",
        "print(\"Genes.csv path:\", GENES_CSV)\n",
        "\n",
        "CLINICAL_VARS = [\n",
        "    \"Adjuvant Chemo\",\"Age\",\"IS_MALE\",\n",
        "    \"Stage_IA\",\"Stage_IB\",\"Stage_II\",\"Stage_III\",\n",
        "    \"Histology_Adenocarcinoma\",\"Histology_Large Cell Carcinoma\",\"Histology_Squamous Cell Carcinoma\",\n",
        "    \"Race_African American\",\"Race_Asian\",\"Race_Caucasian\",\"Race_Native Hawaiian or Other Pacific Islander\",\"Race_Unknown\",\n",
        "    \"Smoked?_No\",\"Smoked?_Unknown\",\"Smoked?_Yes\"\n",
        "]\n",
        "\n",
        "def load_genes_list(genes_csv):\n",
        "    g = pd.read_csv(genes_csv)\n",
        "    if \"Prop\" not in g.columns or \"Gene\" not in g.columns:\n",
        "        raise ValueError(f\"Genes.csv must have columns 'Gene' and 'Prop'. Found: {list(g.columns)}\")\n",
        "    g[\"Prop\"] = pd.to_numeric(g[\"Prop\"], errors=\"coerce\").fillna(0)\n",
        "    genes = g.loc[g[\"Prop\"] == 1, \"Gene\"].astype(str).tolist()\n",
        "    print(f\"[Genes] Selected {len(genes)} genes with Prop == 1\")\n",
        "    return genes\n",
        "\n",
        "def coerce_survival_cols(df):\n",
        "    if df[\"OS_STATUS\"].dtype == object:\n",
        "        df[\"OS_STATUS\"] = df[\"OS_STATUS\"].replace({\"DECEASED\":1,\"LIVING\":0,\"Dead\":1,\"Alive\":0}).astype(int)\n",
        "    else:\n",
        "        df[\"OS_STATUS\"] = pd.to_numeric(df[\"OS_STATUS\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"OS_MONTHS\"] = pd.to_numeric(df[\"OS_MONTHS\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def preprocess_split(df, clinical_vars, gene_names):\n",
        "    if \"Adjuvant Chemo\" in df.columns:\n",
        "        df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
        "    for col in [\"Adjuvant Chemo\",\"IS_MALE\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df = coerce_survival_cols(df)\n",
        "    keep_cols = [c for c in clinical_vars if c in df.columns] + [g for g in gene_names if g in df.columns]\n",
        "    missing_clin = [c for c in clinical_vars if c not in df.columns]\n",
        "    if missing_clin:\n",
        "        print(f\"[WARN] Missing clinical columns: {missing_clin}\")\n",
        "    cols = [\"OS_STATUS\",\"OS_MONTHS\"] + keep_cols\n",
        "    return df[cols].copy()\n",
        "\n",
        "# Load CSVs\n",
        "train_raw = pd.read_csv(TRAIN_CSV)\n",
        "valid_raw = pd.read_csv(VALID_CSV)\n",
        "test_raw  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "GENE_LIST = load_genes_list(GENES_CSV)\n",
        "\n",
        "# Reduce to requested columns on each split\n",
        "train_df = preprocess_split(train_raw, CLINICAL_VARS, GENE_LIST)\n",
        "valid_df = preprocess_split(valid_raw, CLINICAL_VARS, GENE_LIST)\n",
        "test_df  = preprocess_split(test_raw,  CLINICAL_VARS, GENE_LIST)\n",
        "\n",
        "# Intersect features that exist everywhere\n",
        "feat_candidates = [c for c in (CLINICAL_VARS + GENE_LIST)\n",
        "                   if c in train_df.columns and c in valid_df.columns and c in test_df.columns]\n",
        "CLIN_FEATS = [c for c in CLINICAL_VARS if c in feat_candidates]\n",
        "GENE_FEATS = [g for g in GENE_LIST if g in feat_candidates]\n",
        "CLIN_FEATS_PRETX = [c for c in CLIN_FEATS if c != \"Adjuvant Chemo\"]  # PS excludes treatment itself\n",
        "print(f\"[Features] Using {len(feat_candidates)} common features → Clinical={len(CLIN_FEATS)}, Genes={len(GENE_FEATS)}\")\n",
        "\n",
        "# Sort Train/Val by event time & status (desc)\n",
        "train_df = train_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "valid_df = valid_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "# ---- Train-only gene ranking (univariate Cox CI on TRAIN) ----\n",
        "def rank_genes_univariate(train_df, gene_cols):\n",
        "    y = Surv.from_arrays(event=train_df[\"OS_STATUS\"].astype(bool).values,\n",
        "                         time=train_df[\"OS_MONTHS\"].values.astype(float))\n",
        "    ranks = []\n",
        "    for g in gene_cols:\n",
        "        Xg = train_df[[g]].to_numpy(dtype=np.float32)\n",
        "        try:\n",
        "            model = CoxPHSurvivalAnalysis(alpha=1e-12)\n",
        "            model.fit(Xg, y)\n",
        "            pred = model.predict(Xg)\n",
        "            ci = concordance_index_censored(y[\"event\"], y[\"time\"], pred)[0]\n",
        "            ranks.append((g, float(ci)))\n",
        "        except Exception:\n",
        "            ranks.append((g, 0.5))\n",
        "    ranks.sort(key=lambda z: z[1], reverse=True)\n",
        "    return [g for g, _ in ranks]\n",
        "\n",
        "GENE_RANK = rank_genes_univariate(train_df, GENE_FEATS)\n",
        "MAX_GENES = len(GENE_RANK)\n",
        "print(f\"[Gene Ranking] Ranked {MAX_GENES} genes on TRAIN\")\n",
        "\n",
        "# ============================================================\n",
        "# Feature construction (main effects + interactions) & IPTW\n",
        "# ============================================================\n",
        "def build_features_with_interactions(df, main_genes, inter_genes, act_col=\"Adjuvant Chemo\"):\n",
        "    base_cols = CLIN_FEATS + list(main_genes)  # keep ACT main effect via CLIN_FEATS\n",
        "    X_base = df[base_cols].to_numpy(dtype=np.float32)\n",
        "    A = df[act_col].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
        "    if len(inter_genes) > 0:\n",
        "        X_int = df[list(inter_genes)].to_numpy(dtype=np.float32) * A\n",
        "        X = np.concatenate([X_base, X_int], axis=1)\n",
        "        names = base_cols + [f\"{g}*ACT\" for g in inter_genes]\n",
        "    else:\n",
        "        X = X_base\n",
        "        names = base_cols\n",
        "    return X, names\n",
        "\n",
        "def compute_iptw(df, covariate_cols, act_col=\"Adjuvant Chemo\",\n",
        "                 ps_clip=(0.05, 0.95), w_clip=(0.1, 10.0),\n",
        "                 ref_prevalence=None, model=None):\n",
        "    A = df[act_col].astype(int).values\n",
        "    X = df[covariate_cols].astype(float).values\n",
        "    if model is None:\n",
        "        model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", class_weight=\"balanced\")\n",
        "        model.fit(X, A)\n",
        "    ps = model.predict_proba(X)[:, 1]\n",
        "    ps = np.clip(ps, ps_clip[0], ps_clip[1])\n",
        "    if ref_prevalence is None:\n",
        "        ref_prevalence = A.mean()\n",
        "    w = np.where(A == 1, ref_prevalence / ps, (1 - ref_prevalence) / (1 - ps))\n",
        "    w = np.clip(w, w_clip[0], w_clip[1])\n",
        "    return w.astype(np.float32), model, float(ref_prevalence)\n",
        "\n",
        "# ============================================================\n",
        "# Training helpers (with input dropout/noise, weights, warm-ups)\n",
        "# ============================================================\n",
        "def _apply_input_dropout(x, p):\n",
        "    if p <= 0.0: return x\n",
        "    keep = 1.0 - p\n",
        "    mask = torch.bernoulli(torch.full_like(x, keep))\n",
        "    return x * mask / max(keep, 1e-6)\n",
        "\n",
        "def train_one_epoch_reg(model, optimizer, dataloader, device,\n",
        "                        l1_lambda=0.0, epoch=0, warmup_epochs=20,\n",
        "                        input_dropout=0.0, noise_std=0.0, grad_clip=5.0,\n",
        "                        col_dropout_p=0.0, col_start=0,\n",
        "                        inter_start_idx=None, lam_main=0.0, lam_inter=0.0):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))\n",
        "    loss_sum, w_sum = 0.0, 0.0\n",
        "    for x, t, e, w in dataloader:\n",
        "        if e.sum().item() == 0:\n",
        "            continue\n",
        "        x, t, e, w = x.to(device), t.to(device), e.to(device), w.to(device)\n",
        "        if input_dropout > 0.0: x = _apply_input_dropout(x, input_dropout)\n",
        "        if col_dropout_p > 0.0 and col_start > 0: x = column_dropout(x, col_dropout_p, col_start)\n",
        "        if noise_std > 0.0: x = x + noise_std * torch.randn_like(x)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = cox_loss(out, e, t, weight=w)\n",
        "        if l1_lambda > 0:\n",
        "            if inter_start_idx is None:\n",
        "                loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "            else:\n",
        "                loss = loss + (l1_lambda * warm) * l1_first_layer_grouped(model, inter_start_idx,\n",
        "                                                                          lam_main=lam_main, lam_inter=lam_inter)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * float(w.sum().item())\n",
        "        w_sum += float(w.sum().item())\n",
        "    return {'avg_loss': (loss_sum / max(w_sum, 1e-9)), 'warm': warm}\n",
        "\n",
        "def full_risk_set_step_reg(model, optimizer, ds, device,\n",
        "                           l1_lambda=0.0, warm=1.0,\n",
        "                           input_dropout=0.0, noise_std=0.0, grad_clip=5.0,\n",
        "                           col_dropout_p=0.0, col_start=0,\n",
        "                           inter_start_idx=None, lam_main=0.0, lam_inter=0.0):\n",
        "    model.train()\n",
        "    X_all = ds.x.to(device); t_all = ds.time.to(device); e_all = ds.event.to(device); w_all = ds.w.to(device)\n",
        "    XX = X_all\n",
        "    if input_dropout > 0.0: XX = _apply_input_dropout(XX, input_dropout)\n",
        "    if col_dropout_p > 0.0 and col_start > 0: XX = column_dropout(XX, col_dropout_p, col_start)\n",
        "    if noise_std > 0.0: XX = XX + noise_std * torch.randn_like(XX)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(XX), -20, 20)\n",
        "    loss_full = cox_loss(out_all, e_all, t_all, weight=w_all)\n",
        "    if l1_lambda > 0:\n",
        "        if inter_start_idx is None:\n",
        "            loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        else:\n",
        "            loss_full = loss_full + (l1_lambda * warm) * l1_first_layer_grouped(model, inter_start_idx,\n",
        "                                                                                lam_main=lam_main, lam_inter=lam_inter)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "def count_params(in_dim, layers):\n",
        "    params, d = 0, in_dim\n",
        "    for h in layers:\n",
        "        params += d*h + h\n",
        "        d = h\n",
        "    params += d*1 + 1\n",
        "    return int(params)\n",
        "\n",
        "# ============================================================\n",
        "# Optuna: Multi-Objective (Val CI ↑, Train–Val GAP ↓, Params ↓)\n",
        "# + Budgets (feature + params), conservative Pareto selection\n",
        "# ============================================================\n",
        "# Smaller, conservative net choices (you can broaden later if needed)\n",
        "ARCH_CHOICES = (\"16\",\"32\",\"64\",\"32-16\",\"64-32\",\"64-64\")\n",
        "def layers_from_arch(arch_str: str):\n",
        "    return [int(x) for x in arch_str.split(\"-\") if x.strip()]\n",
        "\n",
        "def suggest_hparams(trial):\n",
        "    # Static spaces; budgets will clamp per-trial\n",
        "    base_main  = [16, 32, 64, 96, 128, 192, 256, 384, 512, 800, MAX_GENES]\n",
        "    TOPK_MAIN_CHOICES = tuple(sorted({k for k in base_main if k <= MAX_GENES}))\n",
        "    top_k_genes = trial.suggest_categorical(\"top_k_genes\", TOPK_MAIN_CHOICES)\n",
        "\n",
        "    base_inter = [0, 8, 16, 32, 64, 96, 128, 256, 512]\n",
        "    TOPK_INTER_CHOICES = tuple(sorted({k for k in base_inter if k <= MAX_GENES}))\n",
        "    top_k_inter_raw = trial.suggest_categorical(\"top_k_inter\", TOPK_INTER_CHOICES)\n",
        "\n",
        "    arch = trial.suggest_categorical(\"arch\", ARCH_CHOICES)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.10, 0.50)\n",
        "    input_dropout = trial.suggest_float(\"input_dropout\", 0.00, 0.15)  # keep small; use column-dropout primarily\n",
        "    noise_std = trial.suggest_float(\"noise_std\", 0.0, 0.08)\n",
        "\n",
        "    wd = trial.suggest_float(\"wd\", 1e-6, 3e-3, log=True)\n",
        "    use_l1 = trial.suggest_categorical(\"use_l1\", (0, 1))\n",
        "    l1 = 0.0 if use_l1 == 0 else trial.suggest_float(\"l1\", 1e-7, 1e-3, log=True)\n",
        "\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 4e-4, log=True)\n",
        "    sched = trial.suggest_categorical(\"sched\", (\"cosine\", \"cawr\", \"none\"))\n",
        "    if sched == \"cawr\":\n",
        "        cawr_T0 = trial.suggest_int(\"cawr_T0\", 16, 72, step=8)\n",
        "        cawr_Tmult = trial.suggest_categorical(\"cawr_Tmult\", (1, 2, 3))\n",
        "    else:\n",
        "        cawr_T0 = cawr_Tmult = None\n",
        "\n",
        "    epochs = trial.suggest_int(\"epochs\", 96, 256, step=32)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", (32, 64, 128))\n",
        "    grad_clip = trial.suggest_float(\"grad_clip\", 2.0, 9.0)\n",
        "\n",
        "    # Clamp interactions to <= main set (static space; enforced again post-hoc)\n",
        "    top_k_inter = int(min(top_k_inter_raw, top_k_genes))\n",
        "\n",
        "    hp = dict(\n",
        "        arch=arch, top_k_genes=int(top_k_genes), top_k_inter=int(top_k_inter),\n",
        "        dropout=dropout, input_dropout=input_dropout, noise_std=noise_std,\n",
        "        wd=wd, apply_final_wd=1,  # <-- force WD on last layer\n",
        "        use_l1=int(use_l1), l1=float(l1),\n",
        "        lr=lr, sched=sched, cawr_T0=cawr_T0, cawr_Tmult=cawr_Tmult,\n",
        "        epochs=int(epochs), batch_size=int(batch_size), grad_clip=float(grad_clip)\n",
        "    )\n",
        "    return hp\n",
        "\n",
        "# Warm-ups and caps\n",
        "MAX_EPOCHS_CAP = 256\n",
        "WARMUP_EPOCHS_L1 = 20\n",
        "WARMUP_EPOCHS_DROPOUT = 20\n",
        "WARMUP_EPOCHS_WD = 20\n",
        "DROPOUT_START = 0.10\n",
        "WD_START = 0.0\n",
        "\n",
        "# ====== NEW: capacity budgets tied to sample size ======\n",
        "N_EVENTS_TR = int(train_df[\"OS_STATUS\"].sum())\n",
        "FEAT_EVENT_FRACTION = 0.50         # tune 0.35–0.60\n",
        "FEAT_BUDGET = max(24, int(FEAT_EVENT_FRACTION * N_EVENTS_TR))   # total inputs incl. clinical\n",
        "PARAM_BUDGET = 120_000             # cap total parameters; tune 80k–150k\n",
        "print(f\"[Budgets] events(train)={N_EVENTS_TR} → feature budget≤{FEAT_BUDGET} total inputs, param budget≤{PARAM_BUDGET:,}\")\n",
        "\n",
        "# Column-dropout prob (genes + interactions). Keep moderate.\n",
        "COL_DROPOUT_P = 0.30\n",
        "# Grouped L1 strength split (interactions shrunk more than mains)\n",
        "L1_LAM_MAIN = 0.2\n",
        "L1_LAM_INTER = 1.0\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_hparams(trial)\n",
        "    layers = layers_from_arch(hp[\"arch\"])\n",
        "\n",
        "    # --- Trial-specific features (main + interactions) with budgets\n",
        "    MAX_NONCLIN = max(8, FEAT_BUDGET - len(CLIN_FEATS))\n",
        "    k_main = int(min(hp[\"top_k_genes\"], MAX_NONCLIN))\n",
        "    k_int  = int(min(hp[\"top_k_inter\"], k_main, MAX_NONCLIN - k_main))\n",
        "\n",
        "    genes_main  = GENE_RANK[:k_main]\n",
        "    genes_inter = genes_main[:k_int]\n",
        "\n",
        "    Xtr_raw, feat_names = build_features_with_interactions(train_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "    Xva_raw, _          = build_features_with_interactions(valid_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "\n",
        "    # If params exceed cap, shrink interactions first, then mains\n",
        "    in_dim_trial = Xtr_raw.shape[1]\n",
        "    while count_params(in_dim_trial, layers) > PARAM_BUDGET and (k_int > 0 or k_main > 8):\n",
        "        if k_int > 0:\n",
        "            k_int = max(0, k_int // 2)\n",
        "        else:\n",
        "            k_main = max(8, int(k_main * 0.8))\n",
        "        genes_main  = GENE_RANK[:k_main]\n",
        "        genes_inter = genes_main[:min(k_int, k_main, MAX_NONCLIN - k_main)]\n",
        "        Xtr_raw, feat_names = build_features_with_interactions(train_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "        Xva_raw, _          = build_features_with_interactions(valid_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "        in_dim_trial = Xtr_raw.shape[1]\n",
        "\n",
        "    # --- TRAIN-only impute & standardize\n",
        "    med = np.nanmedian(Xtr_raw, axis=0)\n",
        "    Xtr = np.where(np.isnan(Xtr_raw), med, Xtr_raw)\n",
        "    Xva = np.where(np.isnan(Xva_raw), med, Xva_raw)\n",
        "    sc = StandardScaler().fit(Xtr)\n",
        "    Xtr = sc.transform(Xtr).astype(np.float32)\n",
        "    Xva = sc.transform(Xva).astype(np.float32)\n",
        "\n",
        "    ytr_t = train_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "    ytr_e = train_df[\"OS_STATUS\"].to_numpy(int)\n",
        "    yva_t = valid_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "    yva_e = valid_df[\"OS_STATUS\"].to_numpy(int)\n",
        "\n",
        "    # --- IPTW on clinical pre-treatment covariates ONLY (exclude ACT)\n",
        "    w_tr, ps_model, pi_tr = compute_iptw(train_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\")\n",
        "    w_va, _, _ = compute_iptw(valid_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\",\n",
        "                              ref_prevalence=pi_tr, model=ps_model)\n",
        "\n",
        "    # --- Datasets/loaders\n",
        "    bs = hp[\"batch_size\"]\n",
        "    tr_ds = SurvivalDataset(Xtr, ytr_t, ytr_e, weights=w_tr)\n",
        "    va_ds = SurvivalDataset(Xva, yva_t, yva_e, weights=w_va)\n",
        "    tr_sampler = EventBalancedBatchSampler(ytr_e, bs, seed=42)\n",
        "    tr_loader = DataLoader(tr_ds, batch_sampler=tr_sampler, num_workers=0)\n",
        "    tr_eval_loader = DataLoader(tr_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "    va_loader = DataLoader(va_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "\n",
        "    # --- Model/opt/sched\n",
        "    in_dim_trial = Xtr.shape[1]\n",
        "    model = DeepSurvMLP(in_dim_trial, layers, dropout=hp[\"dropout\"]).to(device)\n",
        "    opt = make_optimizer_groups(model, lr=hp[\"lr\"], wd=hp[\"wd\"], apply_final_wd=True)\n",
        "\n",
        "    epochs = int(min(hp[\"epochs\"], MAX_EPOCHS_CAP))\n",
        "    if hp[\"sched\"] == \"cosine\":\n",
        "        sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "        def sched_step(i): sched.step()\n",
        "    elif hp[\"sched\"] == \"cawr\":\n",
        "        sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            opt, T_0=int(hp[\"cawr_T0\"]), T_mult=int(hp[\"cawr_Tmult\"]))\n",
        "        def sched_step(i): sched.step(i+1)\n",
        "    else:\n",
        "        sched = None\n",
        "        def sched_step(i): pass\n",
        "\n",
        "    # --- Early-stopping on Val CI (tighter patience)\n",
        "    PATIENCE = 16\n",
        "    MIN_DELTA = 1e-4\n",
        "    no_improve = 0\n",
        "    best_val_ci = -np.inf\n",
        "    best_tr_ci_at_best = float(\"nan\")\n",
        "    best_epoch = 0\n",
        "\n",
        "    # indices for regularizers/dropout\n",
        "    col_start = len(CLIN_FEATS)\n",
        "    inter_start_idx = len(CLIN_FEATS) + k_main  # main genes end here; interactions start here\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        # warm-ups\n",
        "        frac_d = min(1.0, ep / float(WARMUP_EPOCHS_DROPOUT))\n",
        "        frac_w = min(1.0, ep / float(WARMUP_EPOCHS_WD))\n",
        "        set_dropout_p(model, DROPOUT_START + (hp['dropout'] - DROPOUT_START) * frac_d)\n",
        "        set_weight_decay(opt, WD_START + (hp['wd'] - WD_START) * frac_w)\n",
        "\n",
        "        st = train_one_epoch_reg(\n",
        "            model, opt, tr_loader, device,\n",
        "            l1_lambda=float(hp.get(\"l1\", 0.0)), epoch=ep, warmup_epochs=WARMUP_EPOCHS_L1,\n",
        "            input_dropout=float(hp.get(\"input_dropout\", 0.0)),\n",
        "            noise_std=float(hp.get(\"noise_std\", 0.0)),\n",
        "            grad_clip=float(hp.get(\"grad_clip\", 5.0)),\n",
        "            col_dropout_p=COL_DROPOUT_P, col_start=col_start,\n",
        "            inter_start_idx=inter_start_idx if hp.get(\"use_l1\", 0)==1 else None,\n",
        "            lam_main=L1_LAM_MAIN, lam_inter=L1_LAM_INTER\n",
        "        )\n",
        "        _ = full_risk_set_step_reg(\n",
        "            model, opt, tr_ds, device,\n",
        "            l1_lambda=float(hp.get(\"l1\", 0.0)), warm=st['warm'],\n",
        "            input_dropout=float(hp.get(\"input_dropout\", 0.0)),\n",
        "            noise_std=float(hp.get(\"noise_std\", 0.0)),\n",
        "            grad_clip=float(hp.get(\"grad_clip\", 5.0)),\n",
        "            col_dropout_p=COL_DROPOUT_P, col_start=col_start,\n",
        "            inter_start_idx=inter_start_idx if hp.get(\"use_l1\", 0)==1 else None,\n",
        "            lam_main=L1_LAM_MAIN, lam_inter=L1_LAM_INTER\n",
        "        )\n",
        "        sched_step(ep)\n",
        "\n",
        "        # eval both splits so we can compute the gap at the best Val epoch\n",
        "        va_ci = evaluate_ci(model, va_loader, device)\n",
        "        tr_ci = evaluate_ci(model, tr_eval_loader, device)\n",
        "\n",
        "        if va_ci > best_val_ci + MIN_DELTA:\n",
        "            best_val_ci = va_ci\n",
        "            best_tr_ci_at_best = tr_ci\n",
        "            best_epoch = ep + 1\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE:\n",
        "                break\n",
        "\n",
        "    gap = max(0.0, best_tr_ci_at_best - best_val_ci)\n",
        "    param_cnt = count_params(in_dim_trial, layers)\n",
        "\n",
        "    # annotate & cleanup\n",
        "    trial.set_user_attr(\"best_epoch\", int(best_epoch))\n",
        "    trial.set_user_attr(\"n_features\", int(in_dim_trial))\n",
        "    trial.set_user_attr(\"k_main\", int(k_main))\n",
        "    trial.set_user_attr(\"k_int\", int(k_int))\n",
        "    del model, opt, sched\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Multi-objective return: (Val CI ↑, Gap ↓, Params ↓)\n",
        "    return float(best_val_ci), float(gap), int(param_cnt)\n",
        "\n",
        "# ---- Optuna Study: NSGA-II (no pruner for multi-objective) ----\n",
        "storage = \"sqlite:///deepsurv_optuna.db\"\n",
        "study_name = f\"deepsurv_cox_mo_gap_size_interactions_bounded_M{MAX_GENES}\"\n",
        "\n",
        "sampler = NSGAIISampler(seed=42, population_size=24)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    directions=[\"maximize\", \"minimize\", \"minimize\"],\n",
        "    study_name=study_name,\n",
        "    storage=storage,\n",
        "    load_if_exists=True,\n",
        "    sampler=sampler\n",
        ")\n",
        "\n",
        "# ---- Launch Optuna Dashboard (proxied URL printed) ----\n",
        "try:\n",
        "    from optuna_dashboard import run_server\n",
        "    PORT = portpicker.pick_unused_port()\n",
        "    def _start_dashboard():\n",
        "        run_server(storage, host=\"0.0.0.0\", port=PORT)\n",
        "    t = threading.Thread(target=_start_dashboard, daemon=True)\n",
        "    t.start(); time.sleep(2)\n",
        "    dash_url = output.eval_js(f\"google.colab.kernel.proxyPort({PORT}, {{'cache': false}})\")\n",
        "    print(\"Optuna Dashboard:\", dash_url)\n",
        "except Exception as ex:\n",
        "    print(\"[Optuna Dashboard] Could not start dashboard automatically.\", ex)\n",
        "    print(\"Run locally:  optuna-dashboard sqlite:///deepsurv_optuna.db\")\n",
        "\n",
        "# ---- Optimize ----\n",
        "N_TRIALS = 100\n",
        "print(f\"Starting multi-objective optimization: {N_TRIALS} trials\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)\n",
        "\n",
        "# ---- Choose a robust solution from Pareto front (conservative) ----\n",
        "pareto = study.best_trials\n",
        "best_val = max(tr.values[0] for tr in pareto)\n",
        "TOL = 0.020  # within 2pp absolute C-index of best\n",
        "cands = [tr for tr in pareto if (best_val - tr.values[0]) <= TOL and tr.values[2] <= PARAM_BUDGET]\n",
        "if not cands:\n",
        "    cands = [tr for tr in pareto if tr.values[2] <= PARAM_BUDGET] or pareto\n",
        "cands.sort(key=lambda tr: (tr.values[1], tr.values[2]))  # gap, then params\n",
        "chosen = cands[0]\n",
        "\n",
        "print(\"\\n[Chosen Pareto] Val CI=%.4f | Gap=%.4f | Params=%d\" % (chosen.values[0], chosen.values[1], chosen.values[2]))\n",
        "print(\"[Chosen Params]\", chosen.params)\n",
        "print(\"[Chosen Attrs] k_main=%s k_int=%s n_features=%s\" %\n",
        "      (str(chosen.user_attrs.get(\"k_main\")), str(chosen.user_attrs.get(\"k_int\")), str(chosen.user_attrs.get(\"n_features\"))))\n",
        "\n",
        "# ============================================================\n",
        "# Final training on Train+Val with chosen hyperparams + interactions + IPTW\n",
        "# (re-apply budgets and safe clamping)\n",
        "# ============================================================\n",
        "best_hp = chosen.params\n",
        "# force WD on last layer\n",
        "best_hp[\"apply_final_wd\"] = 1\n",
        "\n",
        "# Recompute k_main/k_int under budgets\n",
        "MAX_NONCLIN = max(8, FEAT_BUDGET - len(CLIN_FEATS))\n",
        "k_main = int(min(best_hp[\"top_k_genes\"], MAX_NONCLIN))\n",
        "k_int  = int(min(best_hp[\"top_k_inter\"], k_main, MAX_NONCLIN - k_main))\n",
        "genes_main  = GENE_RANK[:k_main]\n",
        "genes_inter = genes_main[:k_int]\n",
        "\n",
        "# Assemble Train+Val & Test\n",
        "trainval_df = pd.concat([train_df, valid_df], axis=0, ignore_index=True)\n",
        "trainval_df = trainval_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "X_trv_raw, feat_names = build_features_with_interactions(trainval_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "X_te_raw,  _          = build_features_with_interactions(test_df,      genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "\n",
        "# If params exceed cap for the chosen arch, shrink further (interactions first)\n",
        "in_dim_final = X_trv_raw.shape[1]\n",
        "best_layers = layers_from_arch(best_hp[\"arch\"])\n",
        "while count_params(in_dim_final, best_layers) > PARAM_BUDGET and (k_int > 0 or k_main > 8):\n",
        "    if k_int > 0:\n",
        "        k_int = max(0, k_int // 2)\n",
        "    else:\n",
        "        k_main = max(8, int(k_main * 0.8))\n",
        "    genes_main  = GENE_RANK[:k_main]\n",
        "    genes_inter = genes_main[:min(k_int, k_main, MAX_NONCLIN - k_main)]\n",
        "    X_trv_raw, feat_names = build_features_with_interactions(trainval_df, genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "    X_te_raw,  _          = build_features_with_interactions(test_df,      genes_main, genes_inter, act_col=\"Adjuvant Chemo\")\n",
        "    in_dim_final = X_trv_raw.shape[1]\n",
        "\n",
        "print(f\"[Final] Using features: {len(CLIN_FEATS)} clinical + {k_main} genes (main) + {k_int} interactions\")\n",
        "\n",
        "# Impute + standardize on Train+Val; apply to Test\n",
        "med_trv = np.nanmedian(X_trv_raw, axis=0)\n",
        "X_trv = np.where(np.isnan(X_trv_raw), med_trv, X_trv_raw)\n",
        "X_te  = np.where(np.isnan(X_te_raw),  med_trv, X_te_raw)\n",
        "sc_trv = StandardScaler().fit(X_trv)\n",
        "X_trv = sc_trv.transform(X_trv).astype(np.float32)\n",
        "X_te  = sc_trv.transform(X_te).astype(np.float32)\n",
        "\n",
        "y_trv_t = trainval_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "y_trv_e = trainval_df[\"OS_STATUS\"].to_numpy(int)\n",
        "y_te_t  = test_df[\"OS_MONTHS\"].to_numpy(np.float32)\n",
        "y_te_e  = test_df[\"OS_STATUS\"].to_numpy(int)\n",
        "\n",
        "# IPTW on Train+Val, apply to Test with same prevalence\n",
        "w_trv, ps_model_fin, pi_fin = compute_iptw(trainval_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\")\n",
        "w_te, _, _ = compute_iptw(test_df, covariate_cols=CLIN_FEATS_PRETX, act_col=\"Adjuvant Chemo\",\n",
        "                          ref_prevalence=pi_fin, model=ps_model_fin)\n",
        "\n",
        "# Loaders\n",
        "bs_fin = int(best_hp[\"batch_size\"])\n",
        "ds_trv = SurvivalDataset(X_trv, y_trv_t, y_trv_e, weights=w_trv)\n",
        "ds_te  = SurvivalDataset(X_te,  y_te_t,  y_te_e,  weights=w_te)\n",
        "sam_trv = EventBalancedBatchSampler(y_trv_e, bs_fin, seed=7)\n",
        "dl_trv  = DataLoader(ds_trv, batch_sampler=sam_trv, num_workers=0)\n",
        "dl_trv_eval = DataLoader(ds_trv, batch_size=bs_fin, shuffle=False, num_workers=0)\n",
        "dl_te   = DataLoader(ds_te,  batch_size=bs_fin, shuffle=False, num_workers=0)\n",
        "\n",
        "# Model / optimizer / scheduler\n",
        "model_final = DeepSurvMLP(in_dim_final, best_layers, dropout=float(best_hp[\"dropout\"])).to(device)\n",
        "opt_final = make_optimizer_groups(model_final, lr=float(best_hp[\"lr\"]), wd=float(best_hp[\"wd\"]),\n",
        "                                  apply_final_wd=True)\n",
        "epochs_fin = int(min(best_hp[\"epochs\"], MAX_EPOCHS_CAP))\n",
        "if best_hp[\"sched\"] == \"cosine\":\n",
        "    sched_final = torch.optim.lr_scheduler.CosineAnnealingLR(opt_final, T_max=epochs_fin)\n",
        "    def sched_step(i): sched_final.step()\n",
        "elif best_hp[\"sched\"] == \"cawr\":\n",
        "    sched_final = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        opt_final, T_0=int(best_hp[\"cawr_T0\"]), T_mult=int(best_hp[\"cawr_Tmult\"]))\n",
        "    def sched_step(i): sched_final.step(i+1)\n",
        "else:\n",
        "    sched_final = None\n",
        "    def sched_step(i): pass\n",
        "\n",
        "# Indices for regularizers/dropout in final training\n",
        "col_start_final = len(CLIN_FEATS)\n",
        "inter_start_idx_final = len(CLIN_FEATS) + k_main\n",
        "\n",
        "# Train\n",
        "PATIENCE_FIN = 16\n",
        "MIN_DELTA = 1e-4\n",
        "no_improve = 0\n",
        "best_trv_ci = -np.inf\n",
        "\n",
        "for ep in range(epochs_fin):\n",
        "    frac_d = min(1.0, ep / float(WARMUP_EPOCHS_DROPOUT))\n",
        "    frac_w = min(1.0, ep / float(WARMUP_EPOCHS_WD))\n",
        "    set_dropout_p(model_final, DROPOUT_START + (float(best_hp['dropout']) - DROPOUT_START) * frac_d)\n",
        "    set_weight_decay(opt_final, WD_START + (float(best_hp['wd']) - WD_START) * frac_w)\n",
        "\n",
        "    st = train_one_epoch_reg(\n",
        "        model_final, opt_final, dl_trv, device,\n",
        "        l1_lambda=float(best_hp.get(\"l1\", 0.0)), epoch=ep, warmup_epochs=WARMUP_EPOCHS_L1,\n",
        "        input_dropout=float(best_hp.get(\"input_dropout\", 0.0)),\n",
        "        noise_std=float(best_hp.get(\"noise_std\", 0.0)),\n",
        "        grad_clip=float(best_hp.get(\"grad_clip\", 5.0)),\n",
        "        col_dropout_p=COL_DROPOUT_P, col_start=col_start_final,\n",
        "        inter_start_idx=inter_start_idx_final if best_hp.get(\"use_l1\", 0)==1 else None,\n",
        "        lam_main=L1_LAM_MAIN, lam_inter=L1_LAM_INTER\n",
        "    )\n",
        "    _ = full_risk_set_step_reg(\n",
        "        model_final, opt_final, ds_trv, device,\n",
        "        l1_lambda=float(best_hp.get(\"l1\", 0.0)), warm=st['warm'],\n",
        "        input_dropout=float(best_hp.get(\"input_dropout\", 0.0)),\n",
        "        noise_std=float(best_hp.get(\"noise_std\", 0.0)),\n",
        "        grad_clip=float(best_hp.get(\"grad_clip\", 5.0)),\n",
        "        col_dropout_p=COL_DROPOUT_P, col_start=col_start_final,\n",
        "        inter_start_idx=inter_start_idx_final if best_hp.get(\"use_l1\", 0)==1 else None,\n",
        "        lam_main=L1_LAM_MAIN, lam_inter=L1_LAM_INTER\n",
        "    )\n",
        "    sched_step(ep)\n",
        "\n",
        "    # mild ES on Train+Val CI to avoid late overfit\n",
        "    trv_ci_now = evaluate_ci(model_final, dl_trv_eval, device)\n",
        "    if trv_ci_now > best_trv_ci + MIN_DELTA:\n",
        "        best_trv_ci = trv_ci_now\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= PATIENCE_FIN:\n",
        "            break\n",
        "\n",
        "# Evaluate\n",
        "trainval_ci = evaluate_ci(model_final, dl_trv_eval, device)\n",
        "test_ci = evaluate_ci(model_final, dl_te, device)\n",
        "print(f\"\\n[Final] Train+Val CI: {trainval_ci:.4f}\")\n",
        "print(f\"[Final] Test CI:      {test_ci:.4f}\")\n",
        "\n",
        "# Per-arm C-indices (sanity)\n",
        "act_trv = trainval_df[\"Adjuvant Chemo\"].to_numpy(int)\n",
        "act_te  = test_df[\"Adjuvant Chemo\"].to_numpy(int)\n",
        "trv_grouped = evaluate_ci_grouped(model_final, X_trv, y_trv_t, y_trv_e, act_trv == 1)\n",
        "te_grouped  = evaluate_ci_grouped(model_final, X_te,  y_te_t,  y_te_e,  act_te == 1)\n",
        "print(\"[Train+Val] CI by arm:\", trv_grouped)\n",
        "print(\"[Test]      CI by arm:\", te_grouped)\n",
        "\n",
        "# Save artifacts\n",
        "OUT_DIR = \"/content/drive/MyDrive/deepsurv_results_optuna_interactions_iptw_bounded\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "torch.save(model_final.state_dict(), os.path.join(OUT_DIR, \"deepsurv_best.pt\"))\n",
        "with open(os.path.join(OUT_DIR, \"chosen_params.txt\"), \"w\") as f:\n",
        "    f.write(str(best_hp))\n",
        "with open(os.path.join(OUT_DIR, \"features_used.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(feat_names))\n",
        "print(\"Saved final model and parameters to:\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rFIdgupnCNU-",
        "outputId": "4d7611f5-e5d4-440d-e59c-ad718401e005"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "Genes.csv path: /content/drive/MyDrive/Genes.csv\n",
            "[Genes] Selected 1555 genes with Prop == 1\n",
            "[Features] Using 1573 common features → Clinical=18, Genes=1555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1410719551.py:317: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-1410719551.py:317: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-1410719551.py:317: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gene Ranking] Ranked 1555 genes on TRAIN\n",
            "[Budgets] events(train)=348 → feature budget≤174 total inputs, param budget≤120,000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-20 00:44:13,962] A new study created in RDB with name: deepsurv_cox_mo_gap_size_interactions_bounded_M1555\n",
            "Bottle v0.13.4 server starting up (using WSGIRefServer())...\n",
            "Listening on http://0.0.0.0:43147/\n",
            "Hit Ctrl-C to quit.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna Dashboard: https://43147-gpu-a100-s-ux6s319d4nv1-f.us-central1-1.prod.colab.dev\n",
            "Starting multi-objective optimization: 100 trials\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-20 00:45:26,615] Trial 0 finished with values: [0.47919225666977844, 0.021841253440887443, 7489.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 0, 'arch': '64-64', 'dropout': 0.1798695128633439, 'input_dropout': 0.07713516576204174, 'noise_std': 0.0473931655089634, 'wd': 1.4504865877614253e-06, 'use_l1': 0, 'lr': 1.2712078160994458e-05, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 2, 'epochs': 96, 'batch_size': 128, 'grad_clip': 3.8114598712001184}.\n",
            "[I 2025-10-20 00:45:46,445] Trial 1 finished with values: [0.6058074976661917, 0.027515306609713908, 13313.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 128, 'arch': '64-32', 'dropout': 0.17948627261366898, 'input_dropout': 0.0008283175685403598, 'noise_std': 0.06523691427638674, 'wd': 0.00028696484378591143, 'use_l1': 1, 'l1': 1.9777828512462694e-07, 'lr': 3.7521794620277125e-05, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 3, 'epochs': 224, 'batch_size': 64, 'grad_clip': 2.8371597215681117}.\n",
            "[I 2025-10-20 00:45:59,228] Trial 2 finished with values: [0.39773006436397584, 0.006053746378987701, 4225.0] and parameters: {'top_k_genes': 96, 'top_k_inter': 16, 'arch': '32', 'dropout': 0.17462802355441434, 'input_dropout': 0.13388384977349665, 'noise_std': 0.04314737935325206, 'wd': 0.0006420477456680135, 'use_l1': 0, 'lr': 1.5007549526723109e-05, 'sched': 'none', 'epochs': 256, 'batch_size': 64, 'grad_clip': 3.5547546732951116}.\n",
            "[I 2025-10-20 00:46:25,342] Trial 3 finished with values: [0.6144548715177124, 0.002453686389332632, 11265.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.19505501759695987, 'input_dropout': 0.10923245229177893, 'noise_std': 0.02942265061754026, 'wd': 0.00015798070335448384, 'use_l1': 0, 'lr': 1.395242280080746e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 5.584651408094967}.\n",
            "[I 2025-10-20 00:47:59,155] Trial 4 finished with values: [0.5898393357244632, 0.059509802278229995, 13313.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 512, 'arch': '64-32', 'dropout': 0.41195021834304957, 'input_dropout': 0.09630474692314316, 'noise_std': 0.006731197199603907, 'wd': 3.6475616181513173e-06, 'use_l1': 0, 'lr': 1.0345088931625271e-05, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 2, 'epochs': 128, 'batch_size': 32, 'grad_clip': 7.225439835826169}.\n",
            "[I 2025-10-20 00:48:31,306] Trial 5 finished with values: [0.6117034343831376, 0.0, 6145.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 8, 'arch': '32-16', 'dropout': 0.10618264661154697, 'input_dropout': 0.1392477843881588, 'noise_std': 0.03425473186538515, 'wd': 0.0022970794537196817, 'use_l1': 0, 'lr': 2.9629531839348142e-05, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 128, 'grad_clip': 2.9805881066556683}.\n",
            "[I 2025-10-20 00:49:34,389] Trial 6 finished with values: [0.5912150542917506, 0.06955534021540644, 5633.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.3363333042276043, 'input_dropout': 0.004575037490857414, 'noise_std': 0.0029878550999371534, 'wd': 0.0007249063325290251, 'use_l1': 0, 'lr': 6.865387147801908e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 6.462009310487446}.\n",
            "[I 2025-10-20 00:49:49,365] Trial 7 finished with values: [0.6499287574313369, 0.0, 1345.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 128, 'arch': '16', 'dropout': 0.24238907146050465, 'input_dropout': 0.11367691656965537, 'noise_std': 0.0011514790903804696, 'wd': 2.532786865696868e-06, 'use_l1': 0, 'lr': 0.00023469248087216023, 'sched': 'cosine', 'epochs': 160, 'batch_size': 32, 'grad_clip': 4.789533140781614}.\n",
            "[I 2025-10-20 00:50:26,021] Trial 8 finished with values: [0.6267872058173243, 0.08517657422512448, 5633.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 512, 'arch': '32', 'dropout': 0.1072887302606199, 'input_dropout': 0.014166444113389258, 'noise_std': 0.05464054187330855, 'wd': 1.7682007182911976e-06, 'use_l1': 1, 'l1': 1.2390468928182026e-07, 'lr': 0.0002017564232754397, 'sched': 'none', 'epochs': 192, 'batch_size': 32, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 00:51:29,552] Trial 9 finished with values: [0.6098855205620793, 0.0753012039589871, 13633.0] and parameters: {'top_k_genes': 96, 'top_k_inter': 32, 'arch': '64-64', 'dropout': 0.3293751552493145, 'input_dropout': 0.09477558182546988, 'noise_std': 0.03587564175826558, 'wd': 1.0459945130878265e-05, 'use_l1': 1, 'l1': 0.00010221111307208344, 'lr': 0.00018542023020327265, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 4.456405087864551}.\n",
            "[I 2025-10-20 00:52:57,179] Trial 10 finished with values: [0.6295877757578735, 0.09563153342512709, 13313.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 8, 'arch': '64-32', 'dropout': 0.29705004171634364, 'input_dropout': 0.0017030467151128604, 'noise_std': 0.0374928513595301, 'wd': 1.5695433550097714e-06, 'use_l1': 0, 'lr': 0.00010966646660982659, 'sched': 'none', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.74255777608428}.\n",
            "[I 2025-10-20 00:55:45,978] Trial 11 finished with values: [0.5961283348892055, 0.07484970371364408, 5633.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 256, 'arch': '32', 'dropout': 0.3334624447403488, 'input_dropout': 0.011660195544747726, 'noise_std': 0.07795158461329332, 'wd': 0.0026864226549000703, 'use_l1': 0, 'lr': 3.132433141922241e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 64, 'grad_clip': 6.293906371550529}.\n",
            "[I 2025-10-20 00:57:18,777] Trial 12 finished with values: [0.626590674593426, 0.07078759010707769, 2817.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 0, 'arch': '16', 'dropout': 0.19893924069727908, 'input_dropout': 0.06758162029651403, 'noise_std': 0.010332753212119598, 'wd': 0.0020765916576049462, 'use_l1': 0, 'lr': 0.00011915294822072587, 'sched': 'cosine', 'epochs': 224, 'batch_size': 64, 'grad_clip': 7.965270502229781}.\n",
            "[I 2025-10-20 00:57:55,770] Trial 13 finished with values: [0.6086572004127155, 0.04260933436365277, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 96, 'arch': '32-16', 'dropout': 0.2968465172318153, 'input_dropout': 0.03873665824484375, 'noise_std': 0.03673086049906091, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 3.2731605907940136}.\n",
            "[I 2025-10-20 00:58:59,243] Trial 14 finished with values: [0.621579128384022, 0.05557780444613325, 5377.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.3993038761534634, 'input_dropout': 0.0276781529034566, 'noise_std': 0.016747945866936825, 'wd': 1.9416744813672684e-05, 'use_l1': 1, 'l1': 2.9898855088295226e-06, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 224, 'batch_size': 32, 'grad_clip': 2.7502040793784324}.\n",
            "[I 2025-10-20 01:00:02,500] Trial 15 finished with values: [0.5697440180808726, 0.08081873233059189, 6145.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 16, 'arch': '32-16', 'dropout': 0.4946060995171919, 'input_dropout': 0.09086622290353306, 'noise_std': 0.01897814333887956, 'wd': 2.2589675841640098e-06, 'use_l1': 1, 'l1': 4.392638796750216e-07, 'lr': 1.9901763677872792e-05, 'sched': 'none', 'epochs': 96, 'batch_size': 128, 'grad_clip': 2.7842723151763664}.\n",
            "[I 2025-10-20 01:01:40,614] Trial 16 finished with values: [0.6342553923254557, 0.0435388255595337, 833.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 0, 'arch': '16', 'dropout': 0.1537658707755897, 'input_dropout': 0.00950624557091516, 'noise_std': 0.07919681859119562, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 0.0001235401844034605, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 01:02:51,270] Trial 17 finished with values: [0.5654694639610868, 0.049294491196211765, 11521.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '64-32', 'dropout': 0.19288507805872693, 'input_dropout': 0.13493618599118526, 'noise_std': 0.030711297709856915, 'wd': 7.76245698246254e-05, 'use_l1': 0, 'lr': 1.5391384019344328e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 32, 'grad_clip': 8.257248081356703}.\n",
            "[I 2025-10-20 01:03:31,349] Trial 18 finished with values: [0.6179924335478799, 0.061225977894434735, 5633.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 8, 'arch': '32', 'dropout': 0.4633802245334514, 'input_dropout': 0.08750876921491807, 'noise_std': 0.03206811334109119, 'wd': 4.040640380225651e-05, 'use_l1': 0, 'lr': 8.6931136931741e-05, 'sched': 'cawr', 'cawr_T0': 64, 'cawr_Tmult': 1, 'epochs': 256, 'batch_size': 32, 'grad_clip': 6.247005324679991}.\n",
            "[I 2025-10-20 01:04:26,724] Trial 19 finished with values: [0.6546946396108682, 0.08363057526502748, 5633.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 96, 'arch': '32', 'dropout': 0.1898419291759928, 'input_dropout': 0.067865927419904, 'noise_std': 0.01126856163038399, 'wd': 4.1050546182692745e-06, 'use_l1': 0, 'lr': 0.0002921716117553693, 'sched': 'none', 'epochs': 96, 'batch_size': 128, 'grad_clip': 3.040639094271791}.\n",
            "[I 2025-10-20 01:05:33,348] Trial 20 finished with values: [0.5931312337247581, 0.053469266128389314, 11521.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 64, 'arch': '64-32', 'dropout': 0.20829786925742153, 'input_dropout': 0.09659993148585236, 'noise_std': 0.03269873368784771, 'wd': 1.2253818315092576e-06, 'use_l1': 1, 'l1': 4.3221095215763176e-05, 'lr': 1.1051199156532657e-05, 'sched': 'none', 'epochs': 96, 'batch_size': 64, 'grad_clip': 6.569222754963123}.\n",
            "[I 2025-10-20 01:06:11,465] Trial 21 finished with values: [0.5945069522920454, 0.061203953040645076, 15425.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 256, 'arch': '64-64', 'dropout': 0.2637646608764962, 'input_dropout': 0.08280707712779703, 'noise_std': 0.034890122330825335, 'wd': 1.056557534237434e-05, 'use_l1': 0, 'lr': 1.6767553980333663e-05, 'sched': 'none', 'epochs': 224, 'batch_size': 32, 'grad_clip': 5.791439502626068}.\n",
            "[I 2025-10-20 01:07:53,466] Trial 22 finished with values: [0.6067410209797082, 0.07891674032101836, 15425.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.2675082099568292, 'input_dropout': 0.057470779237424614, 'noise_std': 0.04151341642262701, 'wd': 1.456486157131174e-06, 'use_l1': 1, 'l1': 2.1438513631935168e-07, 'lr': 9.253070332775052e-05, 'sched': 'cawr', 'cawr_T0': 32, 'cawr_Tmult': 1, 'epochs': 160, 'batch_size': 64, 'grad_clip': 3.5045826501352972}.\n",
            "[I 2025-10-20 01:08:06,467] Trial 23 finished with values: [0.6074780130693264, 0.007430023926456419, 13313.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 64, 'arch': '64-32', 'dropout': 0.17405317153544786, 'input_dropout': 0.08128514210675372, 'noise_std': 0.06983566687011267, 'wd': 0.0003515885439914866, 'use_l1': 0, 'lr': 0.00012854896578041262, 'sched': 'cosine', 'epochs': 128, 'batch_size': 32, 'grad_clip': 6.939026207609819}.\n",
            "[I 2025-10-20 01:08:52,354] Trial 24 finished with values: [0.6236918390409276, 0.07196298520693745, 11265.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 512, 'arch': '64', 'dropout': 0.3993038761534634, 'input_dropout': 0.0276781529034566, 'noise_std': 0.016747945866936825, 'wd': 0.001648923814695195, 'use_l1': 0, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 128, 'batch_size': 32, 'grad_clip': 7.225439835826169}.\n",
            "[I 2025-10-20 01:09:23,797] Trial 25 finished with values: [0.6213825971601238, 0.06381521057928763, 5633.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 512, 'arch': '32', 'dropout': 0.1072887302606199, 'input_dropout': 0.014166444113389258, 'noise_std': 0.05464054187330855, 'wd': 4.245045706082199e-06, 'use_l1': 0, 'lr': 8.6931136931741e-05, 'sched': 'cawr', 'cawr_T0': 48, 'cawr_Tmult': 1, 'epochs': 192, 'batch_size': 32, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 01:10:10,578] Trial 26 finished with values: [0.6147988011595342, 0.08437494491285025, 5633.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 0, 'arch': '32', 'dropout': 0.19893924069727908, 'input_dropout': 0.06758162029651403, 'noise_std': 0.03206811334109119, 'wd': 4.040640380225651e-05, 'use_l1': 0, 'lr': 0.00011915294822072587, 'sched': 'none', 'epochs': 256, 'batch_size': 64, 'grad_clip': 8.609865423794037}.\n",
            "[I 2025-10-20 01:11:43,588] Trial 27 finished with values: [0.5805532353952734, 0.03751074802423193, 6145.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 16, 'arch': '32-16', 'dropout': 0.17462802355441434, 'input_dropout': 0.13388384977349665, 'noise_std': 0.04314737935325206, 'wd': 0.0022970794537196817, 'use_l1': 0, 'lr': 2.9629531839348142e-05, 'sched': 'cawr', 'cawr_T0': 40, 'cawr_Tmult': 1, 'epochs': 256, 'batch_size': 128, 'grad_clip': 3.5547546732951116}.\n",
            "[I 2025-10-20 01:11:55,389] Trial 28 finished with values: [0.6275241979069425, 0.0, 15425.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.1798695128633439, 'input_dropout': 0.057470779237424614, 'noise_std': 0.04151341642262701, 'wd': 0.0007624995883125161, 'use_l1': 1, 'l1': 1.4061463985176633e-07, 'lr': 9.253070332775052e-05, 'sched': 'cosine', 'epochs': 160, 'batch_size': 128, 'grad_clip': 3.8114598712001184}.\n",
            "[I 2025-10-20 01:12:37,290] Trial 29 finished with values: [0.6135704810101705, 0.023265703481082034, 5633.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.17948627261366898, 'input_dropout': 0.0008283175685403598, 'noise_std': 0.03206811334109119, 'wd': 0.00028696484378591143, 'use_l1': 1, 'l1': 2.2882309479728945e-06, 'lr': 3.7521794620277125e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 64, 'grad_clip': 2.8371597215681117}.\n",
            "[I 2025-10-20 01:13:43,877] Trial 30 finished with values: [0.6187785584434727, 0.07797904602971417, 15425.0] and parameters: {'top_k_genes': 96, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.17462802355441434, 'input_dropout': 0.057470779237424614, 'noise_std': 0.04314737935325206, 'wd': 0.0006420477456680135, 'use_l1': 0, 'lr': 9.253070332775052e-05, 'sched': 'none', 'epochs': 256, 'batch_size': 64, 'grad_clip': 3.5547546732951116}.\n",
            "[I 2025-10-20 01:14:46,243] Trial 31 finished with values: [0.6105242470397484, 0.08673764386813343, 11265.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.19505501759695987, 'input_dropout': 0.09086622290353306, 'noise_std': 0.01897814333887956, 'wd': 0.00015798070335448384, 'use_l1': 0, 'lr': 7.316399563921526e-05, 'sched': 'none', 'epochs': 96, 'batch_size': 128, 'grad_clip': 2.7842723151763664}.\n",
            "[I 2025-10-20 01:15:11,530] Trial 32 finished with values: [0.6182380975777526, 0.04331920543189538, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 96, 'arch': '32-16', 'dropout': 0.41628926593558546, 'input_dropout': 0.03873665824484375, 'noise_std': 0.03673086049906091, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 3.2731605907940136}.\n",
            "[I 2025-10-20 01:17:29,005] Trial 33 finished with values: [0.5580995430649044, 0.07596029523093994, 5633.0] and parameters: {'top_k_genes': 96, 'top_k_inter': 256, 'arch': '32', 'dropout': 0.3993038761534634, 'input_dropout': 0.13388384977349665, 'noise_std': 0.016747945866936825, 'wd': 1.9416744813672684e-05, 'use_l1': 1, 'l1': 1.8836925134820866e-06, 'lr': 1.5007549526723109e-05, 'sched': 'cosine', 'epochs': 224, 'batch_size': 32, 'grad_clip': 2.7502040793784324}.\n",
            "[I 2025-10-20 01:20:11,897] Trial 34 finished with values: [0.6108681766815702, 0.07209636256133511, 15425.0] and parameters: {'top_k_genes': 800, 'top_k_inter': 96, 'arch': '64-64', 'dropout': 0.1898419291759928, 'input_dropout': 0.08280707712779703, 'noise_std': 0.01126856163038399, 'wd': 0.0029507873063226698, 'use_l1': 0, 'lr': 1.6767553980333663e-05, 'sched': 'none', 'epochs': 224, 'batch_size': 32, 'grad_clip': 3.040639094271791}.\n",
            "[I 2025-10-20 01:22:33,197] Trial 35 finished with values: [0.6157323244730507, 0.07277936555149866, 5377.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.3993038761534634, 'input_dropout': 0.0276781529034566, 'noise_std': 0.016747945866936825, 'wd': 1.9416744813672684e-05, 'use_l1': 1, 'l1': 2.528408617241556e-06, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 224, 'batch_size': 32, 'grad_clip': 2.7502040793784324}.\n",
            "[I 2025-10-20 01:22:47,715] Trial 36 finished with values: [0.632044416056601, 0.0, 13633.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 128, 'arch': '64-64', 'dropout': 0.2968465172318153, 'input_dropout': 0.03873665824484375, 'noise_std': 0.030711297709856915, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 1.5391384019344328e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 32, 'grad_clip': 8.257248081356703}.\n",
            "[I 2025-10-20 01:23:59,028] Trial 37 finished with values: [0.6209895347123274, 0.08354294742983703, 13313.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 8, 'arch': '64-32', 'dropout': 0.29705004171634364, 'input_dropout': 0.0017030467151128604, 'noise_std': 0.010332753212119598, 'wd': 1.5695433550097714e-06, 'use_l1': 0, 'lr': 0.00010966646660982659, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 7.965270502229781}.\n",
            "[I 2025-10-20 01:24:12,079] Trial 38 finished with values: [0.6086572004127155, 0.0, 13313.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 64, 'arch': '64-32', 'dropout': 0.17405317153544786, 'input_dropout': 0.00950624557091516, 'noise_std': 0.07919681859119562, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 0.00012854896578041262, 'sched': 'cosine', 'epochs': 128, 'batch_size': 32, 'grad_clip': 6.939026207609819}.\n",
            "[I 2025-10-20 01:24:40,191] Trial 39 finished with values: [0.5877757578735322, 0.0, 5633.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.3363333042276043, 'input_dropout': 0.004575037490857414, 'noise_std': 0.03269873368784771, 'wd': 1.2253818315092576e-06, 'use_l1': 0, 'lr': 1.1051199156532657e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 6.462009310487446}.\n",
            "[I 2025-10-20 01:25:43,014] Trial 40 finished with values: [0.6191716208912691, 0.07550233853306854, 15425.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 128, 'arch': '64-64', 'dropout': 0.4073093655458073, 'input_dropout': 0.09477558182546988, 'noise_std': 0.0334213425733867, 'wd': 1.0459945130878265e-05, 'use_l1': 0, 'lr': 0.00018542023020327265, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 6.462009310487446}.\n",
            "[I 2025-10-20 01:26:37,431] Trial 41 finished with values: [0.6276715963248661, 0.0, 9537.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.1798695128633439, 'input_dropout': 0.07713516576204174, 'noise_std': 0.0473931655089634, 'wd': 1.456486157131174e-06, 'use_l1': 1, 'l1': 4.846497679219077e-06, 'lr': 1.2712078160994458e-05, 'sched': 'cawr', 'cawr_T0': 56, 'cawr_Tmult': 3, 'epochs': 160, 'batch_size': 64, 'grad_clip': 3.8114598712001184}.\n",
            "[I 2025-10-20 01:27:06,863] Trial 42 finished with values: [0.6067901537856827, 0.07349778822691888, 15425.0] and parameters: {'top_k_genes': 96, 'top_k_inter': 96, 'arch': '64-64', 'dropout': 0.2968465172318153, 'input_dropout': 0.03873665824484375, 'noise_std': 0.03587564175826558, 'wd': 1.0459945130878265e-05, 'use_l1': 1, 'l1': 5.141531903173456e-06, 'lr': 0.00018542023020327265, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 3.2731605907940136}.\n",
            "[I 2025-10-20 01:27:19,281] Trial 43 finished with values: [0.5993711000835258, 0.022729945618125025, 2817.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 512, 'arch': '16', 'dropout': 0.28696060449994787, 'input_dropout': 0.014166444113389258, 'noise_std': 0.0473931655089634, 'wd': 1.4504865877614253e-06, 'use_l1': 1, 'l1': 2.8421405887320788e-05, 'lr': 0.0002017564232754397, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 1, 'epochs': 192, 'batch_size': 128, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 01:27:31,757] Trial 44 finished with values: [0.599027170441704, 0.007984073483306986, 15425.0] and parameters: {'top_k_genes': 96, 'top_k_inter': 256, 'arch': '64-64', 'dropout': 0.3993038761534634, 'input_dropout': 0.0276781529034566, 'noise_std': 0.016747945866936825, 'wd': 0.00018150983887714342, 'use_l1': 1, 'l1': 2.697411343452543e-05, 'lr': 0.00018542023020327265, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 2.7502040793784324}.\n",
            "[I 2025-10-20 01:27:46,982] Trial 45 finished with values: [0.6021225372181005, 0.016027341143578178, 15425.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 8, 'arch': '64-64', 'dropout': 0.2637646608764962, 'input_dropout': 0.08280707712779703, 'noise_std': 0.041015080881320685, 'wd': 1.056557534237434e-05, 'use_l1': 0, 'lr': 2.9629531839348142e-05, 'sched': 'none', 'epochs': 192, 'batch_size': 32, 'grad_clip': 2.9805881066556683}.\n",
            "[I 2025-10-20 01:29:07,565] Trial 46 finished with values: [0.6195646833390654, 0.07571885309424409, 13313.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 256, 'arch': '64-32', 'dropout': 0.3993038761534634, 'input_dropout': 0.0276781529034566, 'noise_std': 0.016747945866936825, 'wd': 1.5695433550097714e-06, 'use_l1': 1, 'l1': 8.36910943718647e-07, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 224, 'batch_size': 32, 'grad_clip': 2.7502040793784324}.\n",
            "[I 2025-10-20 01:29:19,697] Trial 47 finished with values: [0.6067410209797082, 0.0, 6145.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 128, 'arch': '32-16', 'dropout': 0.19893924069727908, 'input_dropout': 0.1392477843881588, 'noise_std': 0.010332753212119598, 'wd': 0.0022970794537196817, 'use_l1': 0, 'lr': 0.00011915294822072587, 'sched': 'cosine', 'epochs': 192, 'batch_size': 64, 'grad_clip': 7.965270502229781}.\n",
            "[I 2025-10-20 01:30:56,966] Trial 48 finished with values: [0.6027121308897951, 0.08439199040494638, 5633.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.17948627261366898, 'input_dropout': 0.0008283175685403598, 'noise_std': 0.03206811334109119, 'wd': 0.00028696484378591143, 'use_l1': 0, 'lr': 3.7521794620277125e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 64, 'grad_clip': 6.247005324679991}.\n",
            "[I 2025-10-20 01:33:29,081] Trial 49 finished with values: [0.5976514518744166, 0.07036736743033312, 5633.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 256, 'arch': '32', 'dropout': 0.19505501759695987, 'input_dropout': 0.08750876921491807, 'noise_std': 0.02942265061754026, 'wd': 0.00015798070335448384, 'use_l1': 0, 'lr': 1.395242280080746e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 32, 'grad_clip': 6.247005324679991}.\n",
            "[I 2025-10-20 01:34:48,460] Trial 50 finished with values: [0.6083132707708937, 0.08462616498246034, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 96, 'arch': '32-16', 'dropout': 0.19893924069727908, 'input_dropout': 0.06758162029651403, 'noise_std': 0.03673086049906091, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 224, 'batch_size': 32, 'grad_clip': 7.965270502229781}.\n",
            "[I 2025-10-20 01:35:01,620] Trial 51 finished with values: [0.6178450351299563, 0.0, 13313.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 512, 'arch': '64-32', 'dropout': 0.17405317153544786, 'input_dropout': 0.00950624557091516, 'noise_std': 0.07919681859119562, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 128, 'batch_size': 32, 'grad_clip': 7.225439835826169}.\n",
            "[I 2025-10-20 01:36:51,266] Trial 52 finished with values: [0.603154326143566, 0.0595944228381633, 9537.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.1798695128633439, 'input_dropout': 0.07713516576204174, 'noise_std': 0.0473931655089634, 'wd': 1.456486157131174e-06, 'use_l1': 1, 'l1': 0.00018337177967709965, 'lr': 1.2712078160994458e-05, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 3, 'epochs': 160, 'batch_size': 64, 'grad_clip': 3.8114598712001184}.\n",
            "[I 2025-10-20 01:38:16,275] Trial 53 finished with values: [0.5811919618729425, 0.05687445985459871, 5633.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 96, 'arch': '32', 'dropout': 0.1898419291759928, 'input_dropout': 0.1392477843881588, 'noise_std': 0.03425473186538515, 'wd': 4.1050546182692745e-06, 'use_l1': 0, 'lr': 2.9629531839348142e-05, 'sched': 'cawr', 'cawr_T0': 48, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 128, 'grad_clip': 3.040639094271791}.\n",
            "[I 2025-10-20 01:38:30,082] Trial 54 finished with values: [0.5869405001719649, 0.01711706606411878, 13633.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.3993038761534634, 'input_dropout': 0.0276781529034566, 'noise_std': 0.030711297709856915, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 1.5391384019344328e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 32, 'grad_clip': 8.257248081356703}.\n",
            "[I 2025-10-20 01:39:15,102] Trial 55 finished with values: [0.6279663931607135, 0.10010684474610998, 6145.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 8, 'arch': '32-16', 'dropout': 0.10618264661154697, 'input_dropout': 0.014166444113389258, 'noise_std': 0.03425473186538515, 'wd': 1.7682007182911976e-06, 'use_l1': 0, 'lr': 0.0002017564232754397, 'sched': 'none', 'epochs': 192, 'batch_size': 32, 'grad_clip': 2.9805881066556683}.\n",
            "[I 2025-10-20 01:39:38,796] Trial 56 finished with values: [0.5920011791873434, 0.052964545959924836, 5377.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 512, 'arch': '64', 'dropout': 0.1798695128633439, 'input_dropout': 0.0276781529034566, 'noise_std': 0.0473931655089634, 'wd': 1.456486157131174e-06, 'use_l1': 1, 'l1': 0.00028087330790064357, 'lr': 5.508183035487179e-05, 'sched': 'cawr', 'cawr_T0': 64, 'cawr_Tmult': 1, 'epochs': 160, 'batch_size': 64, 'grad_clip': 2.7502040793784324}.\n",
            "[I 2025-10-20 01:41:13,669] Trial 57 finished with values: [0.6108681766815702, 0.051221120808634946, 1345.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 512, 'arch': '16', 'dropout': 0.28696060449994787, 'input_dropout': 0.014166444113389258, 'noise_std': 0.0473931655089634, 'wd': 1.4504865877614253e-06, 'use_l1': 0, 'lr': 0.0001235401844034605, 'sched': 'cawr', 'cawr_T0': 40, 'cawr_Tmult': 3, 'epochs': 160, 'batch_size': 128, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 01:42:01,284] Trial 58 finished with values: [0.5723971896034983, 0.04589677059666464, 5633.0] and parameters: {'top_k_genes': 96, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.17948627261366898, 'input_dropout': 0.0008283175685403598, 'noise_std': 0.03206811334109119, 'wd': 0.0006420477456680135, 'use_l1': 0, 'lr': 1.5007549526723109e-05, 'sched': 'none', 'epochs': 256, 'batch_size': 64, 'grad_clip': 3.5547546732951116}.\n",
            "[I 2025-10-20 01:42:14,904] Trial 59 finished with values: [0.5955878740234855, 0.0, 9537.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.1798695128633439, 'input_dropout': 0.052171381185328956, 'noise_std': 0.0473931655089634, 'wd': 1.456486157131174e-06, 'use_l1': 1, 'l1': 5.1530530114124875e-05, 'lr': 1.2712078160994458e-05, 'sched': 'cawr', 'cawr_T0': 48, 'cawr_Tmult': 2, 'epochs': 96, 'batch_size': 64, 'grad_clip': 3.8114598712001184}.\n",
            "[I 2025-10-20 01:42:50,759] Trial 60 finished with values: [0.6170589102343634, 0.09041062476921091, 11265.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 8, 'arch': '64', 'dropout': 0.1072887302606199, 'input_dropout': 0.014166444113389258, 'noise_std': 0.05464054187330855, 'wd': 0.0022970794537196817, 'use_l1': 0, 'lr': 8.6931136931741e-05, 'sched': 'none', 'epochs': 192, 'batch_size': 32, 'grad_clip': 2.9805881066556683}.\n",
            "[I 2025-10-20 01:44:30,355] Trial 61 finished with values: [0.6364663685943104, 0.05984236553590638, 1665.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 0, 'arch': '32', 'dropout': 0.1537658707755897, 'input_dropout': 0.004575037490857414, 'noise_std': 0.07919681859119562, 'wd': 1.2253818315092576e-06, 'use_l1': 0, 'lr': 0.0001235401844034605, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 01:45:34,970] Trial 62 finished with values: [0.604136982263057, 0.09241004106157602, 13633.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.2968465172318153, 'input_dropout': 0.014166444113389258, 'noise_std': 0.030711297709856915, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 8.6931136931741e-05, 'sched': 'cosine', 'epochs': 192, 'batch_size': 32, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 01:46:01,081] Trial 63 finished with values: [0.6221195892497421, 0.07937609106590793, 5633.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 96, 'arch': '32', 'dropout': 0.1537658707755897, 'input_dropout': 0.00950624557091516, 'noise_std': 0.07919681859119562, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 0.0002921716117553693, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 01:47:03,057] Trial 64 finished with values: [0.6058566304721663, 0.04228997834010062, 15425.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.4633802245334514, 'input_dropout': 0.08750876921491807, 'noise_std': 0.03206811334109119, 'wd': 1.456486157131174e-06, 'use_l1': 1, 'l1': 2.1901578520649185e-06, 'lr': 1.2712078160994458e-05, 'sched': 'none', 'epochs': 160, 'batch_size': 64, 'grad_clip': 6.247005324679991}.\n",
            "[I 2025-10-20 01:47:14,763] Trial 65 finished with values: [0.6177467695180072, 0.0, 2817.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 32, 'arch': '16', 'dropout': 0.28696060449994787, 'input_dropout': 0.057470779237424614, 'noise_std': 0.0473931655089634, 'wd': 1.4504865877614253e-06, 'use_l1': 1, 'l1': 5.643426708406559e-05, 'lr': 0.0002017564232754397, 'sched': 'cosine', 'epochs': 160, 'batch_size': 128, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 01:47:27,133] Trial 66 finished with values: [0.5814376259028153, 0.02694239548779609, 15425.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.1798695128633439, 'input_dropout': 0.057470779237424614, 'noise_std': 0.04151341642262701, 'wd': 0.0007624995883125161, 'use_l1': 1, 'l1': 6.802810205786724e-05, 'lr': 8.6931136931741e-05, 'sched': 'cawr', 'cawr_T0': 16, 'cawr_Tmult': 3, 'epochs': 160, 'batch_size': 128, 'grad_clip': 6.247005324679991}.\n",
            "[I 2025-10-20 01:48:38,063] Trial 67 finished with values: [0.6222669876676657, 0.07087748762506973, 5633.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.1072887302606199, 'input_dropout': 0.004575037490857414, 'noise_std': 0.03269873368784771, 'wd': 4.245045706082199e-06, 'use_l1': 0, 'lr': 8.6931136931741e-05, 'sched': 'cawr', 'cawr_T0': 16, 'cawr_Tmult': 2, 'epochs': 192, 'batch_size': 64, 'grad_clip': 6.462009310487446}.\n",
            "[I 2025-10-20 01:49:37,658] Trial 68 finished with values: [0.6129808873384759, 0.07914393186652491, 11265.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.1898419291759928, 'input_dropout': 0.0276781529034566, 'noise_std': 0.01126856163038399, 'wd': 1.9416744813672684e-05, 'use_l1': 1, 'l1': 9.967983879709285e-05, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 3.040639094271791}.\n",
            "[I 2025-10-20 01:49:59,050] Trial 69 finished with values: [0.5932294993367071, 0.04592307778863858, 1345.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 256, 'arch': '16', 'dropout': 0.3993038761534634, 'input_dropout': 0.00950624557091516, 'noise_std': 0.07919681859119562, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 5.508183035487179e-05, 'sched': 'none', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 01:50:43,962] Trial 70 finished with values: [0.6302265022355427, 0.08649752008605904, 5633.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 512, 'arch': '32', 'dropout': 0.2968465172318153, 'input_dropout': 0.014166444113389258, 'noise_std': 0.05464054187330855, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 0.0002017564232754397, 'sched': 'none', 'epochs': 96, 'batch_size': 32, 'grad_clip': 8.257248081356703}.\n",
            "[I 2025-10-20 01:51:14,562] Trial 71 finished with values: [0.6391686729229106, 0.08532468545849625, 5633.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 96, 'arch': '32', 'dropout': 0.1072887302606199, 'input_dropout': 0.012357156842376404, 'noise_std': 0.01126856163038399, 'wd': 4.1050546182692745e-06, 'use_l1': 0, 'lr': 0.0002921716117553693, 'sched': 'none', 'epochs': 96, 'batch_size': 32, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 01:52:08,203] Trial 72 finished with values: [0.6208912691003783, 0.07385473124320152, 6145.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 96, 'arch': '32-16', 'dropout': 0.4633802245334514, 'input_dropout': 0.03873665824484375, 'noise_std': 0.05443994416597705, 'wd': 4.040640380225651e-05, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 256, 'batch_size': 32, 'grad_clip': 6.5815784999679465}.\n",
            "[I 2025-10-20 01:52:21,992] Trial 73 finished with values: [0.6371050950719795, 0.0, 13313.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 512, 'arch': '64-32', 'dropout': 0.19893924069727908, 'input_dropout': 0.06758162029651403, 'noise_std': 0.010332753212119598, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 128, 'batch_size': 32, 'grad_clip': 7.225439835826169}.\n",
            "[I 2025-10-20 01:52:36,500] Trial 74 finished with values: [0.6156831916670761, 0.0, 15425.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.1798695128633439, 'input_dropout': 0.0409889290497314, 'noise_std': 0.04151341642262701, 'wd': 0.0007624995883125161, 'use_l1': 1, 'l1': 0.0006359958019698479, 'lr': 9.253070332775052e-05, 'sched': 'cosine', 'epochs': 160, 'batch_size': 128, 'grad_clip': 3.8114598712001184}.\n",
            "[I 2025-10-20 01:52:50,086] Trial 75 finished with values: [0.5959809364712819, 0.026602229228373964, 5633.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 512, 'arch': '32', 'dropout': 0.4633802245334514, 'input_dropout': 0.00950624557091516, 'noise_std': 0.03206811334109119, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 8.6931136931741e-05, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 2, 'epochs': 256, 'batch_size': 32, 'grad_clip': 6.247005324679991}.\n",
            "[I 2025-10-20 01:53:55,131] Trial 76 finished with values: [0.5429666388247433, 0.06238766395769868, 5633.0] and parameters: {'top_k_genes': 1555, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.3363333042276043, 'input_dropout': 0.1392477843881588, 'noise_std': 0.03425473186538515, 'wd': 0.0022970794537196817, 'use_l1': 0, 'lr': 1.1051199156532657e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 6.469681633648599}.\n",
            "[I 2025-10-20 01:55:32,180] Trial 77 finished with values: [0.6109664422935194, 0.09374337134216415, 11265.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 0, 'arch': '64', 'dropout': 0.19893924069727908, 'input_dropout': 0.10923245229177893, 'noise_std': 0.02942265061754026, 'wd': 0.0020765916576049462, 'use_l1': 0, 'lr': 4.3348101115788925e-05, 'sched': 'cosine', 'epochs': 224, 'batch_size': 64, 'grad_clip': 5.584651408094967}.\n",
            "[I 2025-10-20 01:56:56,156] Trial 78 finished with values: [0.6284085884144843, 0.08433655852128286, 5633.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 8, 'arch': '32', 'dropout': 0.4633802245334514, 'input_dropout': 0.08750876921491807, 'noise_std': 0.03206811334109119, 'wd': 4.040640380225651e-05, 'use_l1': 0, 'lr': 8.6931136931741e-05, 'sched': 'cosine', 'epochs': 160, 'batch_size': 32, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 01:59:20,792] Trial 79 finished with values: [0.6062988257259372, 0.0, 833.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 0, 'arch': '16', 'dropout': 0.19505501759695987, 'input_dropout': 0.00950624557091516, 'noise_std': 0.02942265061754026, 'wd': 0.00015798070335448384, 'use_l1': 1, 'l1': 4.338464271474827e-06, 'lr': 1.395242280080746e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 01:59:34,241] Trial 80 finished with values: [0.6017786075762787, 0.0, 11265.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 256, 'arch': '64', 'dropout': 0.19505501759695987, 'input_dropout': 0.10923245229177893, 'noise_std': 0.02942265061754026, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:01:16,085] Trial 81 finished with values: [0.6095415909202575, 0.06670651800561234, 833.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 0, 'arch': '16', 'dropout': 0.3993038761534634, 'input_dropout': 0.00950624557091516, 'noise_std': 0.07919681859119562, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 0.0001235401844034605, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:01:52,434] Trial 82 finished with values: [0.5940156242322999, 0.059129516053536335, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 96, 'arch': '32-16', 'dropout': 0.28696060449994787, 'input_dropout': 0.014166444113389258, 'noise_std': 0.03673086049906091, 'wd': 1.4504865877614253e-06, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 192, 'batch_size': 128, 'grad_clip': 3.9742420079991456}.\n",
            "[I 2025-10-20 02:02:13,894] Trial 83 finished with values: [0.5974057878445438, 0.004800880927934403, 15425.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 512, 'arch': '64-64', 'dropout': 0.2968465172318153, 'input_dropout': 0.03873665824484375, 'noise_std': 0.0473931655089634, 'wd': 1.4504865877614253e-06, 'use_l1': 0, 'lr': 1.5391384019344328e-05, 'sched': 'cosine', 'epochs': 160, 'batch_size': 128, 'grad_clip': 8.257248081356703}.\n",
            "[I 2025-10-20 02:02:37,859] Trial 84 finished with values: [0.6231022453692331, 0.03463134731140949, 1345.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 128, 'arch': '16', 'dropout': 0.1537658707755897, 'input_dropout': 0.11367691656965537, 'noise_std': 0.0011514790903804696, 'wd': 2.532786865696868e-06, 'use_l1': 1, 'l1': 0.00011068683358440696, 'lr': 0.00023469248087216023, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:03:13,126] Trial 85 finished with values: [0.6238883702648258, 0.05945299840180762, 3201.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 128, 'arch': '32-16', 'dropout': 0.1537658707755897, 'input_dropout': 0.004575037490857414, 'noise_std': 0.0011514790903804696, 'wd': 2.532786865696868e-06, 'use_l1': 0, 'lr': 0.00023469248087216023, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:03:46,674] Trial 86 finished with values: [0.5872352970078121, 0.0779462184006322, 4737.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 96, 'arch': '32', 'dropout': 0.17462802355441434, 'input_dropout': 0.13388384977349665, 'noise_std': 0.03673086049906091, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 96, 'batch_size': 64, 'grad_clip': 3.5547546732951116}.\n",
            "[I 2025-10-20 02:04:14,514] Trial 87 finished with values: [0.6352871812509212, 0.06048401678956583, 2817.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 128, 'arch': '16', 'dropout': 0.28696060449994787, 'input_dropout': 0.11367691656965537, 'noise_std': 0.0011514790903804696, 'wd': 2.532786865696868e-06, 'use_l1': 0, 'lr': 0.00023469248087216023, 'sched': 'cosine', 'epochs': 160, 'batch_size': 32, 'grad_clip': 4.789533140781614}.\n",
            "[I 2025-10-20 02:04:28,750] Trial 88 finished with values: [0.6121947624428831, 0.0, 5633.0] and parameters: {'top_k_genes': 128, 'top_k_inter': 512, 'arch': '32', 'dropout': 0.28696060449994787, 'input_dropout': 0.014166444113389258, 'noise_std': 0.05464054187330855, 'wd': 1.4504865877614253e-06, 'use_l1': 1, 'l1': 3.1676936120939894e-07, 'lr': 0.0002017564232754397, 'sched': 'cawr', 'cawr_T0': 56, 'cawr_Tmult': 3, 'epochs': 96, 'batch_size': 128, 'grad_clip': 8.257248081356703}.\n",
            "[I 2025-10-20 02:05:13,446] Trial 89 finished with values: [0.6194664177271164, 0.07096266907110804, 5633.0] and parameters: {'top_k_genes': 192, 'top_k_inter': 32, 'arch': '32', 'dropout': 0.2919755338868494, 'input_dropout': 0.0008283175685403598, 'noise_std': 0.010332753212119598, 'wd': 0.0020765916576049462, 'use_l1': 0, 'lr': 0.00011915294822072587, 'sched': 'cosine', 'epochs': 256, 'batch_size': 64, 'grad_clip': 7.965270502229781}.\n",
            "[I 2025-10-20 02:07:08,708] Trial 90 finished with values: [0.5463568024369871, 0.0640514479107488, 1345.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 512, 'arch': '16', 'dropout': 0.3400082192477215, 'input_dropout': 0.014166444113389258, 'noise_std': 0.0473931655089634, 'wd': 1.4504865877614253e-06, 'use_l1': 0, 'lr': 2.9326548183542606e-05, 'sched': 'cosine', 'epochs': 160, 'batch_size': 32, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:08:11,389] Trial 91 finished with values: [0.5866457033361175, 0.05336028160178874, 5249.0] and parameters: {'top_k_genes': 64, 'top_k_inter': 96, 'arch': '32-16', 'dropout': 0.41628926593558546, 'input_dropout': 0.03873665824484375, 'noise_std': 0.03673086049906091, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 3.2731605907940136}.\n",
            "[I 2025-10-20 02:09:15,099] Trial 92 finished with values: [0.5914115855156488, 0.053659430205896474, 13313.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 0, 'arch': '64-32', 'dropout': 0.41628926593558546, 'input_dropout': 0.00950624557091516, 'noise_std': 0.07919681859119562, 'wd': 2.3996635797682574e-05, 'use_l1': 0, 'lr': 1.6316184206483987e-05, 'sched': 'cosine', 'epochs': 96, 'batch_size': 32, 'grad_clip': 3.2731605907940136}.\n",
            "[I 2025-10-20 02:10:39,104] Trial 93 finished with values: [0.5937699602024271, 0.061680489499157276, 6145.0] and parameters: {'top_k_genes': 512, 'top_k_inter': 128, 'arch': '32-16', 'dropout': 0.10618264661154697, 'input_dropout': 0.004575037490857414, 'noise_std': 0.03269873368784771, 'wd': 0.0022970794537196817, 'use_l1': 0, 'lr': 2.9629531839348142e-05, 'sched': 'none', 'epochs': 128, 'batch_size': 128, 'grad_clip': 2.9805881066556683}.\n",
            "[I 2025-10-20 02:10:54,064] Trial 94 finished with values: [0.6198103473689383, 0.03341791705448505, 5633.0] and parameters: {'top_k_genes': 256, 'top_k_inter': 96, 'arch': '32', 'dropout': 0.1072887302606199, 'input_dropout': 0.014166444113389258, 'noise_std': 0.01126856163038399, 'wd': 4.1050546182692745e-06, 'use_l1': 0, 'lr': 0.0001235401844034605, 'sched': 'cawr', 'cawr_T0': 24, 'cawr_Tmult': 3, 'epochs': 96, 'batch_size': 128, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:11:31,731] Trial 95 finished with values: [0.6469316562668894, 0.05500181117338587, 2689.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 96, 'arch': '32', 'dropout': 0.1537658707755897, 'input_dropout': 0.00950624557091516, 'noise_std': 0.01126856163038399, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 0.0002921716117553693, 'sched': 'cosine', 'epochs': 160, 'batch_size': 32, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:12:59,064] Trial 96 finished with values: [0.6017294747703041, 0.04963680897116873, 7425.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 512, 'arch': '64-32', 'dropout': 0.19893924069727908, 'input_dropout': 0.06758162029651403, 'noise_std': 0.07919681859119562, 'wd': 1.320880126246403e-05, 'use_l1': 0, 'lr': 5.508183035487179e-05, 'sched': 'cosine', 'epochs': 128, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:13:11,822] Trial 97 finished with values: [0.6332727362059647, 0.0, 2689.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 128, 'arch': '32', 'dropout': 0.1537658707755897, 'input_dropout': 0.004575037490857414, 'noise_std': 0.07919681859119562, 'wd': 2.532786865696868e-06, 'use_l1': 0, 'lr': 0.00023469248087216023, 'sched': 'cosine', 'epochs': 160, 'batch_size': 64, 'grad_clip': 8.755188374665588}.\n",
            "[I 2025-10-20 02:13:24,792] Trial 98 finished with values: [0.6383334152213432, 0.0, 6145.0] and parameters: {'top_k_genes': 384, 'top_k_inter': 96, 'arch': '32-16', 'dropout': 0.19505501759695987, 'input_dropout': 0.10923245229177893, 'noise_std': 0.02942265061754026, 'wd': 0.002556772566793941, 'use_l1': 0, 'lr': 0.00010345373321678518, 'sched': 'cosine', 'epochs': 96, 'batch_size': 128, 'grad_clip': 6.505043866375675}.\n",
            "[I 2025-10-20 02:15:43,916] Trial 99 finished with values: [0.6014346779344568, 0.05651503750391218, 1345.0] and parameters: {'top_k_genes': 32, 'top_k_inter': 128, 'arch': '16', 'dropout': 0.24238907146050465, 'input_dropout': 0.07896039914041698, 'noise_std': 0.05852716174042655, 'wd': 2.532786865696868e-06, 'use_l1': 1, 'l1': 2.1208987108343724e-07, 'lr': 3.7521794620277125e-05, 'sched': 'cosine', 'epochs': 256, 'batch_size': 32, 'grad_clip': 4.789533140781614}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Chosen Pareto] Val CI=0.6499 | Gap=0.0000 | Params=1345\n",
            "[Chosen Params] {'top_k_genes': 32, 'top_k_inter': 128, 'arch': '16', 'dropout': 0.24238907146050465, 'input_dropout': 0.11367691656965537, 'noise_std': 0.0011514790903804696, 'wd': 2.532786865696868e-06, 'use_l1': 0, 'lr': 0.00023469248087216023, 'sched': 'cosine', 'epochs': 160, 'batch_size': 32, 'grad_clip': 4.789533140781614}\n",
            "[Chosen Attrs] k_main=32 k_int=32 n_features=82\n",
            "[Final] Using features: 18 clinical + 32 genes (main) + 32 interactions\n",
            "\n",
            "[Final] Train+Val CI: 0.6798\n",
            "[Final] Test CI:      0.6027\n",
            "[Train+Val] CI by arm: {'ACT=1': 0.5908468386213194, 'ACT=0': 0.705384074670558}\n",
            "[Test]      CI by arm: {'ACT=1': 0.5933682373472949, 'ACT=0': 0.617439006785044}\n",
            "Saved final model and parameters to: /content/drive/MyDrive/deepsurv_results_optuna_interactions_iptw_bounded\n"
          ]
        }
      ]
    }
  ]
}