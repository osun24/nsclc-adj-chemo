{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1EY87_PKv0dTOF6mK_Kx8YPzRYVzJ6o-8",
      "authorship_tag": "ABX9TyMc20qT20EZPcHyhLweBrn8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osun24/nsclc-adj-chemo/blob/main/TorchSurv_DeepSurv_with_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torchsurv scikit-survival\n",
        "\n",
        "# Import required packages\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "\n",
        "# (Optional) Mount Google Drive if you plan to load/save files there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81TEvMle6mqs",
        "outputId": "7fb1bbdb-0bc7-4238-bcb4-4376726dd3f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsurv\n",
            "  Downloading torchsurv-0.1.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: scikit-survival in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (1.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchsurv) (2.0.2)\n",
            "Collecting torchmetrics (from torchsurv)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.0.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.5.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.14.1)\n",
            "Requirement already satisfied: osqp<1.0.0,>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (0.6.7.post3)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.8,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from scikit-survival) (1.6.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.12/dist-packages (from osqp<1.0.0,>=0.6.3->scikit-survival) (0.1.7.post5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->scikit-survival) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8,>=1.6.1->scikit-survival) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchsurv) (3.4.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->torchsurv) (25.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->torchsurv)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchsurv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchsurv) (3.0.3)\n",
            "Downloading torchsurv-0.1.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, torchsurv\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2 torchsurv-0.1.5\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab-ready single cell: DeepSurv + Optuna HPO + Dashboard\n",
        "# - Keeps ONLY requested clinical vars + genes with Prop==1\n",
        "# - Sorts Train/Val by OS_MONTHS/OS_STATUS (desc)\n",
        "# - Standardizes using TRAIN-only (applies to VAL); after HPO\n",
        "#   restandardizes on TRAIN+VAL and evaluates TEST C-index\n",
        "# - Optuna + Successive Halving pruner\n",
        "# - Optuna Dashboard (proxied URL printed)  [fixed run_server args]\n",
        "# - Encodes architectures as strings to avoid Optuna warning\n",
        "# ============================================================\n",
        "\n",
        "# ---------- Installs (Colab) ----------\n",
        "!pip -q install optuna optuna-dashboard scikit-survival portpicker\n",
        "\n",
        "# ---------- (Optional) Mount Google Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, math, copy, warnings, random, gc, time, threading\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "# Cox loss: Prefer torchsurv if available; fallback to Efron implementation\n",
        "try:\n",
        "    from torchsurv.loss.cox import neg_partial_log_likelihood\n",
        "    _HAS_TORCHSURV = True\n",
        "except Exception:\n",
        "    _HAS_TORCHSURV = False\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sksurv.metrics import concordance_index_censored\n",
        "\n",
        "import optuna\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ties in event time detected; using efron's method to handle ties.\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# Cox loss fallback (Efron) if torchsurv isn't available\n",
        "# ============================================================\n",
        "def _cox_negloglik_efron(pred, event, time):\n",
        "    eta = pred.reshape(-1)\n",
        "    e = event.to(torch.float32).reshape(-1)\n",
        "    t = time.reshape(-1)\n",
        "\n",
        "    order = torch.argsort(t, descending=True)\n",
        "    t = t[order]; e = e[order]; eta = eta[order]\n",
        "    exp_eta = torch.exp(eta)\n",
        "    cum_exp = torch.cumsum(exp_eta, dim=0)\n",
        "\n",
        "    uniq_mask = torch.ones_like(t, dtype=torch.bool)\n",
        "    uniq_mask[1:] = t[1:] != t[:-1]\n",
        "    idxs = torch.nonzero(uniq_mask, as_tuple=False).reshape(-1)\n",
        "    idxs = torch.cat([idxs, torch.tensor([len(t)], device=t.device)])\n",
        "\n",
        "    nll = torch.tensor(0.0, device=t.device)\n",
        "    for k in range(len(idxs)-1):\n",
        "        start, end = idxs[k].item(), idxs[k+1].item()\n",
        "        e_slice = e[start:end]\n",
        "        d = int(e_slice.sum().item())\n",
        "        if d == 0: continue\n",
        "        eta_events = eta[start:end][e_slice.bool()]\n",
        "        exp_events = exp_eta[start:end][e_slice.bool()]\n",
        "        s_eta = eta_events.sum()\n",
        "        risk_sum = cum_exp[end-1]\n",
        "        s_exp = exp_events.sum()\n",
        "        eps = 1e-12\n",
        "        log_terms = 0.0\n",
        "        for j in range(d):\n",
        "            log_terms = log_terms + torch.log(risk_sum - (j / d) * s_exp + eps)\n",
        "        nll = nll - (s_eta - log_terms)\n",
        "    return nll / t.numel()\n",
        "\n",
        "def cox_negloglik(pred, event, time):\n",
        "    if _HAS_TORCHSURV:\n",
        "        return neg_partial_log_likelihood(pred, event, time, reduction='mean')\n",
        "    return _cox_negloglik_efron(pred, event, time)\n",
        "\n",
        "# ============================================================\n",
        "# Model, Dataset, Sampler, Utilities\n",
        "# ============================================================\n",
        "class DeepSurvMLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_layers, dropout=0.0, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        layers, d = [], in_features\n",
        "        for units in hidden_layers:\n",
        "            layers += [nn.Linear(d, units), activation]\n",
        "            if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "            d = units\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class SurvivalDataset(Dataset):\n",
        "    def __init__(self, features, time_vals, events):\n",
        "        self.x = torch.tensor(features, dtype=torch.float32)\n",
        "        self.time = torch.tensor(time_vals, dtype=torch.float32)\n",
        "        self.event = torch.tensor(events.astype(bool), dtype=torch.bool)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.time[idx], self.event[idx]\n",
        "\n",
        "class EventBalancedBatchSampler(Sampler):\n",
        "    def __init__(self, events_numpy, batch_size, seed=0):\n",
        "        events = np.asarray(events_numpy).astype(bool)\n",
        "        self.pos_idx = np.where(events)[0]\n",
        "        self.neg_idx = np.where(~events)[0]\n",
        "        assert len(self.pos_idx) > 0, \"No events in training set — cannot balance batches.\"\n",
        "        self.bs = int(batch_size)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "    def __iter__(self):\n",
        "        pos = self.rng.permutation(self.pos_idx)\n",
        "        neg = self.rng.permutation(self.neg_idx)\n",
        "        n_total = len(pos) + len(neg)\n",
        "        n_batches = math.ceil(n_total / self.bs)\n",
        "        pi = ni = 0\n",
        "        for _ in range(n_batches):\n",
        "            take_pos = 1 if pi < len(pos) else 0\n",
        "            avail_neg = max(0, len(neg) - ni)\n",
        "            take_neg = min(self.bs - take_pos, avail_neg)\n",
        "            need = self.bs - (take_pos + take_neg)\n",
        "            extra_pos = min(need, max(0, len(pos) - (pi + take_pos)))\n",
        "            take_pos += extra_pos\n",
        "            batch = np.concatenate([pos[pi:pi+take_pos], neg[ni:ni+take_neg]])\n",
        "            pi += take_pos; ni += take_neg\n",
        "            if batch.size == 0: break\n",
        "            self.rng.shuffle(batch)\n",
        "            yield batch.tolist()\n",
        "    def __len__(self):\n",
        "        return math.ceil((len(self.pos_idx) + len(self.neg_idx)) / self.bs)\n",
        "\n",
        "def make_optimizer(model, lr, wd):\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight):\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "    param_groups = [{'params': decay, 'weight_decay': wd},\n",
        "                    {'params': no_decay, 'weight_decay': 0.0}]\n",
        "    return optim.AdamW(param_groups, lr=lr)\n",
        "\n",
        "def set_dropout_p(model, p):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout): m.p = float(p)\n",
        "\n",
        "def set_weight_decay(optimizer, wd):\n",
        "    for g in optimizer.param_groups: g['weight_decay'] = float(wd)\n",
        "\n",
        "def l1_penalty_first_layer(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear): return m.weight.abs().sum()\n",
        "    return torch.tensor(0.0, device=next(model.parameters()).device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_ci(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, times, events = [], [], []\n",
        "    for x, t, e in dataloader:\n",
        "        y = torch.clamp(model(x.to(device)), -20, 20)\n",
        "        preds.append(y.cpu().numpy().ravel())\n",
        "        times.append(t.numpy()); events.append(e.numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    times = np.concatenate(times); events = np.concatenate(events)\n",
        "    if np.isnan(preds).any(): return -np.inf\n",
        "    return concordance_index_censored(events.astype(bool), times, preds)[0]\n",
        "\n",
        "def train_one_epoch(model, optimizer, dataloader, device, l1_lambda=0.0, epoch=0, warmup_epochs=20):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))\n",
        "    loss_sum, n_seen = 0.0, 0\n",
        "    for x, t, e in dataloader:\n",
        "        if e.sum().item() == 0:  # safety (shouldn't happen with balanced sampler)\n",
        "            continue\n",
        "        x, t, e = x.to(device), t.to(device), e.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = cox_negloglik(out, e, t)\n",
        "        if l1_lambda > 0:\n",
        "            loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        n_seen += x.size(0)\n",
        "    return {'avg_loss': loss_sum / max(n_seen, 1), 'warm': warm}\n",
        "\n",
        "def full_risk_set_step(model, optimizer, ds, device, l1_lambda=0.0, warm=1.0):\n",
        "    model.train()\n",
        "    X_all = ds.x.to(device); t_all = ds.time.to(device); e_all = ds.event.to(device)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(X_all), -20, 20)\n",
        "    loss_full = cox_negloglik(out_all, e_all, t_all)\n",
        "    if l1_lambda > 0:\n",
        "        loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "# ============================================================\n",
        "# Data loading & preprocessing\n",
        "# ============================================================\n",
        "# Default CSV paths (adjust if needed)\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/affyfRMATrain.csv\"\n",
        "VALID_CSV = \"/content/drive/MyDrive/affyfRMAValidation.csv\"\n",
        "TEST_CSV  = \"/content/drive/MyDrive/affyfRMATest.csv\"\n",
        "\n",
        "# Genes list path (uploaded or Drive)\n",
        "GENES_CSV = \"/mnt/data/Genes.csv\"\n",
        "if not os.path.exists(GENES_CSV):\n",
        "    if os.path.exists(\"/content/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/Genes.csv\"\n",
        "    elif os.path.exists(\"/content/drive/MyDrive/Genes.csv\"):\n",
        "        GENES_CSV = \"/content/drive/MyDrive/Genes.csv\"\n",
        "print(\"Genes.csv path:\", GENES_CSV)\n",
        "\n",
        "# Clinical variables to KEEP (exact names)\n",
        "CLINICAL_VARS = [\n",
        "    \"Adjuvant Chemo\",\"Age\",\"IS_MALE\",\n",
        "    \"Stage_IA\",\"Stage_IB\",\"Stage_II\",\"Stage_III\",\n",
        "    \"Histology_Adenocarcinoma\",\"Histology_Large Cell Carcinoma\",\"Histology_Squamous Cell Carcinoma\",\n",
        "    \"Race_African American\",\"Race_Asian\",\"Race_Caucasian\",\"Race_Native Hawaiian or Other Pacific Islander\",\"Race_Unknown\",\n",
        "    \"Smoked?_No\",\"Smoked?_Unknown\",\"Smoked?_Yes\"\n",
        "]\n",
        "\n",
        "def load_genes_list(genes_csv):\n",
        "    g = pd.read_csv(genes_csv)\n",
        "    if not {\"Gene\",\"Prop\"}.issubset(set(g.columns)):\n",
        "        raise ValueError(f\"Genes.csv must contain 'Gene' and 'Prop' columns. Found: {list(g.columns)}\")\n",
        "    genes = g.loc[g[\"Prop\"] == 1, \"Gene\"].astype(str).tolist()\n",
        "    print(f\"[Genes] Selected {len(genes)} genes with Prop == 1\")\n",
        "    return genes\n",
        "\n",
        "def coerce_survival_cols(df):\n",
        "    # Map to integers {0,1}\n",
        "    if df[\"OS_STATUS\"].dtype == object:\n",
        "        df[\"OS_STATUS\"] = df[\"OS_STATUS\"].replace({\"DECEASED\":1,\"LIVING\":0,\"Dead\":1,\"Alive\":0}).astype(int)\n",
        "    else:\n",
        "        df[\"OS_STATUS\"] = pd.to_numeric(df[\"OS_STATUS\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df[\"OS_MONTHS\"] = pd.to_numeric(df[\"OS_MONTHS\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "    return df\n",
        "\n",
        "def preprocess_split(df, clinical_vars, gene_names):\n",
        "    if \"Adjuvant Chemo\" in df.columns:\n",
        "        df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
        "    for col in [\"Adjuvant Chemo\",\"IS_MALE\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
        "    df = coerce_survival_cols(df)\n",
        "    keep_cols = [c for c in clinical_vars if c in df.columns] + [g for g in gene_names if g in df.columns]\n",
        "    missing_clin = [c for c in clinical_vars if c not in df.columns]\n",
        "    if missing_clin:\n",
        "        print(f\"[WARN] Missing clinical columns: {missing_clin}\")\n",
        "    if len(keep_cols) == 0:\n",
        "        raise ValueError(\"No feature columns found after filtering clinical+genes.\")\n",
        "    cols = [\"OS_STATUS\",\"OS_MONTHS\"] + keep_cols\n",
        "    return df[cols].copy()\n",
        "\n",
        "# Load CSVs\n",
        "train_raw = pd.read_csv(TRAIN_CSV)\n",
        "valid_raw = pd.read_csv(VALID_CSV)\n",
        "test_raw  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# Load genes (Prop==1)\n",
        "GENE_LIST = load_genes_list(GENES_CSV)\n",
        "\n",
        "# Reduce to requested columns per split\n",
        "train_df = preprocess_split(train_raw, CLINICAL_VARS, GENE_LIST)\n",
        "valid_df = preprocess_split(valid_raw, CLINICAL_VARS, GENE_LIST)\n",
        "test_df  = preprocess_split(test_raw,  CLINICAL_VARS, GENE_LIST)\n",
        "\n",
        "# Ensure consistent columns across splits (intersection)\n",
        "feat_candidates = [c for c in (CLINICAL_VARS + GENE_LIST)\n",
        "                   if c in train_df.columns and c in valid_df.columns and c in test_df.columns]\n",
        "if len(feat_candidates) == 0:\n",
        "    raise ValueError(\"After filtering, no common features across train/val/test.\")\n",
        "print(f\"[Features] Using {len(feat_candidates)} common features.\")\n",
        "\n",
        "# Sort Train/Val by event time & status (descending)\n",
        "train_df = train_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "valid_df = valid_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "# Build arrays & TRAIN-only standardization (apply to VAL)\n",
        "X_train = train_df[feat_candidates].values.astype(np.float32)\n",
        "X_valid = valid_df[feat_candidates].values.astype(np.float32)\n",
        "\n",
        "train_medians = np.nanmedian(X_train, axis=0)\n",
        "X_train = np.where(np.isnan(X_train), train_medians, X_train)\n",
        "X_valid = np.where(np.isnan(X_valid), train_medians, X_valid)\n",
        "\n",
        "scaler_tv = StandardScaler().fit(X_train)\n",
        "X_train = scaler_tv.transform(X_train).astype(np.float32)\n",
        "X_valid = scaler_tv.transform(X_valid).astype(np.float32)\n",
        "\n",
        "ytr_time = train_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "ytr_event = train_df[\"OS_STATUS\"].values.astype(int)\n",
        "yva_time = valid_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "yva_event = valid_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_ds = SurvivalDataset(X_train, ytr_time, ytr_event)\n",
        "valid_ds = SurvivalDataset(X_valid, yva_time, yva_event)\n",
        "\n",
        "train_sampler = EventBalancedBatchSampler(ytr_event, BATCH_SIZE, seed=42)\n",
        "train_loader  = DataLoader(train_ds, batch_sampler=train_sampler, num_workers=0)\n",
        "train_eval_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "valid_loader      = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "in_dim = X_train.shape[1]\n",
        "print(\"Input dim:\", in_dim)\n",
        "\n",
        "# ============================================================\n",
        "# Optuna Objective & Study  (arch encoded as strings -> parsed)\n",
        "# ============================================================# ============================================================\n",
        "# Optuna Objective & Study — Expanded Space to Reduce Overfitting\n",
        "#   * Architectures include smaller, narrower and bottlenecks\n",
        "#   * Stronger regularizers: higher dropout, input dropout, L1/L2\n",
        "#   * Optionally apply WD to final layer\n",
        "#   * Batch size, grad clip, scheduler, epochs per trial\n",
        "#   * Input Gaussian noise\n",
        "# ============================================================\n",
        "\n",
        "# Wider set of architectures (strings -> parsed lists)\n",
        "ARCH_CHOICES = (\n",
        "    # very small\n",
        "    \"16\", \"32\",\n",
        "    # small / medium singles\n",
        "    \"64\", \"128\", \"256\",\n",
        "    # conservative big single\n",
        "    \"512\",\n",
        "    # 2-layer bottlenecks and symmetric\n",
        "    \"32-16\", \"32-32\", \"64-32\", \"64-64\",\n",
        "    \"128-64\", \"128-128\", \"256-128\", \"256-256\",\n",
        "    \"512-256\", \"512-512\",\n",
        "    # 3-layer (narrowing)\n",
        "    \"64-32-16\", \"128-64-32\", \"256-128-64\"\n",
        ")\n",
        "\n",
        "def layers_from_arch(arch_str: str):\n",
        "    return [int(x) for x in arch_str.split(\"-\") if x.strip()]\n",
        "\n",
        "def suggest_hparams(trial):\n",
        "    arch = trial.suggest_categorical(\"arch\", ARCH_CHOICES)\n",
        "\n",
        "    # Regularization knobs\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.10, 0.70)             # ↑ upper bound\n",
        "    input_dropout = trial.suggest_float(\"input_dropout\", 0.00, 0.30) # feature dropout before first layer\n",
        "\n",
        "    # L2 (weight decay): allow much stronger; optionally apply to final layer too\n",
        "    wd = trial.suggest_float(\"wd\", 1e-6, 1e-1, log=True)\n",
        "    apply_final_wd = trial.suggest_categorical(\"apply_final_wd\", (0, 1))\n",
        "\n",
        "    # L1: allow disabling OR stronger values\n",
        "    use_l1 = trial.suggest_categorical(\"use_l1\", (0, 1))\n",
        "    l1 = 0.0 if use_l1 == 0 else trial.suggest_float(\"l1\", 1e-8, 3e-3, log=True)\n",
        "\n",
        "    # Optim & schedule\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
        "    sched = trial.suggest_categorical(\"sched\", (\"cosine\", \"cawr\", \"none\"))\n",
        "    if sched == \"cawr\":\n",
        "        cawr_T0 = trial.suggest_int(\"cawr_T0\", 16, 80, step=8)\n",
        "        cawr_Tmult = trial.suggest_categorical(\"cawr_Tmult\", (1, 2, 3))\n",
        "    else:\n",
        "        cawr_T0, cawr_Tmult = None, None\n",
        "\n",
        "    # Training controls\n",
        "    epochs = trial.suggest_int(\"epochs\", 64, 512, step=32)           # per-trial budget\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", (32, 64, 128))\n",
        "    grad_clip = trial.suggest_float(\"grad_clip\", 1.0, 10.0)          # stability & regularization\n",
        "\n",
        "    # Data regularization\n",
        "    noise_std = trial.suggest_float(\"noise_std\", 0.0, 0.10)          # Gaussian feature noise\n",
        "\n",
        "    return {\n",
        "        \"arch\": arch,\n",
        "        \"dropout\": dropout,\n",
        "        \"input_dropout\": input_dropout,\n",
        "        \"lr\": lr,\n",
        "        \"wd\": wd,\n",
        "        \"apply_final_wd\": apply_final_wd,\n",
        "        \"l1\": l1,\n",
        "        \"sched\": sched,\n",
        "        \"cawr_T0\": cawr_T0,\n",
        "        \"cawr_Tmult\": cawr_Tmult,\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"grad_clip\": grad_clip,\n",
        "        \"noise_std\": noise_std,\n",
        "    }\n",
        "\n",
        "# Warmups (as before)\n",
        "MAX_EPOCHS_CAP = 512  # absolute cap (safety)\n",
        "WARMUP_EPOCHS_L1 = 30\n",
        "WARMUP_EPOCHS_DROPOUT = 30\n",
        "WARMUP_EPOCHS_WD = 30\n",
        "DROPOUT_START = 0.15\n",
        "WD_START = 0.0\n",
        "\n",
        "# Local helpers that add input dropout/noise and variable grad clip\n",
        "def _apply_input_dropout(x, p):\n",
        "    if p <= 0.0: return x\n",
        "    # inverted dropout on features\n",
        "    keep = 1.0 - p\n",
        "    mask = torch.bernoulli(torch.full_like(x, keep))\n",
        "    return x * mask / max(keep, 1e-6)\n",
        "\n",
        "def train_one_epoch_reg(model, optimizer, dataloader, device,\n",
        "                        l1_lambda=0.0, epoch=0, warmup_epochs=20,\n",
        "                        input_dropout=0.0, noise_std=0.0, grad_clip=5.0):\n",
        "    model.train()\n",
        "    warm = min(1.0, (epoch + 1) / float(warmup_epochs))\n",
        "    loss_sum, n_seen = 0.0, 0\n",
        "    for x, t, e in dataloader:\n",
        "        if e.sum().item() == 0:\n",
        "            continue\n",
        "        x, t, e = x.to(device), t.to(device), e.to(device)\n",
        "\n",
        "        # data-level regularization\n",
        "        if input_dropout > 0.0:\n",
        "            x = _apply_input_dropout(x, input_dropout)\n",
        "        if noise_std > 0.0:\n",
        "            x = x + noise_std * torch.randn_like(x)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        out = torch.clamp(model(x), -20, 20)\n",
        "        loss = cox_negloglik(out, e, t)\n",
        "        if l1_lambda > 0:\n",
        "            loss = loss + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        n_seen += x.size(0)\n",
        "    return {'avg_loss': loss_sum / max(n_seen, 1), 'warm': warm}\n",
        "\n",
        "def full_risk_set_step_reg(model, optimizer, ds, device,\n",
        "                           l1_lambda=0.0, warm=1.0,\n",
        "                           input_dropout=0.0, noise_std=0.0, grad_clip=5.0):\n",
        "    model.train()\n",
        "    X_all = ds.x.to(device); t_all = ds.time.to(device); e_all = ds.event.to(device)\n",
        "    XX = X_all\n",
        "    if input_dropout > 0.0:\n",
        "        XX = _apply_input_dropout(XX, input_dropout)\n",
        "    if noise_std > 0.0:\n",
        "        XX = XX + noise_std * torch.randn_like(XX)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    out_all = torch.clamp(model(XX), -20, 20)\n",
        "    loss_full = cox_negloglik(out_all, e_all, t_all)\n",
        "    if l1_lambda > 0:\n",
        "        loss_full = loss_full + (l1_lambda * warm) * l1_penalty_first_layer(model)\n",
        "    loss_full.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))\n",
        "    optimizer.step()\n",
        "    return float(loss_full.detach().cpu().item())\n",
        "\n",
        "# Optimizer that can optionally apply WD to the final layer\n",
        "def make_optimizer_hpo(model, lr, wd, apply_final_wd=False):\n",
        "    linears = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    last_linear = linears[-1] if len(linears) > 0 else None\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if name.endswith('bias'):\n",
        "            no_decay.append(p); continue\n",
        "        if (last_linear is not None) and (p is last_linear.weight) and not apply_final_wd:\n",
        "            no_decay.append(p); continue\n",
        "        decay.append(p)\n",
        "    param_groups = [{'params': decay, 'weight_decay': float(wd)},\n",
        "                    {'params': no_decay, 'weight_decay': 0.0}]\n",
        "    return optim.AdamW(param_groups, lr=float(lr))\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_hparams(trial)\n",
        "    layers = layers_from_arch(hp[\"arch\"])\n",
        "\n",
        "    # Build loaders with TRIAL-SPECIFIC batch size (smaller batches often regularize more)\n",
        "    bs = int(hp[\"batch_size\"])\n",
        "    tr_sampler = EventBalancedBatchSampler(ytr_event, bs, seed=42)\n",
        "    tr_loader = DataLoader(train_ds, batch_sampler=tr_sampler, num_workers=0)\n",
        "    tr_eval_loader = DataLoader(train_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "    va_loader = DataLoader(valid_ds, batch_size=bs, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Model / optimizer\n",
        "    model = DeepSurvMLP(in_dim, layers, dropout=hp[\"dropout\"]).to(device)\n",
        "    optimizer = make_optimizer_hpo(model, lr=hp[\"lr\"], wd=hp[\"wd\"],\n",
        "                                   apply_final_wd=bool(hp[\"apply_final_wd\"]))\n",
        "\n",
        "    # Scheduler per trial\n",
        "    epochs = int(min(hp[\"epochs\"], MAX_EPOCHS_CAP))\n",
        "    if hp[\"sched\"] == \"cosine\":\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "        def step_sched(epoch_idx): scheduler.step()\n",
        "    elif hp[\"sched\"] == \"cawr\":\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=int(hp[\"cawr_T0\"]), T_mult=int(hp[\"cawr_Tmult\"])\n",
        "        )\n",
        "        def step_sched(epoch_idx): scheduler.step(epoch_idx + 1)\n",
        "    else:\n",
        "        scheduler = None\n",
        "        def step_sched(epoch_idx): pass\n",
        "\n",
        "    best_val_ci = -np.inf\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Warm-up schedules for dropout & WD\n",
        "        frac_d = min(1.0, epoch / float(WARMUP_EPOCHS_DROPOUT))\n",
        "        frac_w = min(1.0, epoch / float(WARMUP_EPOCHS_WD))\n",
        "        set_dropout_p(model, DROPOUT_START + (hp['dropout'] - DROPOUT_START) * frac_d)\n",
        "        set_weight_decay(optimizer, WD_START + (hp['wd'] - WD_START) * frac_w)\n",
        "\n",
        "        # One epoch + full risk-set step with extra regularizers\n",
        "        stats = train_one_epoch_reg(\n",
        "            model, optimizer, tr_loader, device,\n",
        "            l1_lambda=hp[\"l1\"], epoch=epoch, warmup_epochs=WARMUP_EPOCHS_L1,\n",
        "            input_dropout=hp[\"input_dropout\"], noise_std=hp[\"noise_std\"],\n",
        "            grad_clip=hp[\"grad_clip\"]\n",
        "        )\n",
        "        _ = full_risk_set_step_reg(\n",
        "            model, optimizer, train_ds, device,\n",
        "            l1_lambda=hp[\"l1\"], warm=stats['warm'],\n",
        "            input_dropout=hp[\"input_dropout\"], noise_std=hp[\"noise_std\"],\n",
        "            grad_clip=hp[\"grad_clip\"]\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_ci = evaluate_ci(model, va_loader, device)\n",
        "        step_sched(epoch)\n",
        "\n",
        "        # Report for pruning\n",
        "        trial.report(val_ci, step=epoch)\n",
        "        if val_ci > best_val_ci:\n",
        "            best_val_ci = val_ci\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "        if trial.should_prune():\n",
        "            # Clean up GPU mem before pruning\n",
        "            del model, optimizer, scheduler\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    trial.set_user_attr(\"best_epoch\", int(best_epoch))\n",
        "\n",
        "    # Clean up\n",
        "    del model, optimizer, scheduler\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return best_val_ci\n",
        "\n",
        "# ---- Study / Sampler / Pruner (tune TPE a bit for broader exploration) ----\n",
        "storage = \"sqlite:///deepsurv_optuna.db\"\n",
        "study_name = \"deepsurv_cox_hpo_overfit_reducer\"\n",
        "\n",
        "sampler = optuna.samplers.TPESampler(\n",
        "    seed=42,\n",
        "    multivariate=True,\n",
        "    group=True,\n",
        "    n_startup_trials=40,       # explore more before exploitation\n",
        "    constant_liar=True,        # better parallel behavior if you run parallel\n",
        "    consider_prior=True\n",
        ")\n",
        "\n",
        "# Hyperband/SH both work; Hyperband gives a bit more flexibility across resource levels\n",
        "pruner = optuna.pruners.HyperbandPruner(min_resource=16, reduction_factor=3)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",       # single objective (Val CI)\n",
        "    study_name=study_name,\n",
        "    storage=storage,\n",
        "    load_if_exists=True,\n",
        "    sampler=sampler,\n",
        "    pruner=pruner\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Launch Optuna Dashboard (Colab proxied URL printed)\n",
        "#   (No unsupported args; previous TypeError fixed)\n",
        "# ============================================================\n",
        "try:\n",
        "    from optuna_dashboard import run_server\n",
        "    from google.colab import output\n",
        "    import portpicker\n",
        "    PORT = portpicker.pick_unused_port()\n",
        "    def _start_dashboard():\n",
        "        # NOTE: do NOT pass unsupported kwargs like reload/quiet\n",
        "        run_server(storage, host=\"0.0.0.0\", port=PORT)\n",
        "    t = threading.Thread(target=_start_dashboard, daemon=True)\n",
        "    t.start()\n",
        "    time.sleep(2)\n",
        "    dash_url = output.eval_js(f\"google.colab.kernel.proxyPort({PORT}, {{'cache': false}})\")\n",
        "    print(\"Optuna Dashboard:\", dash_url)\n",
        "except Exception as ex:\n",
        "    print(\"[Optuna Dashboard] Could not start dashboard automatically.\", ex)\n",
        "    print(\"You can run it locally with:  optuna-dashboard sqlite:///deepsurv_optuna.db\")\n",
        "\n",
        "# ============================================================\n",
        "# Run Optimization\n",
        "# ============================================================\n",
        "N_TRIALS = 100  # adjust as needed\n",
        "print(f\"Starting optimization: {N_TRIALS} trials × up to {MAX_EPOCHS_CAP} epochs\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True)\n",
        "\n",
        "print(\"\\n[Best] Val CI:\", study.best_value)\n",
        "print(\"[Best] Params:\", study.best_params)\n",
        "print(\"[Best] Best epoch:\", study.best_trial.user_attrs.get(\"best_epoch\", MAX_EPOCHS_CAP))\n",
        "\n",
        "# ============================================================\n",
        "# Retrain on Train+Val with best hyperparams; evaluate Test\n",
        "# - Combine Train+Val, sort, restandardize; apply to Test\n",
        "# ============================================================\n",
        "# Prepare Train+Val\n",
        "trainval_df = pd.concat([train_df, valid_df], axis=0, ignore_index=True)\n",
        "trainval_df = trainval_df.sort_values(by=[\"OS_MONTHS\",\"OS_STATUS\"], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "X_trv = trainval_df[feat_candidates].values.astype(np.float32)\n",
        "y_trv_time = trainval_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "y_trv_event = trainval_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "# Median impute by Train+Val medians (for retraining phase)\n",
        "trv_medians = np.nanmedian(X_trv, axis=0)\n",
        "X_trv = np.where(np.isnan(X_trv), trv_medians, X_trv)\n",
        "\n",
        "scaler_trv = StandardScaler().fit(X_trv)\n",
        "X_trv = scaler_trv.transform(X_trv).astype(np.float32)\n",
        "\n",
        "# Test set standardized with Train+Val scaler\n",
        "X_test = test_df[feat_candidates].values.astype(np.float32)\n",
        "X_test = np.where(np.isnan(X_test), trv_medians, X_test)\n",
        "X_test = scaler_trv.transform(X_test).astype(np.float32)\n",
        "y_te_time = test_df[\"OS_MONTHS\"].values.astype(np.float32)\n",
        "y_te_event = test_df[\"OS_STATUS\"].values.astype(int)\n",
        "\n",
        "# Loaders\n",
        "BATCH_SIZE = 64\n",
        "trv_ds = SurvivalDataset(X_trv, y_trv_time, y_trv_event)\n",
        "te_ds  = SurvivalDataset(X_test, y_te_time, y_te_event)\n",
        "\n",
        "trv_sampler = EventBalancedBatchSampler(y_trv_event, BATCH_SIZE, seed=7)\n",
        "trv_loader  = DataLoader(trv_ds, batch_sampler=trv_sampler, num_workers=0)\n",
        "trv_eval_loader = DataLoader(trv_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "te_loader  = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# Build & train final model\n",
        "best_hp = study.best_params\n",
        "best_layers = layers_from_arch(best_hp[\"arch\"])\n",
        "best_n_epochs = int(study.best_trial.user_attrs.get(\"best_epoch\", MAX_EPOCHS_CAP))\n",
        "best_n_epochs = max(16, min(best_n_epochs, MAX_EPOCHS_CAP))\n",
        "\n",
        "model_final = DeepSurvMLP(in_dim, best_layers, dropout=best_hp[\"dropout\"]).to(device)\n",
        "opt_final = make_optimizer(model_final, lr=best_hp[\"lr\"], wd=best_hp[\"wd\"])\n",
        "sched_final = torch.optim.lr_scheduler.CosineAnnealingLR(opt_final, T_max=best_n_epochs)\n",
        "\n",
        "for epoch in range(best_n_epochs):\n",
        "    frac_d = min(1.0, epoch / float(WARMUP_EPOCHS_DROPOUT))\n",
        "    frac_w = min(1.0, epoch / float(WARMUP_EPOCHS_WD))\n",
        "    set_dropout_p(model_final, DROPOUT_START + (best_hp['dropout'] - DROPOUT_START) * frac_d)\n",
        "    set_weight_decay(opt_final, WD_START + (best_hp['wd'] - WD_START) * frac_w)\n",
        "\n",
        "    stats = train_one_epoch(model_final, opt_final, trv_loader, device, l1_lambda=best_hp[\"l1\"],\n",
        "                            epoch=epoch, warmup_epochs=WARMUP_EPOCHS_L1)\n",
        "    _ = full_risk_set_step(model_final, opt_final, trv_ds, device, l1_lambda=best_hp[\"l1\"], warm=stats['warm'])\n",
        "    sched_final.step()\n",
        "\n",
        "# Evaluate\n",
        "trainval_ci = evaluate_ci(model_final, trv_eval_loader, device)\n",
        "test_ci = evaluate_ci(model_final, te_loader, device)\n",
        "print(f\"\\n[Final] Train+Val CI: {trainval_ci:.4f}\")\n",
        "print(f\"[Final] Test CI:      {test_ci:.4f}\")\n",
        "\n",
        "# (Optional) Save artifacts to Drive\n",
        "OUT_DIR = \"/content/drive/MyDrive/deepsurv_results_optuna\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "torch.save(model_final.state_dict(), os.path.join(OUT_DIR, \"deepsurv_best.pt\"))\n",
        "with open(os.path.join(OUT_DIR, \"best_params.txt\"), \"w\") as f:\n",
        "    f.write(str(study.best_params))\n",
        "print(\"Saved final model and best params to:\", OUT_DIR)\n"
      ],
      "metadata": {
        "id": "lbcrdoE4tyWq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "62a7114b-6b41-4c16-b468-a3ea1a9f8ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "Genes.csv path: /content/drive/MyDrive/Genes.csv\n",
            "[Genes] Selected 1555 genes with Prop == 1\n",
            "[Features] Using 1573 common features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2650183406.py:263: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-2650183406.py:263: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n",
            "/tmp/ipython-input-2650183406.py:263: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Adjuvant Chemo\"] = df[\"Adjuvant Chemo\"].replace({\"OBS\":0, \"ACT\":1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim: 1573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.\n",
            "  warnings.warn(\n",
            "[I 2025-10-19 05:02:14,045] A new study created in RDB with name: deepsurv_cox_hpo_overfit_reducer\n",
            "Bottle v0.13.4 server starting up (using WSGIRefServer())...\n",
            "Listening on http://0.0.0.0:40971/\n",
            "Hit Ctrl-C to quit.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna Dashboard: https://40971-gpu-t4-s-104ldvpjndb22-c.asia-southeast1-1.prod.colab.dev\n",
            "Starting optimization: 100 trials × up to 512 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [19/Oct/2025 05:02:24] \"GET / HTTP/1.1\" 302 0\n",
            "127.0.0.1 - - [19/Oct/2025 05:02:30] \"GET / HTTP/1.1\" 302 0\n",
            "127.0.0.1 - - [19/Oct/2025 05:02:35] \"GET / HTTP/1.1\" 302 0\n"
          ]
        }
      ]
    }
  ]
}